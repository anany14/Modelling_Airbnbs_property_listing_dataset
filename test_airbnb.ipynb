{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/airbnb-property-listing/tabular_data/listing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Amenities</th>\n",
       "      <th>Location</th>\n",
       "      <th>guests</th>\n",
       "      <th>beds</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>Price_Night</th>\n",
       "      <th>Cleanliness_rating</th>\n",
       "      <th>Accuracy_rating</th>\n",
       "      <th>Communication_rating</th>\n",
       "      <th>Location_rating</th>\n",
       "      <th>Check-in_rating</th>\n",
       "      <th>Value_rating</th>\n",
       "      <th>amenities_count</th>\n",
       "      <th>url</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f9dcbd09-32ac-41d9-a0b1-fdb2793378cf</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Red Kite Tree Tent - Ynys Affalon</td>\n",
       "      <td>['About this space', \"Escape to one of these t...</td>\n",
       "      <td>['What this place offers', 'Bathroom', 'Shampo...</td>\n",
       "      <td>Llandrindod Wells United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>105</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/26620994?adults...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b4736a7-e73e-45bc-a9b5-d3e7fcf652fd</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Az Alom Cabin - Treehouse Tree to Nature Cabin</td>\n",
       "      <td>['About this space', \"Come and spend a romanti...</td>\n",
       "      <td>['What this place offers', 'Bedroom and laundr...</td>\n",
       "      <td>Guyonvelle Grand Est France</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/27055498?adults...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d577bc30-2222-4bef-a35e-a9825642aec4</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Cabane Entre Les Pins\\nüå≤üèïÔ∏èüå≤</td>\n",
       "      <td>['About this space', 'Rustic cabin between the...</td>\n",
       "      <td>['What this place offers', 'Scenic views', 'Ga...</td>\n",
       "      <td>Duclair Normandie France</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>52</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>51.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/51427108?adults...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca9cbfd4-7798-4e8d-8c17-d5a64fba0abc</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Tree Top Cabin with log burner &amp; private hot tub</td>\n",
       "      <td>['About this space', 'The Tree top cabin is si...</td>\n",
       "      <td>['What this place offers', 'Bathroom', 'Hot wa...</td>\n",
       "      <td>Barmouth Wales United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>132</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/49543851?adults...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8b2d0f78-16d8-4559-8692-62ebce2a1302</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Hanging cabin</td>\n",
       "      <td>['About this space', 'Feel refreshed at this u...</td>\n",
       "      <td>['What this place offers', 'Heating and coolin...</td>\n",
       "      <td>Wargnies-le-Petit Hauts-de-France France</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/50166553?adults...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID    Category  \\\n",
       "0  f9dcbd09-32ac-41d9-a0b1-fdb2793378cf  Treehouses   \n",
       "1  1b4736a7-e73e-45bc-a9b5-d3e7fcf652fd  Treehouses   \n",
       "2  d577bc30-2222-4bef-a35e-a9825642aec4  Treehouses   \n",
       "3  ca9cbfd4-7798-4e8d-8c17-d5a64fba0abc  Treehouses   \n",
       "4  8b2d0f78-16d8-4559-8692-62ebce2a1302  Treehouses   \n",
       "\n",
       "                                              Title  \\\n",
       "0                 Red Kite Tree Tent - Ynys Affalon   \n",
       "1    Az Alom Cabin - Treehouse Tree to Nature Cabin   \n",
       "2                       Cabane Entre Les Pins\\nüå≤üèïÔ∏èüå≤   \n",
       "3  Tree Top Cabin with log burner & private hot tub   \n",
       "4                                     Hanging cabin   \n",
       "\n",
       "                                         Description  \\\n",
       "0  ['About this space', \"Escape to one of these t...   \n",
       "1  ['About this space', \"Come and spend a romanti...   \n",
       "2  ['About this space', 'Rustic cabin between the...   \n",
       "3  ['About this space', 'The Tree top cabin is si...   \n",
       "4  ['About this space', 'Feel refreshed at this u...   \n",
       "\n",
       "                                           Amenities  \\\n",
       "0  ['What this place offers', 'Bathroom', 'Shampo...   \n",
       "1  ['What this place offers', 'Bedroom and laundr...   \n",
       "2  ['What this place offers', 'Scenic views', 'Ga...   \n",
       "3  ['What this place offers', 'Bathroom', 'Hot wa...   \n",
       "4  ['What this place offers', 'Heating and coolin...   \n",
       "\n",
       "                                   Location guests  beds  bathrooms  \\\n",
       "0          Llandrindod Wells United Kingdom      2   1.0        1.0   \n",
       "1               Guyonvelle Grand Est France      3   3.0        0.0   \n",
       "2                  Duclair Normandie France      4   2.0        1.5   \n",
       "3             Barmouth Wales United Kingdom      2   NaN        1.0   \n",
       "4  Wargnies-le-Petit Hauts-de-France France      2   1.0        NaN   \n",
       "\n",
       "   Price_Night  Cleanliness_rating  Accuracy_rating  Communication_rating  \\\n",
       "0          105                 4.6              4.7                   4.3   \n",
       "1           92                 4.3              4.7                   4.6   \n",
       "2           52                 4.2              4.6                   4.8   \n",
       "3          132                 4.8              4.9                   4.9   \n",
       "4          111                 NaN              NaN                   NaN   \n",
       "\n",
       "   Location_rating  Check-in_rating  Value_rating  amenities_count  \\\n",
       "0              5.0              4.3           4.3             13.0   \n",
       "1              4.9              4.7           4.5              8.0   \n",
       "2              4.8              4.8           4.7             51.0   \n",
       "3              4.9              5.0           4.6             23.0   \n",
       "4              NaN              NaN           NaN              5.0   \n",
       "\n",
       "                                                 url bedrooms  Unnamed: 19  \n",
       "0  https://www.airbnb.co.uk/rooms/26620994?adults...      NaN          NaN  \n",
       "1  https://www.airbnb.co.uk/rooms/27055498?adults...        1          NaN  \n",
       "2  https://www.airbnb.co.uk/rooms/51427108?adults...        1          NaN  \n",
       "3  https://www.airbnb.co.uk/rooms/49543851?adults...      NaN          NaN  \n",
       "4  https://www.airbnb.co.uk/rooms/50166553?adults...        1          NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing rows with missing columns in rating\n",
    "df = df.dropna(subset=['Cleanliness_rating','Accuracy_rating','Communication_rating','Location_rating','Check-in_rating','Value_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Amenities</th>\n",
       "      <th>Location</th>\n",
       "      <th>guests</th>\n",
       "      <th>beds</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>Price_Night</th>\n",
       "      <th>Cleanliness_rating</th>\n",
       "      <th>Accuracy_rating</th>\n",
       "      <th>Communication_rating</th>\n",
       "      <th>Location_rating</th>\n",
       "      <th>Check-in_rating</th>\n",
       "      <th>Value_rating</th>\n",
       "      <th>amenities_count</th>\n",
       "      <th>url</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f9dcbd09-32ac-41d9-a0b1-fdb2793378cf</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Red Kite Tree Tent - Ynys Affalon</td>\n",
       "      <td>['About this space', \"Escape to one of these t...</td>\n",
       "      <td>['What this place offers', 'Bathroom', 'Shampo...</td>\n",
       "      <td>Llandrindod Wells United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>105</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/26620994?adults...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b4736a7-e73e-45bc-a9b5-d3e7fcf652fd</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Az Alom Cabin - Treehouse Tree to Nature Cabin</td>\n",
       "      <td>['About this space', \"Come and spend a romanti...</td>\n",
       "      <td>['What this place offers', 'Bedroom and laundr...</td>\n",
       "      <td>Guyonvelle Grand Est France</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/27055498?adults...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d577bc30-2222-4bef-a35e-a9825642aec4</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Cabane Entre Les Pins\\nüå≤üèïÔ∏èüå≤</td>\n",
       "      <td>['About this space', 'Rustic cabin between the...</td>\n",
       "      <td>['What this place offers', 'Scenic views', 'Ga...</td>\n",
       "      <td>Duclair Normandie France</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>52</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>51.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/51427108?adults...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca9cbfd4-7798-4e8d-8c17-d5a64fba0abc</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Tree Top Cabin with log burner &amp; private hot tub</td>\n",
       "      <td>['About this space', 'The Tree top cabin is si...</td>\n",
       "      <td>['What this place offers', 'Bathroom', 'Hot wa...</td>\n",
       "      <td>Barmouth Wales United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>132</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/49543851?adults...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cfe479b9-c8f8-44af-9bc6-46ede9f14bb5</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Treehouse near Paris Disney</td>\n",
       "      <td>['About this space', 'Charming cabin nestled i...</td>\n",
       "      <td>['What this place offers', 'Bathroom', 'Hair d...</td>\n",
       "      <td>Le Plessis-Feu-Aussoux √éle-de-France France</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>143</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>32.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/935398?adults=1...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID    Category  \\\n",
       "0  f9dcbd09-32ac-41d9-a0b1-fdb2793378cf  Treehouses   \n",
       "1  1b4736a7-e73e-45bc-a9b5-d3e7fcf652fd  Treehouses   \n",
       "2  d577bc30-2222-4bef-a35e-a9825642aec4  Treehouses   \n",
       "3  ca9cbfd4-7798-4e8d-8c17-d5a64fba0abc  Treehouses   \n",
       "5  cfe479b9-c8f8-44af-9bc6-46ede9f14bb5  Treehouses   \n",
       "\n",
       "                                              Title  \\\n",
       "0                 Red Kite Tree Tent - Ynys Affalon   \n",
       "1    Az Alom Cabin - Treehouse Tree to Nature Cabin   \n",
       "2                       Cabane Entre Les Pins\\nüå≤üèïÔ∏èüå≤   \n",
       "3  Tree Top Cabin with log burner & private hot tub   \n",
       "5                       Treehouse near Paris Disney   \n",
       "\n",
       "                                         Description  \\\n",
       "0  ['About this space', \"Escape to one of these t...   \n",
       "1  ['About this space', \"Come and spend a romanti...   \n",
       "2  ['About this space', 'Rustic cabin between the...   \n",
       "3  ['About this space', 'The Tree top cabin is si...   \n",
       "5  ['About this space', 'Charming cabin nestled i...   \n",
       "\n",
       "                                           Amenities  \\\n",
       "0  ['What this place offers', 'Bathroom', 'Shampo...   \n",
       "1  ['What this place offers', 'Bedroom and laundr...   \n",
       "2  ['What this place offers', 'Scenic views', 'Ga...   \n",
       "3  ['What this place offers', 'Bathroom', 'Hot wa...   \n",
       "5  ['What this place offers', 'Bathroom', 'Hair d...   \n",
       "\n",
       "                                      Location guests  beds  bathrooms  \\\n",
       "0             Llandrindod Wells United Kingdom      2   1.0        1.0   \n",
       "1                  Guyonvelle Grand Est France      3   3.0        0.0   \n",
       "2                     Duclair Normandie France      4   2.0        1.5   \n",
       "3                Barmouth Wales United Kingdom      2   NaN        1.0   \n",
       "5  Le Plessis-Feu-Aussoux √éle-de-France France      4   3.0        1.0   \n",
       "\n",
       "   Price_Night  Cleanliness_rating  Accuracy_rating  Communication_rating  \\\n",
       "0          105                 4.6              4.7                   4.3   \n",
       "1           92                 4.3              4.7                   4.6   \n",
       "2           52                 4.2              4.6                   4.8   \n",
       "3          132                 4.8              4.9                   4.9   \n",
       "5          143                 5.0              4.9                   5.0   \n",
       "\n",
       "   Location_rating  Check-in_rating  Value_rating  amenities_count  \\\n",
       "0              5.0              4.3           4.3             13.0   \n",
       "1              4.9              4.7           4.5              8.0   \n",
       "2              4.8              4.8           4.7             51.0   \n",
       "3              4.9              5.0           4.6             23.0   \n",
       "5              4.7              5.0           4.7             32.0   \n",
       "\n",
       "                                                 url bedrooms  Unnamed: 19  \n",
       "0  https://www.airbnb.co.uk/rooms/26620994?adults...      NaN          NaN  \n",
       "1  https://www.airbnb.co.uk/rooms/27055498?adults...        1          NaN  \n",
       "2  https://www.airbnb.co.uk/rooms/51427108?adults...        1          NaN  \n",
       "3  https://www.airbnb.co.uk/rooms/49543851?adults...      NaN          NaN  \n",
       "5  https://www.airbnb.co.uk/rooms/935398?adults=1...        2          NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problematic_row = df.loc[df['Title'] == 'Stunning Cotswolds Water Park'].index[0]\n",
    "problematic_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                   943e9242-7cfe-4db2-bb56-d7147fef352e\n",
       "Category                                                    Amazing pools\n",
       "Title                    The Burrow - Luxurious Pet Friendly, 3 Bed Lodge\n",
       "Description             ['About this space', \"Set in a peaceful area o...\n",
       "Amenities               ['What this place offers', 'Bathroom', 'Shampo...\n",
       "Location                                  Cotswold England United Kingdom\n",
       "guests                                                                  6\n",
       "beds                                                                  5.0\n",
       "bathrooms                                                             1.5\n",
       "Price_Night                                                           163\n",
       "Cleanliness_rating                                                    5.0\n",
       "Accuracy_rating                                                       4.9\n",
       "Communication_rating                                                  4.9\n",
       "Location_rating                                                       4.9\n",
       "Check-in_rating                                                       4.9\n",
       "Value_rating                                                          4.9\n",
       "amenities_count                                                      37.0\n",
       "url                     https://www.airbnb.co.uk/rooms/41170958?adults...\n",
       "bedrooms                                                                3\n",
       "Unnamed: 19                                                           NaN\n",
       "Name: 587, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[587]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n trying something\\ndef combine_description_settings(df1):\\n    # Drop rows where description is empty\\n    df1 = df1.dropna(subset=['Description'])\\n\\n    # Helper function to clean and combine list items into a single string\\n    def clean_and_combine_list_items(item):\\n        if isinstance(item, str):\\n            return ', '.join(v.strip() for v in ast.literal_eval(item) if isinstance(v, str))\\n        return ''\\n\\n    # Cleaning and combining description and amenities columns\\n    df1['Description'] = df1['Description'].str.replace('About this space', '').str.strip().apply(clean_and_combine_list_items)\\n    df1['Amenities'] = df1['Amenities'].str.replace('What this place offers', '').str.strip().apply(clean_and_combine_list_items)\\n\\n    # Removing the first comma\\n    df1['Amenities'] = df1['Amenities'].str.replace(',', '', 1)\\n\\n    return df1\\n\\ndf = combine_description_settings(df)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " trying something\n",
    "def combine_description_settings(df1):\n",
    "    # Drop rows where description is empty\n",
    "    df1 = df1.dropna(subset=['Description'])\n",
    "\n",
    "    # Helper function to clean and combine list items into a single string\n",
    "    def clean_and_combine_list_items(item):\n",
    "        if isinstance(item, str):\n",
    "            return ', '.join(v.strip() for v in ast.literal_eval(item) if isinstance(v, str))\n",
    "        return ''\n",
    "\n",
    "    # Cleaning and combining description and amenities columns\n",
    "    df1['Description'] = df1['Description'].str.replace('About this space', '').str.strip().apply(clean_and_combine_list_items)\n",
    "    df1['Amenities'] = df1['Amenities'].str.replace('What this place offers', '').str.strip().apply(clean_and_combine_list_items)\n",
    "\n",
    "    # Removing the first comma\n",
    "    df1['Amenities'] = df1['Amenities'].str.replace(',', '', 1)\n",
    "\n",
    "    return df1\n",
    "\n",
    "df = combine_description_settings(df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([586], dtype='int64')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['ID']=='4c917b3c-d693-4ee4-a321-f5babc728dc9'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.at[586, 'Title'] = 'Stunning Cotswolds Water Park, sleeps 6 with pool'\n",
    "df.at[586, 'Description'] = df['Amenities'][586]\n",
    "df.at[586, 'Amenities'] = df['Location'][586]\n",
    "df.at[586, 'Location'] = df['guests'][586]\n",
    "df.at[586, 'guests'] = df['beds'][586]\n",
    "df.at[586, 'beds'] = df['bathrooms'][586]\n",
    "df.at[586, 'bathrooms'] = df['Price_Night'][586]\n",
    "df.at[586, 'Price_Night'] = df['Cleanliness_rating'][586]\n",
    "df.at[586, 'Cleanliness_rating'] = df['Accuracy_rating'][586]\n",
    "df.at[586, 'Accuracy_rating'] = df['Communication_rating'][586]\n",
    "df.at[586, 'Communication_rating'] = df['bedrooms'][586]\n",
    "df.at[586, 'bedrooms'] = df['Unnamed: 19'][586]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Description'] != 'sleeps 6 with pool']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df = df.dropna(subset=['Description'])\n",
    "# Remove \"About this space\" prefix\n",
    "df['Description'] = df['Description'].apply(lambda x: x.replace('About this space', '').strip() if isinstance(x, str) else x)\n",
    "\n",
    "# Combine list items into a single string\n",
    "df['Description'] = df['Description'].apply(lambda x: ' '.join(['' if v == '' else v.strip() for v in ast.literal_eval(x) if isinstance(v, str)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Amenities</th>\n",
       "      <th>Location</th>\n",
       "      <th>guests</th>\n",
       "      <th>beds</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>Price_Night</th>\n",
       "      <th>Cleanliness_rating</th>\n",
       "      <th>Accuracy_rating</th>\n",
       "      <th>Communication_rating</th>\n",
       "      <th>Location_rating</th>\n",
       "      <th>Check-in_rating</th>\n",
       "      <th>Value_rating</th>\n",
       "      <th>amenities_count</th>\n",
       "      <th>url</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f9dcbd09-32ac-41d9-a0b1-fdb2793378cf</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Red Kite Tree Tent - Ynys Affalon</td>\n",
       "      <td>Escape to one of these two fabulous Tree Tent...</td>\n",
       "      <td>['What this place offers', 'Bathroom', 'Shampo...</td>\n",
       "      <td>Llandrindod Wells United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>105</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/26620994?adults...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b4736a7-e73e-45bc-a9b5-d3e7fcf652fd</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Az Alom Cabin - Treehouse Tree to Nature Cabin</td>\n",
       "      <td>Come and spend a romantic stay with a couple ...</td>\n",
       "      <td>['What this place offers', 'Bedroom and laundr...</td>\n",
       "      <td>Guyonvelle Grand Est France</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/27055498?adults...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d577bc30-2222-4bef-a35e-a9825642aec4</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Cabane Entre Les Pins\\nüå≤üèïÔ∏èüå≤</td>\n",
       "      <td>Rustic cabin between the pines, 3 meters high...</td>\n",
       "      <td>['What this place offers', 'Scenic views', 'Ga...</td>\n",
       "      <td>Duclair Normandie France</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>52</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>51.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/51427108?adults...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca9cbfd4-7798-4e8d-8c17-d5a64fba0abc</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Tree Top Cabin with log burner &amp; private hot tub</td>\n",
       "      <td>The Tree top cabin is situated in our peacefu...</td>\n",
       "      <td>['What this place offers', 'Bathroom', 'Hot wa...</td>\n",
       "      <td>Barmouth Wales United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>132</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/49543851?adults...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cfe479b9-c8f8-44af-9bc6-46ede9f14bb5</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Treehouse near Paris Disney</td>\n",
       "      <td>Charming cabin nestled in the leaves, real un...</td>\n",
       "      <td>['What this place offers', 'Bathroom', 'Hair d...</td>\n",
       "      <td>Le Plessis-Feu-Aussoux √éle-de-France France</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>143</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>32.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/935398?adults=1...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID    Category  \\\n",
       "0  f9dcbd09-32ac-41d9-a0b1-fdb2793378cf  Treehouses   \n",
       "1  1b4736a7-e73e-45bc-a9b5-d3e7fcf652fd  Treehouses   \n",
       "2  d577bc30-2222-4bef-a35e-a9825642aec4  Treehouses   \n",
       "3  ca9cbfd4-7798-4e8d-8c17-d5a64fba0abc  Treehouses   \n",
       "5  cfe479b9-c8f8-44af-9bc6-46ede9f14bb5  Treehouses   \n",
       "\n",
       "                                              Title  \\\n",
       "0                 Red Kite Tree Tent - Ynys Affalon   \n",
       "1    Az Alom Cabin - Treehouse Tree to Nature Cabin   \n",
       "2                       Cabane Entre Les Pins\\nüå≤üèïÔ∏èüå≤   \n",
       "3  Tree Top Cabin with log burner & private hot tub   \n",
       "5                       Treehouse near Paris Disney   \n",
       "\n",
       "                                         Description  \\\n",
       "0   Escape to one of these two fabulous Tree Tent...   \n",
       "1   Come and spend a romantic stay with a couple ...   \n",
       "2   Rustic cabin between the pines, 3 meters high...   \n",
       "3   The Tree top cabin is situated in our peacefu...   \n",
       "5   Charming cabin nestled in the leaves, real un...   \n",
       "\n",
       "                                           Amenities  \\\n",
       "0  ['What this place offers', 'Bathroom', 'Shampo...   \n",
       "1  ['What this place offers', 'Bedroom and laundr...   \n",
       "2  ['What this place offers', 'Scenic views', 'Ga...   \n",
       "3  ['What this place offers', 'Bathroom', 'Hot wa...   \n",
       "5  ['What this place offers', 'Bathroom', 'Hair d...   \n",
       "\n",
       "                                      Location guests  beds  bathrooms  \\\n",
       "0             Llandrindod Wells United Kingdom      2   1.0        1.0   \n",
       "1                  Guyonvelle Grand Est France      3   3.0        0.0   \n",
       "2                     Duclair Normandie France      4   2.0        1.5   \n",
       "3                Barmouth Wales United Kingdom      2   NaN        1.0   \n",
       "5  Le Plessis-Feu-Aussoux √éle-de-France France      4   3.0        1.0   \n",
       "\n",
       "   Price_Night  Cleanliness_rating  Accuracy_rating Communication_rating  \\\n",
       "0          105                 4.6              4.7                  4.3   \n",
       "1           92                 4.3              4.7                  4.6   \n",
       "2           52                 4.2              4.6                  4.8   \n",
       "3          132                 4.8              4.9                  4.9   \n",
       "5          143                 5.0              4.9                  5.0   \n",
       "\n",
       "   Location_rating  Check-in_rating  Value_rating  amenities_count  \\\n",
       "0              5.0              4.3           4.3             13.0   \n",
       "1              4.9              4.7           4.5              8.0   \n",
       "2              4.8              4.8           4.7             51.0   \n",
       "3              4.9              5.0           4.6             23.0   \n",
       "5              4.7              5.0           4.7             32.0   \n",
       "\n",
       "                                                 url bedrooms  Unnamed: 19  \n",
       "0  https://www.airbnb.co.uk/rooms/26620994?adults...      NaN          NaN  \n",
       "1  https://www.airbnb.co.uk/rooms/27055498?adults...        1          NaN  \n",
       "2  https://www.airbnb.co.uk/rooms/51427108?adults...        1          NaN  \n",
       "3  https://www.airbnb.co.uk/rooms/49543851?adults...      NaN          NaN  \n",
       "5  https://www.airbnb.co.uk/rooms/935398?adults=1...        2          NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"guests\"].fillna(1, inplace=True)\n",
    "df[\"beds\"].fillna(1, inplace=True)\n",
    "df[\"bathrooms\"].fillna(1, inplace=True)\n",
    "df[\"bedrooms\"].fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"About this space\" prefix\n",
    "df['Amenities'] = df['Amenities'].apply(lambda x: x.replace('What this place offers', '').strip() if isinstance(x, str) else x)\n",
    "\n",
    "# Combine list items into a single string\n",
    "df['Amenities'] = df['Amenities'].apply(lambda x: ', '.join(['' if v == '' else v.strip() for v in ast.literal_eval(x) if isinstance(v, str)]))\n",
    "\n",
    "df['Amenities'] = df['Amenities'].str.replace(',', '', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Amenities</th>\n",
       "      <th>Location</th>\n",
       "      <th>guests</th>\n",
       "      <th>beds</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>Price_Night</th>\n",
       "      <th>Cleanliness_rating</th>\n",
       "      <th>Accuracy_rating</th>\n",
       "      <th>Communication_rating</th>\n",
       "      <th>Location_rating</th>\n",
       "      <th>Check-in_rating</th>\n",
       "      <th>Value_rating</th>\n",
       "      <th>amenities_count</th>\n",
       "      <th>url</th>\n",
       "      <th>bedrooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f9dcbd09-32ac-41d9-a0b1-fdb2793378cf</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Red Kite Tree Tent - Ynys Affalon</td>\n",
       "      <td>Escape to one of these two fabulous Tree Tent...</td>\n",
       "      <td>Bathroom, Shampoo, Bedroom and laundry, Essen...</td>\n",
       "      <td>Llandrindod Wells United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>105</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/26620994?adults...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b4736a7-e73e-45bc-a9b5-d3e7fcf652fd</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Az Alom Cabin - Treehouse Tree to Nature Cabin</td>\n",
       "      <td>Come and spend a romantic stay with a couple ...</td>\n",
       "      <td>Bedroom and laundry, Essentials, Towels, bed ...</td>\n",
       "      <td>Guyonvelle Grand Est France</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/27055498?adults...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d577bc30-2222-4bef-a35e-a9825642aec4</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Cabane Entre Les Pins\\nüå≤üèïÔ∏èüå≤</td>\n",
       "      <td>Rustic cabin between the pines, 3 meters high...</td>\n",
       "      <td>Scenic views, Garden view, River view, Bathro...</td>\n",
       "      <td>Duclair Normandie France</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>52</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>51.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/51427108?adults...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca9cbfd4-7798-4e8d-8c17-d5a64fba0abc</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Tree Top Cabin with log burner &amp; private hot tub</td>\n",
       "      <td>The Tree top cabin is situated in our peacefu...</td>\n",
       "      <td>Bathroom, Hot water, Bedroom and laundry, Ess...</td>\n",
       "      <td>Barmouth Wales United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>132</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/49543851?adults...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cfe479b9-c8f8-44af-9bc6-46ede9f14bb5</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Treehouse near Paris Disney</td>\n",
       "      <td>Charming cabin nestled in the leaves, real un...</td>\n",
       "      <td>Bathroom, Hair dryer, Shampoo, Hot water, Sho...</td>\n",
       "      <td>Le Plessis-Feu-Aussoux √éle-de-France France</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>143</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>32.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/935398?adults=1...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID    Category  \\\n",
       "0  f9dcbd09-32ac-41d9-a0b1-fdb2793378cf  Treehouses   \n",
       "1  1b4736a7-e73e-45bc-a9b5-d3e7fcf652fd  Treehouses   \n",
       "2  d577bc30-2222-4bef-a35e-a9825642aec4  Treehouses   \n",
       "3  ca9cbfd4-7798-4e8d-8c17-d5a64fba0abc  Treehouses   \n",
       "4  cfe479b9-c8f8-44af-9bc6-46ede9f14bb5  Treehouses   \n",
       "\n",
       "                                              Title  \\\n",
       "0                 Red Kite Tree Tent - Ynys Affalon   \n",
       "1    Az Alom Cabin - Treehouse Tree to Nature Cabin   \n",
       "2                       Cabane Entre Les Pins\\nüå≤üèïÔ∏èüå≤   \n",
       "3  Tree Top Cabin with log burner & private hot tub   \n",
       "4                       Treehouse near Paris Disney   \n",
       "\n",
       "                                         Description  \\\n",
       "0   Escape to one of these two fabulous Tree Tent...   \n",
       "1   Come and spend a romantic stay with a couple ...   \n",
       "2   Rustic cabin between the pines, 3 meters high...   \n",
       "3   The Tree top cabin is situated in our peacefu...   \n",
       "4   Charming cabin nestled in the leaves, real un...   \n",
       "\n",
       "                                           Amenities  \\\n",
       "0   Bathroom, Shampoo, Bedroom and laundry, Essen...   \n",
       "1   Bedroom and laundry, Essentials, Towels, bed ...   \n",
       "2   Scenic views, Garden view, River view, Bathro...   \n",
       "3   Bathroom, Hot water, Bedroom and laundry, Ess...   \n",
       "4   Bathroom, Hair dryer, Shampoo, Hot water, Sho...   \n",
       "\n",
       "                                      Location guests  beds  bathrooms  \\\n",
       "0             Llandrindod Wells United Kingdom      2   1.0        1.0   \n",
       "1                  Guyonvelle Grand Est France      3   3.0        0.0   \n",
       "2                     Duclair Normandie France      4   2.0        1.5   \n",
       "3                Barmouth Wales United Kingdom      2   1.0        1.0   \n",
       "4  Le Plessis-Feu-Aussoux √éle-de-France France      4   3.0        1.0   \n",
       "\n",
       "   Price_Night  Cleanliness_rating  Accuracy_rating Communication_rating  \\\n",
       "0          105                 4.6              4.7                  4.3   \n",
       "1           92                 4.3              4.7                  4.6   \n",
       "2           52                 4.2              4.6                  4.8   \n",
       "3          132                 4.8              4.9                  4.9   \n",
       "4          143                 5.0              4.9                  5.0   \n",
       "\n",
       "   Location_rating  Check-in_rating  Value_rating  amenities_count  \\\n",
       "0              5.0              4.3           4.3             13.0   \n",
       "1              4.9              4.7           4.5              8.0   \n",
       "2              4.8              4.8           4.7             51.0   \n",
       "3              4.9              5.0           4.6             23.0   \n",
       "4              4.7              5.0           4.7             32.0   \n",
       "\n",
       "                                                 url bedrooms  \n",
       "0  https://www.airbnb.co.uk/rooms/26620994?adults...        1  \n",
       "1  https://www.airbnb.co.uk/rooms/27055498?adults...        1  \n",
       "2  https://www.airbnb.co.uk/rooms/51427108?adults...        1  \n",
       "3  https://www.airbnb.co.uk/rooms/49543851?adults...        1  \n",
       "4  https://www.airbnb.co.uk/rooms/935398?adults=1...        2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('Unnamed: 19',axis=1)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Amenities</th>\n",
       "      <th>Location</th>\n",
       "      <th>guests</th>\n",
       "      <th>beds</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>Price_Night</th>\n",
       "      <th>Cleanliness_rating</th>\n",
       "      <th>Accuracy_rating</th>\n",
       "      <th>Communication_rating</th>\n",
       "      <th>Location_rating</th>\n",
       "      <th>Check-in_rating</th>\n",
       "      <th>Value_rating</th>\n",
       "      <th>amenities_count</th>\n",
       "      <th>url</th>\n",
       "      <th>bedrooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f9dcbd09-32ac-41d9-a0b1-fdb2793378cf</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Red Kite Tree Tent - Ynys Affalon</td>\n",
       "      <td>Escape to one of these two fabulous Tree Tent...</td>\n",
       "      <td>Bathroom, Shampoo, Bedroom and laundry, Essen...</td>\n",
       "      <td>Llandrindod Wells United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>105</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/26620994?adults...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b4736a7-e73e-45bc-a9b5-d3e7fcf652fd</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Az Alom Cabin - Treehouse Tree to Nature Cabin</td>\n",
       "      <td>Come and spend a romantic stay with a couple ...</td>\n",
       "      <td>Bedroom and laundry, Essentials, Towels, bed ...</td>\n",
       "      <td>Guyonvelle Grand Est France</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/27055498?adults...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d577bc30-2222-4bef-a35e-a9825642aec4</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Cabane Entre Les Pins\\nüå≤üèïÔ∏èüå≤</td>\n",
       "      <td>Rustic cabin between the pines, 3 meters high...</td>\n",
       "      <td>Scenic views, Garden view, River view, Bathro...</td>\n",
       "      <td>Duclair Normandie France</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>52</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>51.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/51427108?adults...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca9cbfd4-7798-4e8d-8c17-d5a64fba0abc</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Tree Top Cabin with log burner &amp; private hot tub</td>\n",
       "      <td>The Tree top cabin is situated in our peacefu...</td>\n",
       "      <td>Bathroom, Hot water, Bedroom and laundry, Ess...</td>\n",
       "      <td>Barmouth Wales United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>132</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/49543851?adults...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cfe479b9-c8f8-44af-9bc6-46ede9f14bb5</td>\n",
       "      <td>Treehouses</td>\n",
       "      <td>Treehouse near Paris Disney</td>\n",
       "      <td>Charming cabin nestled in the leaves, real un...</td>\n",
       "      <td>Bathroom, Hair dryer, Shampoo, Hot water, Sho...</td>\n",
       "      <td>Le Plessis-Feu-Aussoux √éle-de-France France</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>143</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>32.0</td>\n",
       "      <td>https://www.airbnb.co.uk/rooms/935398?adults=1...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID    Category  \\\n",
       "0  f9dcbd09-32ac-41d9-a0b1-fdb2793378cf  Treehouses   \n",
       "1  1b4736a7-e73e-45bc-a9b5-d3e7fcf652fd  Treehouses   \n",
       "2  d577bc30-2222-4bef-a35e-a9825642aec4  Treehouses   \n",
       "3  ca9cbfd4-7798-4e8d-8c17-d5a64fba0abc  Treehouses   \n",
       "4  cfe479b9-c8f8-44af-9bc6-46ede9f14bb5  Treehouses   \n",
       "\n",
       "                                              Title  \\\n",
       "0                 Red Kite Tree Tent - Ynys Affalon   \n",
       "1    Az Alom Cabin - Treehouse Tree to Nature Cabin   \n",
       "2                       Cabane Entre Les Pins\\nüå≤üèïÔ∏èüå≤   \n",
       "3  Tree Top Cabin with log burner & private hot tub   \n",
       "4                       Treehouse near Paris Disney   \n",
       "\n",
       "                                         Description  \\\n",
       "0   Escape to one of these two fabulous Tree Tent...   \n",
       "1   Come and spend a romantic stay with a couple ...   \n",
       "2   Rustic cabin between the pines, 3 meters high...   \n",
       "3   The Tree top cabin is situated in our peacefu...   \n",
       "4   Charming cabin nestled in the leaves, real un...   \n",
       "\n",
       "                                           Amenities  \\\n",
       "0   Bathroom, Shampoo, Bedroom and laundry, Essen...   \n",
       "1   Bedroom and laundry, Essentials, Towels, bed ...   \n",
       "2   Scenic views, Garden view, River view, Bathro...   \n",
       "3   Bathroom, Hot water, Bedroom and laundry, Ess...   \n",
       "4   Bathroom, Hair dryer, Shampoo, Hot water, Sho...   \n",
       "\n",
       "                                      Location guests  beds  bathrooms  \\\n",
       "0             Llandrindod Wells United Kingdom      2   1.0        1.0   \n",
       "1                  Guyonvelle Grand Est France      3   3.0        0.0   \n",
       "2                     Duclair Normandie France      4   2.0        1.5   \n",
       "3                Barmouth Wales United Kingdom      2   1.0        1.0   \n",
       "4  Le Plessis-Feu-Aussoux √éle-de-France France      4   3.0        1.0   \n",
       "\n",
       "   Price_Night  Cleanliness_rating  Accuracy_rating Communication_rating  \\\n",
       "0          105                 4.6              4.7                  4.3   \n",
       "1           92                 4.3              4.7                  4.6   \n",
       "2           52                 4.2              4.6                  4.8   \n",
       "3          132                 4.8              4.9                  4.9   \n",
       "4          143                 5.0              4.9                  5.0   \n",
       "\n",
       "   Location_rating  Check-in_rating  Value_rating  amenities_count  \\\n",
       "0              5.0              4.3           4.3             13.0   \n",
       "1              4.9              4.7           4.5              8.0   \n",
       "2              4.8              4.8           4.7             51.0   \n",
       "3              4.9              5.0           4.6             23.0   \n",
       "4              4.7              5.0           4.7             32.0   \n",
       "\n",
       "                                                 url bedrooms  \n",
       "0  https://www.airbnb.co.uk/rooms/26620994?adults...        1  \n",
       "1  https://www.airbnb.co.uk/rooms/27055498?adults...        1  \n",
       "2  https://www.airbnb.co.uk/rooms/51427108?adults...        1  \n",
       "3  https://www.airbnb.co.uk/rooms/49543851?adults...        1  \n",
       "4  https://www.airbnb.co.uk/rooms/935398?adults=1...        2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.copy()\n",
    "#making a copy to work later\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     beds  bathrooms  Price_Night  Cleanliness_rating  Accuracy_rating  \\\n",
      "0     1.0        1.0          105                 4.6              4.7   \n",
      "1     3.0        0.0           92                 4.3              4.7   \n",
      "2     2.0        1.5           52                 4.2              4.6   \n",
      "3     1.0        1.0          132                 4.8              4.9   \n",
      "4     3.0        1.0          143                 5.0              4.9   \n",
      "..    ...        ...          ...                 ...              ...   \n",
      "825   2.0        1.5          240                 4.9              5.0   \n",
      "826   1.0        1.0           78                 4.8              5.0   \n",
      "827   2.0        1.5          113                 4.8              5.0   \n",
      "828   3.0        2.0           80                 4.7              4.8   \n",
      "829   2.0        1.0          104                 4.9              4.9   \n",
      "\n",
      "    Communication_rating  Location_rating  Check-in_rating  Value_rating  \\\n",
      "0                    4.3              5.0              4.3           4.3   \n",
      "1                    4.6              4.9              4.7           4.5   \n",
      "2                    4.8              4.8              4.8           4.7   \n",
      "3                    4.9              4.9              5.0           4.6   \n",
      "4                    5.0              4.7              5.0           4.7   \n",
      "..                   ...              ...              ...           ...   \n",
      "825                  5.0              5.0              4.9           4.8   \n",
      "826                  4.9              4.9              5.0           4.9   \n",
      "827                  5.0              5.0              5.0           4.8   \n",
      "828                  5.0              5.0              5.0           4.7   \n",
      "829                  4.9              4.9              4.9           4.3   \n",
      "\n",
      "     amenities_count bedrooms  \n",
      "0               13.0        1  \n",
      "1                8.0        1  \n",
      "2               51.0        1  \n",
      "3               23.0        1  \n",
      "4               32.0        2  \n",
      "..               ...      ...  \n",
      "825             33.0        2  \n",
      "826             54.0        1  \n",
      "827             38.0        2  \n",
      "828             24.0        2  \n",
      "829             29.0        2  \n",
      "\n",
      "[830 rows x 11 columns] 0      2\n",
      "1      3\n",
      "2      4\n",
      "3      2\n",
      "4      4\n",
      "      ..\n",
      "825    4\n",
      "826    2\n",
      "827    4\n",
      "828    6\n",
      "829    4\n",
      "Name: guests, Length: 830, dtype: object\n"
     ]
    }
   ],
   "source": [
    "label = 'guests'\n",
    "features = df.drop(columns=[label,'Category','ID','Title','Description','Amenities','Location','url'])\n",
    "labels = df[label]\n",
    "\n",
    "print(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREVIOUS CODE SO FAR\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def remove_rows_with_missing_ratings(df):\n",
    "    \n",
    "    #removing rows with missing columns in rating\n",
    "    df = df.dropna(subset=['Cleanliness_rating','Accuracy_rating','Communication_rating','Location_rating','Check-in_rating','Value_rating'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def combine_description_settings(df):\n",
    "    #there seems to be a problematic row where every element of a column has shifted to the next column, fixing this manually and then moving ahead\n",
    "    problematic_row = df.loc[df['Title'] == 'Stunning Cotswolds Water Park'].index[0]\n",
    "    df.at[problematic_row, 'Title'] = 'Stunning Cotswolds Water Park, sleeps 6 with pool'\n",
    "    df.at[problematic_row, 'Description'] = df['Amenities'][problematic_row]\n",
    "    df.at[problematic_row, 'Amenities'] = df['Location'][problematic_row]\n",
    "    df.at[problematic_row, 'Location'] = df['guests'][problematic_row]\n",
    "    df.at[problematic_row, 'guests'] = df['beds'][problematic_row]\n",
    "    df.at[problematic_row, 'beds'] = df['bathrooms'][problematic_row]\n",
    "    df.at[problematic_row, 'bathrooms'] = df['Price_Night'][problematic_row]\n",
    "    df.at[problematic_row, 'Price_Night'] = df['Cleanliness_rating'][problematic_row]\n",
    "    df.at[problematic_row, 'Cleanliness_rating'] = df['Accuracy_rating'][problematic_row]\n",
    "    df.at[problematic_row, 'Accuracy_rating'] = df['Communication_rating'][problematic_row]\n",
    "    df.at[problematic_row, 'Communication_rating'] = df['bedrooms'][problematic_row]\n",
    "    df.at[problematic_row, 'bedrooms'] = df['Unnamed: 19'][problematic_row]\n",
    "\n",
    "\n",
    "\n",
    "    # Dropping rows where description is empty\n",
    "    df = df.dropna(subset=['Description'])\n",
    "    # Remove \"About this space\" prefix\n",
    "    df['Description'] = df['Description'].apply(lambda x: x.replace('About this space', '').strip() if isinstance(x, str) else x)\n",
    "    # Combine list items into a single string\n",
    "    df['Description'] = df['Description'].apply(lambda x: ' '.join(['' if v == '' else v.strip() for v in ast.literal_eval(x) if isinstance(v, str)]))\n",
    "    # Remove \"What this Place Offers\" prefix\n",
    "    df['Amenities'] = df['Amenities'].apply(lambda x: x.replace('What this place offers', '').strip() if isinstance(x, str) else x)\n",
    "    # Combine list items into a single string\n",
    "    df['Amenities'] = df['Amenities'].apply(lambda x: ', '.join(['' if v == '' else v.strip() for v in ast.literal_eval(x) if isinstance(v, str)]))\n",
    "    # Removing the first comma\n",
    "    df['Amenities'] = df['Amenities'].str.replace(',', '', 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def set_default_feature_values(df):\n",
    "        \n",
    "    #Set default feature values for empty entries in the guests, beds, bathrooms, and bedrooms columns.\n",
    "    df[\"guests\"].fillna(1, inplace=True)\n",
    "    df[\"beds\"].fillna(1, inplace=True)\n",
    "    df[\"bathrooms\"].fillna(1, inplace=True)\n",
    "    df[\"bedrooms\"].fillna(1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def cleaning_the_data_properly(df):\n",
    "    pass\n",
    "\n",
    "\n",
    "def clean_tabular_data(df):\n",
    "\n",
    "    df1 = remove_rows_with_missing_ratings(df)\n",
    "    df2 = combine_description_settings(df1)\n",
    "    df3 = set_default_feature_values(df2)\n",
    "    df3 = df3.reset_index(drop=True)\n",
    "    return df3\n",
    "\n",
    "\n",
    "def load_airbnb(df,label):\n",
    "\n",
    "    labels = df[label]\n",
    "    features = df.drop(columns=[label,'Category','ID','Title','Description','Amenities','Location','url'])\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "\n",
    "    #df = pd.read_csv(\"C:/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnb's_property_listing_dataset/airbnb-property-listing/tabular_data/listing.csv\")\n",
    "    #clean_df = clean_tabular_data(df)\n",
    "    #clean_df.to_csv(\"C:/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnb's_property_listing_dataset/airbnb-property-listing/tabular_data/clean_listing.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2 Completed\n",
    "## use df1 from this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                       object\n",
      "Category                 object\n",
      "Title                    object\n",
      "Description              object\n",
      "Amenities                object\n",
      "Location                 object\n",
      "guests                   object\n",
      "beds                    float64\n",
      "bathrooms               float64\n",
      "Price_Night               int64\n",
      "Cleanliness_rating      float64\n",
      "Accuracy_rating         float64\n",
      "Communication_rating     object\n",
      "Location_rating         float64\n",
      "Check-in_rating         float64\n",
      "Value_rating            float64\n",
      "amenities_count         float64\n",
      "url                      object\n",
      "bedrooms                 object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ID                       string\n",
       "Category                 string\n",
       "Title                    string\n",
       "Description              string\n",
       "Amenities                string\n",
       "Location                 string\n",
       "guests                   object\n",
       "beds                      Int64\n",
       "bathrooms               Float64\n",
       "Price_Night               Int64\n",
       "Cleanliness_rating      Float64\n",
       "Accuracy_rating         Float64\n",
       "Communication_rating     object\n",
       "Location_rating         Float64\n",
       "Check-in_rating         Float64\n",
       "Value_rating            Float64\n",
       "amenities_count         Float64\n",
       "url                      string\n",
       "bedrooms                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df1.dtypes)\n",
    "df1 = df1.convert_dtypes()\n",
    "df1.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df1['Communication_rating'].unique()\\ndf1.loc[df1['Communication_rating'] == 'https://www.airbnb.co.uk/rooms/49009981?adults=1&category_tag=Tag%3A677&children=0&infants=0&search_mode=flex_destinations_search&check_in=2022-04-18&check_out=2022-04-25&previous_page_section_name=1000&federated_search_id=0b044c1c-8d17-4b03-bffb-5de13ff710bc'].index\\ndf1.loc[498]\\n# Interchange values in row 2 (index 1) between column1 and column2\\ntemp_value = df.at[498, 'Communication_rating']  \\ndf1.at[498, 'Communication_rating'] = df.at[498, 'url']  \\ndf1.at[498, 'url'] = temp_value \""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df1['Communication_rating'].unique()\n",
    "df1.loc[df1['Communication_rating'] == 'https://www.airbnb.co.uk/rooms/49009981?adults=1&category_tag=Tag%3A677&children=0&infants=0&search_mode=flex_destinations_search&check_in=2022-04-18&check_out=2022-04-25&previous_page_section_name=1000&federated_search_id=0b044c1c-8d17-4b03-bffb-5de13ff710bc'].index\n",
    "df1.loc[498]\n",
    "# Interchange values in row 2 (index 1) between column1 and column2\n",
    "temp_value = df.at[498, 'Communication_rating']  \n",
    "df1.at[498, 'Communication_rating'] = df.at[498, 'url']  \n",
    "df1.at[498, 'url'] = temp_value \"\"\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "830"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1['Communication_rating'].unique())\n",
    "len(df1['url'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      3\n",
       "2      4\n",
       "3      2\n",
       "4      4\n",
       "      ..\n",
       "825    4\n",
       "826    2\n",
       "827    4\n",
       "828    6\n",
       "829    4\n",
       "Name: guests, Length: 830, dtype: Int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['guests'].unique()\n",
    "df1['guests'] = pd.to_numeric(df1['guests'],errors='coerce')\n",
    "df1['guests'] = df1['guests'].astype('Int64')\n",
    "df1['guests']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IntegerArray>\n",
       "[1, 2, 5, 3, 4, 8, 6, 10, 7]\n",
       "Length: 9, dtype: Int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df1['bedrooms'] = pd.to_numeric(df1['bedrooms'],errors='coerce')\n",
    "df1['bedrooms'] = df1['bedrooms'].astype(\"Int64\")\n",
    "(df1['bedrooms'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.3, 4.6, 4.8, 4.9, 5.0, 4.4, 4.7, 4.5, 3.9, 4.2, 4.0,\n",
       "       'https://www.airbnb.co.uk/rooms/49009981?adults=1&category_tag=Tag%3A677&children=0&infants=0&search_mode=flex_destinations_search&check_in=2022-04-18&check_out=2022-04-25&previous_page_section_name=1000&federated_search_id=0b044c1c-8d17-4b03-bffb-5de13ff710bc'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Communication_rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ID                       string\n",
       "Category                 string\n",
       "Title                    string\n",
       "Description              string\n",
       "Amenities                string\n",
       "Location                 string\n",
       "guests                    Int64\n",
       "beds                      Int64\n",
       "bathrooms               Float64\n",
       "Price_Night               Int64\n",
       "Cleanliness_rating      Float64\n",
       "Accuracy_rating         Float64\n",
       "Communication_rating    Float64\n",
       "Location_rating         Float64\n",
       "Check-in_rating         Float64\n",
       "Value_rating            Float64\n",
       "amenities_count         Float64\n",
       "url                      string\n",
       "bedrooms                  Int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df1.convert_dtypes()\n",
    "#upon further investigation there seems to be a row where a value has been interchanged between url and communication_rating\n",
    "problematic_row = df1.loc[df1['url']=='46'].index[0]\n",
    "print(problematic_row)\n",
    "temp_value = df1.at[problematic_row, 'Communication_rating']  \n",
    "df1.at[problematic_row, 'Communication_rating'] = df1.at[problematic_row, 'url']  \n",
    "df1.at[problematic_row, 'url'] = temp_value\n",
    "#however the problem is still not fixed as the value in communication_rating is '46' in the string format but we can\n",
    "#take a reasonable guess and safely say that the rating is 4.6 \n",
    "df1.at[problematic_row, 'Communication_rating'] = 4.6\n",
    "# we can now safely convert the datatype to float\n",
    "df1['Communication_rating'] = df1['Communication_rating'].astype('Float64')\n",
    "\n",
    "# fixing the column 'guests'\n",
    "df1['guests'] = pd.to_numeric(df1['guests'],errors='coerce')\n",
    "df1['guests'] = df1['guests'].astype('Int64')\n",
    "df1['guests']\n",
    "\n",
    "# fixing the column 'bedrooms'\n",
    "df1['bedrooms'] = pd.to_numeric(df1['bedrooms'],errors='coerce')\n",
    "df1['bedrooms'] = df1['bedrooms'].astype(\"Int64\")\n",
    "\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df1 is now clean. \n",
    "### Milestone 4 task 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (664, 11)\n",
      "Training Label shape: (664,)\n",
      "Training RMSE: 1859439085.9402883\n",
      "Test RMSE: 1846233053.6606545\n",
      "Training R-squared (R2): -216745173790919.88\n",
      "Test R-squared (R2): -128225534799964.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def load_airbnb(df, label):\n",
    "    \"\"\"\n",
    "    Extract features and labels from the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        label (str): The name of the column representing the label.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame, Series: Features DataFrame and labels Series.\n",
    "    \"\"\"\n",
    "    labels = df[label]\n",
    "    # Drop irrelevant columns (e.g., 'Category', 'ID', 'Title', 'Description', 'Amenities', 'Location', 'url') to get the features DataFrame\n",
    "    features = df.drop(columns=[label, 'Category', 'ID', 'Title', 'Description', 'Amenities', 'Location', 'url'])\n",
    "    return features, labels\n",
    "\n",
    "# loading the clean dataset as features and labels\n",
    "X, y = load_airbnb(df1, label=\"Price_Night\")\n",
    "\n",
    "#splitting dataset into training set and testing set 80:20 ratio and then 50:50 into testing and validation sets\n",
    "#respectively for cross-validation purposes later on in model evaluation process\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test,y_test,test_size=0.5,random_state=42)\n",
    "\n",
    "#print(X_train.sum(), X_test.sum(),X_val.sum(), y_train, y_test,y_val)\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Training Label shape:\", y_train.shape)\n",
    "\n",
    "model = SGDRegressor(random_state=42) #creating an instance of linear regression algorithm\n",
    "model.fit(X_train, y_train)#fitting our training set\n",
    "#y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "#msevalue = mean_squared_error(y_test, y_pred)\n",
    "#r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "#print('msevalue: ',msevalue)\n",
    "#print('r2: ',r2)\n",
    "\n",
    "#the outputs at this point suggest that there might be some issues with modelling\n",
    "#extreme high values of mse and negative r-squared suggest that model is doing poorly and not fitting well\n",
    "\n",
    "# Make predictions on the training and test sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE for training and test sets\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "\n",
    "# Compute R^2 (coefficient of determination) for training and test sets\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training RMSE:\", train_rmse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Training R-squared (R2):\", train_r2)\n",
    "print(\"Test R-squared (R2):\", test_r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 1859439085.9402883\n",
      "Test RMSE: 1846233053.6606545\n",
      "Training R-squared (R2): -216745173790919.88\n",
      "Test R-squared (R2): -128225534799964.14\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the training and test sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE for training and test sets\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "test_rmse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "# Compute R^2 (coefficient of determination) for training and test sets\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training RMSE:\", train_rmse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Training R-squared (R2):\", train_r2)\n",
    "print(\"Test R-squared (R2):\", test_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools  \n",
    "def custom_tune_regression_model_hyperparameters(model_class, X_train, y_train, X_val, y_val, X_test, y_test, hyperparameters):\n",
    "    best_model = None\n",
    "    best_hyperparameters = {}\n",
    "    best_performance_metrics = {'validation_rmse':np.nan}\n",
    "    best_rmse = float('inf')\n",
    "\n",
    "    for hyperparam_values in itertools.product(*hyperparameters.values()):\n",
    "\n",
    "        hyperparam_dict = dict(zip(hyperparameters.keys(), hyperparam_values))\n",
    "        #make an instance of the model class\n",
    "        model = model_class(**hyperparam_dict)\n",
    "        #fit the training data into the model\n",
    "        model.fit(X_train,y_train)\n",
    "        #making prediction on the validation sets\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        #calculating rmse for validation set\n",
    "        val_rmse = mean_squared_error(y_val,y_val_pred,squared=False)\n",
    "        if (val_rmse < best_rmse):\n",
    "            best_model = model\n",
    "            best_hyperparameters = hyperparam_dict\n",
    "            best_rmse = val_rmse\n",
    "    \n",
    "    best_model.fit(pd.concat([X_train,X_val]),pd.concat([y_train, y_val]))\n",
    "    final_val_pred = best_model.predict(X_val)\n",
    "    final_val_rmse = mean_squared_error(y_val,final_val_pred,squared=False)\n",
    "\n",
    "    final_test_pred = best_model.predict(X_test)\n",
    "    final_test_rmse = mean_squared_error(y_test,final_test_pred,squared=False)\n",
    "\n",
    "    best_performance_metrics = {'validation_rmse':final_val_rmse,'test_rmse':final_test_rmse}\n",
    "\n",
    "    return best_model,best_hyperparameters,best_performance_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'alpha': 1.0, 'max_iter': 3000, 'tol': 1e-05}\n",
      "Validation RMSE for Best Model: 21210038.276614416\n",
      "Test RMSE for Best Model: 25584343.785515755\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters to search over\n",
    "hyperparameters_to_search = {\n",
    "    \"alpha\": [0.001, 0.01, 0.1, 1.0],\n",
    "    \"max_iter\": [1000, 2000, 3000],\n",
    "    \"tol\": [1e-3, 1e-4, 1e-5]\n",
    "}\n",
    "\n",
    "# Call the custom_tune_regression_model_hyperparameters function\n",
    "best_model, best_hyperparameters, performance_metrics = custom_tune_regression_model_hyperparameters(\n",
    "    model_class=SGDRegressor,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_val=X_val, y_val=y_val,\n",
    "    X_test=X_test, y_test=y_test,\n",
    "    hyperparameters=hyperparameters_to_search\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "print(\"Validation RMSE for Best Model:\", performance_metrics[\"validation_rmse\"])\n",
    "print(\"Test RMSE for Best Model:\", performance_metrics[\"test_rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#m4t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best Hyperparameters: {'alpha': 1.0, 'max_iter': 1000, 'tol': 0.001}\n",
      "Best Validation RMSE: 789551567.4131179\n",
      "percentage improvement: -59.24988062810971\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def tune_regression_model_hyperparameters(model_class,X_train,y_train,hyperparameters):\n",
    "    model = model_class(random_state=42)\n",
    "    gridsearch = GridSearchCV(estimator=model,param_grid=hyperparameters,cv=5, scoring='neg_root_mean_squared_error', verbose=1)\n",
    "    gridsearch.fit(X_train,y_train)\n",
    "    return gridsearch\n",
    "\n",
    "hyperparameters_to_search = {\n",
    "    \"alpha\": [0.001, 0.01, 0.1, 1.0],\n",
    "    \"max_iter\": [1000, 2000, 3000],\n",
    "    \"tol\": [1e-3, 1e-4, 1e-5]\n",
    "}\n",
    "\n",
    "# Call the tune_regression_model_hyperparameters function\n",
    "grid_search = tune_regression_model_hyperparameters(\n",
    "    model_class=SGDRegressor,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    hyperparameters=hyperparameters_to_search\n",
    ")\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Validation RMSE:\", -grid_search.best_score_)\n",
    "y_val_pred = model.predict(X_val)\n",
    "val_rmse = mean_squared_error(y_val_pred,y_val,squared=False)\n",
    "percentimprovement = (-grid_search.best_score_ - val_rmse)/val_rmse\n",
    "print('percentage improvement:',percentimprovement*100  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now you need to save the model.\n",
    "\n",
    "- Create a folder called models.\n",
    "\n",
    "- Within your models folder, create a folder called regression to save your regression models and their metrics in.\n",
    "\n",
    "- Define a function called save_model which saves the model in a file called model.joblib, its hyperparameters in a file called hyperparameters.json, and its performance metrics in a file called metrics.json once it's trained and tuned.\n",
    "\n",
    "- The function should take in the name of a folder where these files should be saved as a keyword argument \"folder\". In this case, set that argument equal to models/regression/linear_regression.\n",
    "\n",
    "- Note: If you're on windows, the path separator will be different. Use Python's os library to avoid having to write the separator explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import json\n",
    "\n",
    "def save_model(model,hyperparametrics,performance_metrics,folder='models/regression/linear_regression'):\n",
    "\n",
    "    \"\"\"\n",
    "    Saves the trained regression model, its hyperparameters and performance metrics in a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        model: The trained regression model.\n",
    "        hyperparametrics (dict): dictionary containing all of the parameters used for training.\n",
    "        performance_metrics (dict): dictionary containing all the performance metrics.\n",
    "        folder (str): The folders where the file will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # create folder if it doesn't exist\n",
    "    os.makedirs(folder,exist_ok=True)\n",
    "\n",
    "    # save model using joblib\n",
    "    model_name = os.path.join(folder,'model.joblib')\n",
    "    joblib.dump(model, model_name)\n",
    "\n",
    "    # save hyperparameters to a json file\n",
    "    hyperparametrics_name = os.path.join(folder,'hyperparametrics.json')\n",
    "    with open(hyperparametrics_name, 'w') as f:\n",
    "        json.dump(hyperparametrics,f,indent=4)\n",
    "\n",
    "    # save performance_metrics to a json file\n",
    "    performance_metrics_name = os.path.join(folder,'performance_metrics.json')\n",
    "    with open(performance_metrics_name, 'w') as f:\n",
    "        json.dump(performance_metrics,f,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model, hyperparameters, and metrics\n",
    "save_model(best_model, grid_search.best_params_, -grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m4t6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LinearRegression...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "SGDRegressor(alpha=1.0, max_iter=500, random_state=42, tol=0.01)\n",
      "{'alpha': 1.0, 'max_iter': 500, 'tol': 0.01}\n",
      "val rmse original 1937544182.895709\n",
      "789551567.4131179\n",
      "percentage improvement: -59.24988062810971\n",
      "\n",
      "\n",
      "Evaluating DecisionTree...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "DecisionTreeRegressor(max_depth=1, min_samples_leaf=10, random_state=42)\n",
      "{'max_depth': 1, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "val rmse original 1937544182.895709\n",
      "103.24955313838775\n",
      "percentage improvement: -99.99999467111232\n",
      "\n",
      "\n",
      "Evaluating RandomForest...\n",
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
      "RandomForestRegressor(max_depth=5, min_samples_leaf=6, n_estimators=50,\n",
      "                      random_state=42)\n",
      "{'max_depth': 5, 'min_samples_leaf': 6, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "val rmse original 1937544182.895709\n",
      "100.31327166353587\n",
      "percentage improvement: -99.99999482265888\n",
      "\n",
      "\n",
      "Evaluating GradientBoosting...\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "GradientBoostingRegressor(learning_rate=0.01, n_estimators=150, random_state=42)\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 150}\n",
      "val rmse original 1937544182.895709\n",
      "102.97262689917568\n",
      "percentage improvement: -99.99999468540497\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabular_data import load_airbnb\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import itertools\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "def evaluate_all_models(val_rmse=val_rmse):\n",
    "    \"\"\"\n",
    "    Evaluate different regression models using hyperparameter tuning and save results.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load data and split\n",
    "    #df = pd.read_csv(\"C:/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/airbnb-property-listing/tabular_data/clean_listing.csv\")\n",
    "    #X, y = load_airbnb(df, label=\"Price_Night\")\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    #X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    # List of models to evaluate\n",
    "    models_to_evaluate = [\n",
    "        (\"LinearRegression\", SGDRegressor, {\n",
    "            \"alpha\": [0.0001 ,0.001, 0.01, 0.1, 1.0],\n",
    "            \"max_iter\": [500, 1000, 2000, 3000, 4000],\n",
    "            \"tol\": [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "        }),\n",
    "        (\"DecisionTree\", DecisionTreeRegressor, {\n",
    "            \"max_depth\": [None, 5, 1, 2],\n",
    "            \"min_samples_split\": [2, 5, 7, 1],\n",
    "            \"min_samples_leaf\": [ 10, 8, 6]\n",
    "        }),\n",
    "        (\"RandomForest\", RandomForestRegressor, {\n",
    "            \"n_estimators\": [50, 100, 200, 300],\n",
    "            \"max_depth\": [None, 5, 10, 15],\n",
    "            \"min_samples_split\": [2, 5, 7, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 4, 6]\n",
    "        }),\n",
    "        (\"GradientBoosting\", GradientBoostingRegressor, {\n",
    "            \"n_estimators\": [50, 100, 150, 200],\n",
    "            \"max_depth\": [3, 5, 7, 9],\n",
    "            \"learning_rate\": [0.01, 0.1, 0.2, 0.3]\n",
    "        })\n",
    "    ]\n",
    "\n",
    "    for model_name, model_class, hyperparameters in models_to_evaluate:\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        gridsearch = tune_regression_model_hyperparameters(model_class, X_train, y_train, hyperparameters)\n",
    "        best_model = gridsearch.best_estimator_\n",
    "        best_hyperparameters = gridsearch.best_params_\n",
    "        print(best_model)\n",
    "        print(best_hyperparameters)\n",
    "        print(f'val rmse original {val_rmse}')\n",
    "        print(-gridsearch.best_score_)\n",
    "        percentimprovement = (-gridsearch.best_score_ - val_rmse)/val_rmse\n",
    "        print('percentage improvement:',percentimprovement*100  )\n",
    "        print('\\n')\n",
    "\n",
    "        # Evaluate on validation and test sets\n",
    "        # final_model, best_hyperparameters, performance_metrics = custom_tune_regression_model_hyperparameters(\n",
    "        #    best_model, X_train, y_train, X_val, y_val, X_test, y_test, best_hyperparameters\n",
    "        \n",
    "        \n",
    "        # Save model, hyperparameters, and metrics\n",
    "        #model_folder = f\"models/regression/{model_name.lower()}\"\n",
    "        #save_model(final_model, best_hyperparameters, performance_metrics, folder=model_folder)\n",
    "        #print(f\"{model_name} evaluation complete.\")\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    evaluate_all_models()\n",
    "evaluate_all_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_to_search = {\n",
    "    \"alpha\": [0.001, 0.01, 0.1, 1.0],\n",
    "    \"max_iter\": [1000, 2000, 3000],\n",
    "    \"tol\": [1e-3, 1e-4, 1e-5]\n",
    "}\n",
    "\n",
    "# Call the tune_regression_model_hyperparameters function\n",
    "grid_search = tune_regression_model_hyperparameters(\n",
    "    model_class=SGDRegressor,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    hyperparameters=hyperparameters_to_search\n",
    ")\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Validation RMSE:\", -grid_search.best_score_)\n",
    "y_val_pred = model.predict(X_val)\n",
    "val_rmse = mean_squared_error(y_val_pred,y_val,squared=False)\n",
    "percentimprovement = (-grid_search.best_score_ - val_rmse)/val_rmse\n",
    "print('percentage improvement:',percentimprovement*100  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LinearRegression...\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "{'alpha': 0.1, 'max_iter': 1000, 'tol': 1}\n",
      "val rmse original 1937544182.895709\n",
      "51652833.52606116\n",
      "percentage improvement: -97.33410809508021\n",
      "\n",
      "\n",
      "LinearRegression evaluation complete.\n",
      "Evaluating DecisionTree...\n",
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n",
      "{'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 0.1}\n",
      "val rmse original 1937544182.895709\n",
      "99.88793428346632\n",
      "percentage improvement: -99.99999484461128\n",
      "\n",
      "\n",
      "DecisionTree evaluation complete.\n",
      "Evaluating RandomForest...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "{'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 1, 'n_estimators': 75}\n",
      "val rmse original 1937544182.895709\n",
      "97.75541408726694\n",
      "percentage improvement: -99.99999495467434\n",
      "\n",
      "\n",
      "RandomForest evaluation complete.\n",
      "Evaluating GradientBoosting...\n",
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 200}\n",
      "val rmse original 1937544182.895709\n",
      "99.18367510190775\n",
      "percentage improvement: -99.9999948809593\n",
      "\n",
      "\n",
      "GradientBoosting evaluation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "120 fits failed out of a total of 480.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "120 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 420, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_depth' parameter of GradientBoostingRegressor must be an int in the range [1, inf) or None. Got 0.1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [          nan           nan           nan -122.5320832  -122.33483255\n",
      " -122.1392936  -122.44953108 -122.21231898 -121.97769915 -122.35376561\n",
      " -122.06989268 -121.7902857            nan           nan           nan\n",
      " -119.23208688 -117.61489143 -116.13778874 -118.57409703 -116.76073944\n",
      " -115.1689333  -117.92073824 -115.97378855 -114.24931398           nan\n",
      "           nan           nan -105.60472767 -103.7162892  -102.58948555\n",
      " -103.40393748 -102.91153762 -103.21977074 -105.97905156 -105.91224656\n",
      " -105.93035393           nan           nan           nan  -99.74260682\n",
      "  -99.37470947  -99.1836751  -105.04162408 -107.16842975 -107.83074442\n",
      " -109.88038416 -111.38953134 -112.72812368]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anany\\OneDrive\\Desktop\\Github\\AIcore\\Modelling_Airbnbs_property_listing_dataset\\test_airbnb.ipynb Cell 40\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/test_airbnb.ipynb#X54sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     best_model_name \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(best_models, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m model: best_models[model][\u001b[39m\"\u001b[39m\u001b[39mperformance_metrics\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mvalidation_RMSE\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/test_airbnb.ipynb#X54sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (best_models[best_model_name][\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], best_models[best_model_name][\u001b[39m\"\u001b[39m\u001b[39mhyperparameters\u001b[39m\u001b[39m\"\u001b[39m], best_models[best_model_name][\u001b[39m\"\u001b[39m\u001b[39mperformance_metrics\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/test_airbnb.ipynb#X54sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m best_model, best_hyperparameters, best_performance_metrics \u001b[39m=\u001b[39m find_best_model()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/test_airbnb.ipynb#X54sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Model:\u001b[39m\u001b[39m\"\u001b[39m, best_model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/test_airbnb.ipynb#X54sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Hyperparameters:\u001b[39m\u001b[39m\"\u001b[39m, best_hyperparameters)\n",
      "\u001b[1;32mc:\\Users\\Anany\\OneDrive\\Desktop\\Github\\AIcore\\Modelling_Airbnbs_property_listing_dataset\\test_airbnb.ipynb Cell 40\u001b[0m in \u001b[0;36mfind_best_model\u001b[1;34m(val_rmse)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/test_airbnb.ipynb#X54sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m evaluation complete.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/test_airbnb.ipynb#X54sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Find the best model based on validation RMSE\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/test_airbnb.ipynb#X54sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m best_model_name \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39;49m(best_models, key\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m model: best_models[model][\u001b[39m\"\u001b[39;49m\u001b[39mperformance_metrics\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mvalidation_RMSE\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/test_airbnb.ipynb#X54sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mreturn\u001b[39;00m (best_models[best_model_name][\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], best_models[best_model_name][\u001b[39m\"\u001b[39m\u001b[39mhyperparameters\u001b[39m\u001b[39m\"\u001b[39m], best_models[best_model_name][\u001b[39m\"\u001b[39m\u001b[39mperformance_metrics\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;32mc:\\Users\\Anany\\OneDrive\\Desktop\\Github\\AIcore\\Modelling_Airbnbs_property_listing_dataset\\test_airbnb.ipynb Cell 40\u001b[0m in \u001b[0;36mfind_best_model.<locals>.<lambda>\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/test_airbnb.ipynb#X54sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m evaluation complete.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/test_airbnb.ipynb#X54sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Find the best model based on validation RMSE\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/test_airbnb.ipynb#X54sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m best_model_name \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(best_models, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m model: best_models[model][\u001b[39m\"\u001b[39;49m\u001b[39mperformance_metrics\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mvalidation_RMSE\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/test_airbnb.ipynb#X54sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mreturn\u001b[39;00m (best_models[best_model_name][\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], best_models[best_model_name][\u001b[39m\"\u001b[39m\u001b[39mhyperparameters\u001b[39m\u001b[39m\"\u001b[39m], best_models[best_model_name][\u001b[39m\"\u001b[39m\u001b[39mperformance_metrics\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "def find_best_model(val_rmse=val_rmse):\n",
    "    best_models = {}\n",
    "    models_to_evaluate = [\n",
    "        (\"LinearRegression\", SGDRegressor, {\n",
    "            \"alpha\": [0.001, 0.01, 0.1, 1.0, 10],\n",
    "            \"max_iter\": [1000,2000,5000],\n",
    "            \"tol\": [1, 0.1 ,1e-2, 1e-3]\n",
    "            #{'alpha': 0.1, 'max_iter': 500, 'tol': 0.01}\n",
    "        }),\n",
    "        (\"DecisionTree\", DecisionTreeRegressor, {\n",
    "            \"max_depth\": [None,  1, 5, 10],\n",
    "            \"min_samples_split\": [ 0.1, 1, 6, 10],\n",
    "            \"min_samples_leaf\": [ 10, 8, 6]\n",
    "            # {'max_depth': 5, 'min_samples_leaf': 6, 'min_samples_split': 2}\n",
    "        }),\n",
    "        (\"RandomForest\", RandomForestRegressor, {\n",
    "            \"n_estimators\": [75, 100, 150],\n",
    "            \"max_depth\": [None, 5, 10, 15],\n",
    "            \"min_samples_split\": [1,2,10],\n",
    "            \"min_samples_leaf\": [1, 6, 8, 10]\n",
    "            #{'max_depth': 10, 'min_samples_leaf': 6, 'min_samples_split': 2, 'n_estimators': 100}\n",
    "        }),\n",
    "        (\"GradientBoosting\", GradientBoostingRegressor, {\n",
    "            \"n_estimators\": [100, 150, 200],\n",
    "            \"max_depth\": [0.1, 1, 3, 5],\n",
    "            \"learning_rate\": [0.0001, 0.001, 0.01, 0.1,]\n",
    "            # {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 150}\n",
    "        })\n",
    "    ]\n",
    "\n",
    "    for model_name, model_class, hyperparameters in models_to_evaluate:\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        gridsearch = tune_regression_model_hyperparameters(model_class, X_train, y_train, hyperparameters)\n",
    "        best_model = gridsearch.best_estimator_\n",
    "        best_hyperparameters = gridsearch.best_params_\n",
    "        print(best_hyperparameters)\n",
    "        print(f'val rmse original {val_rmse}')\n",
    "        print(-gridsearch.best_score_)\n",
    "        percentimprovement = (-gridsearch.best_score_ - val_rmse)/val_rmse\n",
    "        print('percentage improvement:',percentimprovement*100  )\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "        # Save model, hyperparameters, and metrics\n",
    "        best_models[model_name] = {\n",
    "            \"model\": best_model,\n",
    "            \"hyperparameters\": best_hyperparameters,\n",
    "            \"performance_metrics\": gridsearch.best_score_\n",
    "        }\n",
    "        print(f\"{model_name} evaluation complete.\")\n",
    "\n",
    "    # Find the best model based on validation RMSE\n",
    "    best_model_name = min(best_models, key=lambda model: best_models[model][\"performance_metrics\"][\"validation_RMSE\"])\n",
    "\n",
    "    return (best_models[best_model_name][\"model\"], best_models[best_model_name][\"hyperparameters\"], best_models[best_model_name][\"performance_metrics\"])\n",
    "\n",
    "best_model, best_hyperparameters, best_performance_metrics = find_best_model()\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best Performance Metrics:\", best_performance_metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tabular_data import load_airbnb\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import itertools\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from tabular_data import load_airbnb, clean_df\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import itertools\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "\n",
    "def split_X_y(X,y):\n",
    "\n",
    "    # splitting dataset into training set and testing set 80:20 ratio and then 50:50 into testing and validation sets\n",
    "    # respectively for cross-validation purposes later on in model evaluation process\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "    # printing the shapes of features and labels\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Training Label shape: {y_train.shape}\")\n",
    "    print(\"Number of samples in:\")\n",
    "    print(f\"    Training:   {len(y_train)}\")\n",
    "    print(f\"    Testing:    {len(y_test)}\")\n",
    "    print(f\"    Validation:   {len(y_val)}\")\n",
    "    return X_train,y_train,X_test,y_test,X_val,y_val\n",
    "\n",
    "\n",
    "def train_regression_model(X_train,y_train,X_test,y_test,modelclass=SGDRegressor):\n",
    "    \n",
    "\n",
    "    # create an instance of linear regression algorithm\n",
    "    # the class SGDRegressor is already the default class in the func, we'll just use it\n",
    "    model = modelclass(random_state=42)\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    # Make predictions on the training and test sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute RMSE for training and test sets\n",
    "    train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "    test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "\n",
    "    # Compute R^2 (coefficient of determination) for training and test sets\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Initial Training RMSE: {train_rmse} | Initial Training R-squared (R2): {train_r2}\")\n",
    "    print(f\"Initial Test RMSE: {test_rmse} | Initial Test R-squared (R2): {test_r2}\")\n",
    "    # the outputs at this point suggest that there might be some issues with modelling\n",
    "    # extreme high values of mse and negative r-squared suggest that the model is doing poorly and not fitting well\n",
    "\n",
    "\n",
    "def custom_tune_regression_model_hyperparameters(\n",
    "    model_class: type,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    hyperparameters: Dict[str, List]\n",
    ") -> Tuple[Any, Dict[str, Any], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Perform a grid search over a range of hyperparameter values for a given regression model.\n",
    "\n",
    "    Parameters:\n",
    "        model_class (class): The class of the regression model.\n",
    "        X_train (DataFrame): Training features.\n",
    "        y_train (Series): Training labels.\n",
    "        X_val (DataFrame): Validation features.\n",
    "        y_val (Series): Validation labels.\n",
    "        X_test (DataFrame): Test features.\n",
    "        y_test (Series): Test labels.\n",
    "        hyperparameters (dict): A dictionary of hyperparameter names mapping to a list of values to be tried.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - best_model: The best regression model.\n",
    "            - best_hyperparameters (dict): The best hyperparameters found during the grid search.\n",
    "            - performance_metrics (dict): A dictionary of performance metrics, including \"validation_RMSE\" and \"test_RMSE\".\n",
    "    \"\"\"\n",
    "    best_model = None\n",
    "    best_hyperparameters = {}\n",
    "    best_val_rmse = float(\"inf\")\n",
    "\n",
    "    # Iterate through all combinations of hyperparameter values\n",
    "    for hyperparam_values in itertools.product(*hyperparameters.values()):\n",
    "        hyperparam_dict = dict(zip(hyperparameters.keys(), hyperparam_values))\n",
    "        model = model_class(**hyperparam_dict, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "\n",
    "        # Update best model if validation RMSE improves\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_model = model\n",
    "            best_hyperparameters = hyperparam_dict\n",
    "            best_val_rmse = val_rmse\n",
    "\n",
    "    # Train the best model on the full training and validation sets\n",
    "    best_model.fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))\n",
    "\n",
    "    # Calculate final validation RMSE\n",
    "    final_val_pred = best_model.predict(X_val)\n",
    "    final_val_rmse = mean_squared_error(y_val, final_val_pred, squared=False)\n",
    "\n",
    "    # Calculate final test RMSE\n",
    "    final_test_pred = best_model.predict(X_test)\n",
    "    final_test_rmse = mean_squared_error(y_test, final_test_pred, squared=False)\n",
    "\n",
    "    # Store performance metrics\n",
    "    performance_metrics = {\n",
    "        \"validation_RMSE\": final_val_rmse,\n",
    "        \"test_RMSE\": final_test_rmse\n",
    "    }\n",
    "\n",
    "    return best_model, best_hyperparameters, performance_metrics\n",
    "\n",
    "\n",
    "def tune_regression_model_hyperparameters(\n",
    "    model_class: type,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val : pd.DataFrame,\n",
    "    y_val   : pd.Series,\n",
    "    hyperparameters: List[Tuple[str, List]],\n",
    "    cv_folds: int = 5\n",
    ") -> GridSearchCV:\n",
    "    \"\"\"\n",
    "    Perform a grid search over a range of hyperparameters for a given regression model using GridSearchCV.\n",
    "\n",
    "    Parameters:\n",
    "        model_class (class): The class of the regression model.\n",
    "        X_train (pd.DataFrame): Training features with shape [n_samples, n_features].\n",
    "        y_train (pd.Series): Training labels with shape [n_samples].\n",
    "        hyperparameters (list of tuples): List of tuples containing name-value pairs of hyperparameters to search.\n",
    "        cv_folds (int): Number of cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "        GridSearchCV: A grid search object containing the best estimator and hyperparameters.\n",
    "    \"\"\"\n",
    "    performance_metrics = {}\n",
    "    model = model_class(random_state=42)\n",
    "    gridsearch = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=dict(hyperparameters),\n",
    "        cv=cv_folds,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        verbose=1\n",
    "    )\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    best_model = gridsearch.best_estimator_\n",
    "    best_hyperparameters = gridsearch.best_params_\n",
    "\n",
    "    y_val_pred = gridsearch.best_estimator_.predict(X_val)\n",
    "    val_rmse = mean_squared_error(y_val,y_val_pred,squared=False)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"{model_class} best hyperparameters: {best_hyperparameters}\")\n",
    "    print(f\"{model_class} gridsearch_rmse: {gridsearch.best_score_}\")\n",
    "    print(f\"{model_class} validation_rmse: {val_rmse}\")\n",
    "    print(f\"{model_class} validation_r2: {val_r2}\")\n",
    "\n",
    "    performance_metrics['validation_rmse'] = val_rmse\n",
    "    performance_metrics['gridsearch_rmse'] = gridsearch.best_score_\n",
    "    performance_metrics['validation_r2'] = val_r2\n",
    "\n",
    "    return best_model,best_hyperparameters,performance_metrics\n",
    "\n",
    "\n",
    "def save_model(model, hyperparameters, performance_metrics, folder='models/regression/linear_regression'):\n",
    "\n",
    "    \"\"\"\n",
    "    Saves the trained regression model, its hyperparameters, and performance metrics in a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        model: The trained regression model.\n",
    "        hyperparameters (dict): dictionary containing all of the parameters used for training.\n",
    "        performance_metrics (dict): dictionary containing all the performance metrics.\n",
    "        folder (str): The folders where the file will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # create folder if it doesn't exist\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # save model using joblib\n",
    "    model_name = os.path.join(folder, 'model.joblib')\n",
    "    joblib.dump(model, model_name)\n",
    "\n",
    "    # save hyperparameters to a json file\n",
    "    hyperparameters_name = os.path.join(folder, 'hyperparameters.json')\n",
    "    with open(hyperparameters_name, 'w') as f:\n",
    "        json.dump(hyperparameters, f, indent=4)\n",
    "\n",
    "    # save performance_metrics to a json file\n",
    "    performance_metrics_name = os.path.join(folder, 'performance_metrics.json')\n",
    "    with open(performance_metrics_name, 'w') as f:\n",
    "        json.dump(performance_metrics, f, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_all_models(X_train,y_train,X_val,y_val):\n",
    "    \"\"\"\n",
    "    Evaluate different regression models using hyperparameter tuning and save results.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # List of models to evaluate\n",
    "    models_to_evaluate = [\n",
    "        (\"LinearRegression\", SGDRegressor, {\n",
    "            \"alpha\": [0.0001 ,0.001, 0.01, 0.1, 1.0],\n",
    "            \"max_iter\": [500, 1000, 2000, 3000, 4000],\n",
    "            \"tol\": [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "            # {'alpha': 0.1, 'max_iter': 1000, 'tol': 1}\n",
    "\n",
    "        }),\n",
    "        (\"DecisionTree\", DecisionTreeRegressor, {\n",
    "            \"max_depth\": [None, 5, 1, 2],\n",
    "            \"min_samples_split\": [2, 5, 7, 1],\n",
    "            \"min_samples_leaf\": [ 10, 8, 6]\n",
    "            # {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 0.1}\n",
    "\n",
    "        }),\n",
    "        (\"RandomForest\", RandomForestRegressor, {\n",
    "            \"n_estimators\": [50, 100, 200, 300],\n",
    "            \"max_depth\": [None, 5, 10, 15],\n",
    "            \"min_samples_split\": [2, 5, 7, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 4, 6]\n",
    "            # {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 1, 'n_estimators': 75}\n",
    "\n",
    "        }),\n",
    "        (\"GradientBoosting\", GradientBoostingRegressor, {\n",
    "            \"n_estimators\": [50, 100, 150, 200],\n",
    "            \"max_depth\": [3, 5, 7, 9],\n",
    "            \"learning_rate\": [0.01, 0.1, 0.2, 0.3]\n",
    "            # {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 200}\n",
    "        })\n",
    "    ]\n",
    "\n",
    "    for model_name, model_class, hyperparameters in models_to_evaluate:\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        best_model,best_hyperparameters,performance_metrics = tune_regression_model_hyperparameters(model_class, X_train, y_train, X_val, y_val, hyperparameters)\n",
    "        # Save model, hyperparameters, and metrics\n",
    "        model_folder = f\"models/regression/{model_name.lower()}\"\n",
    "        save_model(best_model, best_hyperparameters, performance_metrics, folder=model_folder)\n",
    "        print(f\"{model_name} evaluation complete.\")\n",
    "\n",
    "\n",
    "def find_best_model():\n",
    "    \"\"\"\n",
    "    Find the best model based on the saved RMSE values from previously tuned models.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: best_model (model), best_hyperparameters (dict), best_rmse (float)\n",
    "    \"\"\"\n",
    "\n",
    "    best_model = None\n",
    "    best_hyperparameters = {}\n",
    "    best_rmse = float(\"inf\")\n",
    "\n",
    "    # List of models to evaluate\n",
    "    models_to_evaluate = [\"LinearRegression\",\"DecisionTree\",\"RandomForest\",\"GradientBoosting\"]\n",
    "\n",
    "    for model_name in models_to_evaluate:\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        model_folder = f\"models/regression/{model_name.lower()}\"\n",
    "        performance_metrics_path = os.path.join(model_folder, 'performance_metrics.json')\n",
    "\n",
    "        with open(performance_metrics_path, 'r') as f:\n",
    "            performance_metrics = json.load(f)\n",
    "\n",
    "        model_rmse = performance_metrics.get(\"validation_rmse\", float(\"inf\"))\n",
    "\n",
    "        if model_rmse < best_rmse:\n",
    "            best_model = joblib.load(os.path.join(model_folder, 'model.joblib'))\n",
    "            best_hyperparameters = json.load(os.path.join(model_folder, 'hyperparameters.json'))\n",
    "            performance_metrics = json.load(os.path.join(model_folder, 'performance_metrics.json'))\n",
    "            best_rmse = model_rmse\n",
    "            model_name_string = f\"{best_model}\"\n",
    "\n",
    "    print(\"\\n\",f\"{model_name_string} is the best model\")\n",
    "\n",
    "    return best_model, best_hyperparameters, performance_metrics\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   # loading the clean dataset as features and labels\n",
    "    df = pd.read_csv(\"C:/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/airbnb-property-listing/tabular_data/clean_listing.csv\")\n",
    "    X, y = load_airbnb(df, label=\"Price_Night\")\n",
    "\n",
    "    X_train,y_train,X_test,y_test,X_val,y_val = split_X_y(X,y)\n",
    "    train_regression_model(X_train,y_train,X_test,y_test)\n",
    "\n",
    "    evaluate_all_models()\n",
    "\n",
    "    best_model, best_hyperparameters, performance_metrics = find_best_model()\n",
    "\n",
    "    print(best_model)\n",
    "    print(best_hyperparameters)\n",
    "    print(performance_metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluatingLogistic_Regression.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anany\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=10000, random_state=42)\n",
      "{'max_iter': 10000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "{'training_accuracy': 0.9831281394661676, 'validation_accuracy': 0.9887640449438202, 'validation_precision': 0.9887640449438202, 'validaiton_recall': 0.9887640449438202, 'validation_f1': 0.9887640449438202}\n",
      "EvaluatingDecisionTree_Classifier.....\n",
      "DecisionTreeClassifier(min_samples_leaf=2, random_state=42)\n",
      "{'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "{'training_accuracy': 0.9943760464887225, 'validation_accuracy': 1.0, 'validation_precision': 1.0, 'validaiton_recall': 1.0, 'validation_f1': 1.0}\n",
      "EvaluatingGradientBoosting_Classifier.....\n",
      "GradientBoostingClassifier(max_depth=2, random_state=42)\n",
      "{'criterion': 'friedman_mse', 'learning_rate': 0.1, 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "{'training_accuracy': 0.9943760464887225, 'validation_accuracy': 1.0, 'validation_precision': 1.0, 'validaiton_recall': 1.0, 'validation_f1': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"The best Hyperparameters are:\\n\\t {best_hyperparameters}\")\\nprint(f\"The best Performance Metrics are:\\n\\t {performance_metrics}\")\\n\\ny_test_pred = best_model.predict(X_test)\\ntest_accuracy_score = accuracy_score(y_test,y_test_pred)\\ntest_precision_score = precision_score(y_test,y_test_pred,average =\\'micro\\')\\ntest_recall_score = recall_score(y_test,y_test_pred,average =\\'micro\\')\\ntest_f1_score = f1_score(y_test,y_test_pred,average =\\'micro\\')\\nprint(f\\'Test Accuracy:\\t {test_accuracy_score}\\')\\nprint(f\\'Test Precision:\\t {test_precision_score}\\')\\nprint(f\\'Test Recall:\\t {test_recall_score}\\')    \\nprint(f\\'Test F1:\\t {test_f1_score}\\')\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabular_data import load_airbnb\n",
    "from tabular_data import load_airbnb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "def tune_classification_model_hyperparameters(model_class:type,X_train:pd.DataFrame,y_train:pd.DataFrame,\n",
    "    X_val:pd.DataFrame,y_val:pd.Series,hyperparameters:List[Tuple[str, List]]) ->Tuple[Any, Dict[str, Any], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Perform a grid search over a range of hyperparameters for a given logistic_regression model using GridSearchCV.\n",
    "\n",
    "    Parameters:\n",
    "        model_class (class): The class for logistic regression.\n",
    "        X_train (pd.DataFrame): features for training set.\n",
    "        Y_train (pd.Series): labels for training set.\n",
    "        X_val (pd.DataFrame): features for validation set.\n",
    "        y_val (pd.Series): label for validation set.\n",
    "        hyperparameters (list of tuples): list containing all possible values for each hyperparameter\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Any, Dict[str, Any], Dict[str, float]]: tuple containing trained model,best parameters and performance metrics\n",
    "    \"\"\"\n",
    "    model = model_class(random_state=42)\n",
    "    gridsearch = GridSearchCV(estimator=model,param_grid=dict(hyperparameters),cv=5,scoring='accuracy')\n",
    "    gridsearch.fit(X_train,y_train)\n",
    "\n",
    "    # Training Data\n",
    "    best_model = gridsearch.best_estimator_\n",
    "    best_hyperparameters = gridsearch.best_params_\n",
    "    train_accuracy_score = gridsearch.best_score_\n",
    "\n",
    "    # Validation Data, Will use these scores to compare different models\n",
    "    y_val_predict = best_model.predict(X_val)\n",
    "    val_accuracy_score = accuracy_score(y_val,y_val_predict)\n",
    "    val_precision_score = precision_score(y_val,y_val_predict,average ='micro')\n",
    "    val_recall_score = recall_score(y_val,y_val_predict,average ='micro')\n",
    "    val_f1_score = f1_score(y_val,y_val_predict,average ='micro')\n",
    "\n",
    "    #making the dictionary performance metrics with scores\n",
    "    performance_metrics = {\"training_accuracy\":train_accuracy_score,\"validation_accuracy\":val_accuracy_score,\n",
    "    \"validation_precision\":val_precision_score,\"validaiton_recall\":val_recall_score,\"validation_f1\":val_f1_score}\n",
    "\n",
    "    print(best_model)\n",
    "    print(best_hyperparameters)\n",
    "    print(performance_metrics)\n",
    "\n",
    "    return best_model,best_hyperparameters,performance_metrics\n",
    "\n",
    "\n",
    "def evaluate_all_models(X_train,y_train,X_val,y_val):\n",
    "    \"\"\"\n",
    "    DecisionTreeClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    models_to_evaluate = [\n",
    "    #(\"Logistic_Regression\", LogisticRegression, {\n",
    "    #    \"penalty\" : ['l1', 'l2', 'elasticnet', None],\n",
    "    #    \"max_iter\": [1000,10000,100000,20000000],\n",
    "    #    \"solver\" : ['lbfgs', 'liblinear','sag']\n",
    "    #}),\n",
    "    (\"Logistic_Regression\", LogisticRegression, {\n",
    "        \"penalty\" : ['l2', 'none'],\n",
    "        \"max_iter\": [1000, 10000],  # You can reduce the max_iter value\n",
    "        \"solver\" : ['lbfgs', 'saga']  # You can also try 'saga' solver\n",
    "    }),\n",
    "    (\"DecisionTree_Classifier\", DecisionTreeClassifier, {\n",
    "        \"criterion\" : [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"splitter\" : [\"best\", \"random\"],\n",
    "        \"max_depth\": [None, 5, 10, 20],\n",
    "        \"min_samples_split\": [2,5,10],\n",
    "        \"min_samples_leaf\": [2,5,10]\n",
    "    }),\n",
    "    #(\"RandomForest_Classifier\", RandomForestClassifier, {\n",
    "    #    \"n_estimators\": [100, 200, 300],\n",
    "    #    \"criterion\" : [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    #    \"max_depth\": [None, 5, 10, 15],\n",
    "    #    \"min_samples_split\": [2, 5, 7, 10],\n",
    "    #    \"min_samples_leaf\": [1, 2, 4, 6]\n",
    "    #}),\n",
    "    (\"GradientBoosting_Classifier\", GradientBoostingClassifier, {\n",
    "        'criterion': ['friedman_mse', 'squared_error'],\n",
    "        'min_samples_split': [2, 4, 6, 8],\n",
    "        'min_samples_leaf': [1, 2, 3, 4],\n",
    "        'max_depth': [2, 3, 4, 5],\n",
    "        #\"loss\" : [\"log_loss\" ,\"exponential\"],\n",
    "        #\"n_estimators\": [100, 150, 200, 500],\n",
    "        #\"max_depth\": [3, 5, 10],\n",
    "        \"learning_rate\": [0.1,1]\n",
    "    })\n",
    "    ]\n",
    "\n",
    "    for model_name,model,hyperparameters in models_to_evaluate:\n",
    "        print(f\"Evaluating{model_name}.....\")\n",
    "        best_model,best_hyperparameters,performance_metrics = tune_classification_model_hyperparameters(model,X_train,y_train,\n",
    "            X_val,y_val,hyperparameters)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('airbnb-property-listing/tabular_data/clean_listing.csv')\n",
    "X,y = load_airbnb(df,label='Category')\n",
    "#scaler = MinMaxScaler()\n",
    "#X = scaler.fit_transform(X) \n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=43)\n",
    "X_test,X_val,y_test,y_val = train_test_split(X_test,y_test, test_size=0.5, random_state=43)\n",
    "\n",
    "evaluate_all_models(X_train,y_train,X_val,y_val)\n",
    "\"\"\"\n",
    "print(f\"The best Hyperparameters are:\\n\\t {best_hyperparameters}\")\n",
    "print(f\"The best Performance Metrics are:\\n\\t {performance_metrics}\")\n",
    "\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_accuracy_score = accuracy_score(y_test,y_test_pred)\n",
    "test_precision_score = precision_score(y_test,y_test_pred,average ='micro')\n",
    "test_recall_score = recall_score(y_test,y_test_pred,average ='micro')\n",
    "test_f1_score = f1_score(y_test,y_test_pred,average ='micro')\n",
    "print(f'Test Accuracy:\\t {test_accuracy_score}')\n",
    "print(f'Test Precision:\\t {test_precision_score}')\n",
    "print(f'Test Recall:\\t {test_recall_score}')    \n",
    "print(f'Test F1:\\t {test_f1_score}')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old code \n",
    "\n",
    "from tabular_data import load_airbnb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "\n",
    "def split_X_y(X: pd.DataFrame,y: pd.Series) -> Tuple:\n",
    "    \"\"\"\n",
    "    Split the dataset into training,testing and validation tests.\n",
    "    \n",
    "    Parameters: \n",
    "        X (pd.DataFrame): Features.\n",
    "        y (pd.Series): Labels.\n",
    "\n",
    "    Returns:\n",
    "        Tuple (X_train,y_train,X_test,y_test,X_val,y_val): a tuple containing the split features and labels.\n",
    "    \"\"\"\n",
    "    #Normalise features using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X) \n",
    "\n",
    "    # Splitting training and testing data 80-20\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=24)\n",
    "    # Splitting testing and validation data 50:50\n",
    "    X_test,X_val,y_test,y_val = train_test_split(X_test,y_test, test_size=0.5, random_state=24)\n",
    "\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Training Label shape: {y_train.shape}\")\n",
    "    print(\"Number of samples in:\")\n",
    "    print(f\"    Training:   {len(y_train)}\")\n",
    "    print(f\"    Testing:    {len(y_test)}\")\n",
    "    print(f\"    Validation:   {len(y_val)}\\n\")\n",
    "    \n",
    "    return (X_train,y_train,X_test,y_test, X_val,y_val)\n",
    "\n",
    "def train_logistic_model(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series,modelclass = LogisticRegression)->Dict[str,float]:\n",
    "    \"\"\"\n",
    "    Train a logistic regression classifier on the given dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): features for training set.\n",
    "        Y_train (pd.Series): labels for training set.\n",
    "        X_test (pd.DataFrame): features for testing set.\n",
    "        Y_test (pd.Series): label for testing set.\n",
    "        modelclass : The class for the logistic model(LogisticRegression).\n",
    "\n",
    "    Return:\n",
    "        performance_metrics (Dict[str,float]) : Dictionary containing the performace of the model on training and test sets.\n",
    "    \"\"\"\n",
    "    model = modelclass(max_iter=10000,random_state=24)\n",
    "    # Fitting the Logistic Regression Model to the Training Set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculating accuracy,precision,recall and f1 scores\n",
    "    train_accuracy_score = accuracy_score(y_train,y_train_pred)\n",
    "    train_precision_score = precision_score(y_train,y_train_pred,average ='micro')\n",
    "    train_recall_score = recall_score(y_train,y_train_pred,average ='micro')\n",
    "    train_f1_score = f1_score(y_train,y_train_pred,average ='micro')\n",
    "\n",
    "    print(\"Initial Logistic Regression Model parametrics:\")\n",
    "    print(f'Train Accuracy Score:\\t {train_accuracy_score}')\n",
    "    print(f'Train Precision Score:\\t {train_precision_score}')\n",
    "    print(f'Train Recall Score:\\t {train_recall_score}')    \n",
    "    print(f'Train F1 Score:\\t \\t {train_f1_score}')\n",
    "\n",
    "    # Test set evaluation metrics\n",
    "    test_accuracy_score = accuracy_score(y_test,y_test_pred)\n",
    "    test_precision_score = precision_score(y_test,y_test_pred,average ='micro')\n",
    "    test_recall_score = recall_score(y_test,y_test_pred,average ='micro')\n",
    "    test_f1_score = f1_score(y_test,y_test_pred,average ='micro')\n",
    "\n",
    "    #print(f'Test Accuracy Score:\\t {test_accuracy_score}')\n",
    "    #print(f'Test Precision Score:\\t {test_precision_score}')\n",
    "    #print(f'Test Recall Score:\\t {test_recall_score}')    \n",
    "    #print(f'Test F1 Score:\\t \\t {test_f1_score}')\n",
    "\n",
    "    performance_metrics = {\"training_accuracy\":train_accuracy_score,\"training_precision\":train_precision_score,\"training_recall\":train_recall_score,\n",
    "                           \"training_f1\":train_f1_score,\"test_accuracy\":test_accuracy_score,\"test_precision\":test_precision_score,\n",
    "                           \"test_recall\":test_recall_score,\"test_f1\":test_f1_score}\n",
    "\n",
    "    return performance_metrics\n",
    "\n",
    "def tune_classification_model_hyperparameters(model_class:type,X_train:pd.DataFrame,y_train:pd.DataFrame,\n",
    "    X_val:pd.DataFrame,y_val:pd.Series,hyperparameters:List[Tuple[str, List]]) ->Tuple[Any, Dict[str, Any], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Perform a grid search over a range of hyperparameters for a given logistic_regression model using GridSearchCV.\n",
    "\n",
    "    Parameters:\n",
    "        model_class (class): The class for logistic regression.\n",
    "        X_train (pd.DataFrame): features for training set.\n",
    "        Y_train (pd.Series): labels for training set.\n",
    "        X_val (pd.DataFrame): features for validation set.\n",
    "        y_val (pd.Series): label for validation set.\n",
    "        hyperparameters (list of tuples): list containing all possible values for each hyperparameter\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Any, Dict[str, Any], Dict[str, float]]: tuple containing trained model,best parameters and performance metrics\n",
    "    \"\"\"\n",
    "    model = model_class(random_state=24)\n",
    "    gridsearch = GridSearchCV(estimator=model,param_grid=dict(hyperparameters),cv=5,scoring='accuracy',verbose=1)\n",
    "    gridsearch.fit(X_train,y_train)\n",
    "\n",
    "    # Training Data\n",
    "    best_model = gridsearch.best_estimator_\n",
    "    best_hyperparameters = gridsearch.best_params_\n",
    "    train_accuracy_score = gridsearch.best_score_\n",
    "\n",
    "    # Validation Data, Will use these scores to compare different models\n",
    "    y_val_predict = best_model.predict(X_val)\n",
    "    val_accuracy_score = accuracy_score(y_val,y_val_predict)\n",
    "    val_precision_score = precision_score(y_val,y_val_predict,average ='micro')\n",
    "    val_recall_score = recall_score(y_val,y_val_predict,average ='micro')\n",
    "    val_f1_score = f1_score(y_val,y_val_predict,average ='micro')\n",
    "\n",
    "    #making the dictionary performance metrics with scores\n",
    "    performance_metrics = {\"training_accuracy\":train_accuracy_score,\"validation_accuracy\":val_accuracy_score,\n",
    "    \"validation_precision\":val_precision_score,\"validaiton_recall\":val_recall_score,\"validation_f1\":val_f1_score}\n",
    "\n",
    "    print(best_model)\n",
    "    print(performance_metrics)\n",
    "\n",
    "    return best_model,best_hyperparameters,performance_metrics\n",
    "\n",
    "def save_model(model:type, hyperparameters:dict, performance_metrics:dict, folder:str=\"models/classification/logistic_regression\"):\n",
    "    \"\"\"\n",
    "    A function that saves a model,it's hyperparameters and performance metrics in the designated folder.\n",
    "    \n",
    "    Parameters:\n",
    "        model (class): The model to be saved in the folder.\n",
    "        hyperparameters (dict): The model's hyperparameters.\n",
    "        performance_metrics (dict): The models performance metrics.\n",
    "        folder (str): The path to the designated folder passed as string.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # create folder if it doesn't exist\n",
    "    os.makedirs(folder,exist_ok=True)\n",
    "\n",
    "    #save model using joblib\n",
    "    model_name = os.path.join(folder,'model.joblib')\n",
    "    joblib.dump(model,model_name)\n",
    "    \n",
    "    #save hyperparameter values in a json file\n",
    "    hyperparameters_name = os.path.join(folder,'hyperparameters.json')\n",
    "    with open (hyperparameters_name,\"w\") as f:\n",
    "        json.dump(hyperparameters,f,indent=4)\n",
    "\n",
    "    # save performance metrics in a json file\n",
    "    performance_metrics_name = os.path.join(folder, 'performance_metrics.json')\n",
    "    with open(performance_metrics_name, 'w') as f:\n",
    "        json.dump(performance_metrics, f, indent=4)\n",
    "\n",
    "\n",
    "def evaluate_all_models(X_train,y_train,X_val,y_val):\n",
    "    \"\"\"\n",
    "    DecisionTreeClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    models_to_evaluate = [\n",
    "    (\"Logistic_Regression\", LogisticRegression, {\n",
    "        \"penalty\" : ['l1', 'l2', 'elasticnet', None],\n",
    "        \"max_iter\": [1000,10000,100000],\n",
    "        \"solver\" : ['lbfgs','saga']\n",
    "    }),\n",
    "    (\"DecisionTree_Classifier\", DecisionTreeClassifier, {\n",
    "        \"criterion\" : [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"splitter\" : [\"best\", \"random\"],\n",
    "        \"max_depth\": [None, 5, 10],\n",
    "        \"min_samples_split\": [2,3,5],\n",
    "        \"min_samples_leaf\": [1,2,5]\n",
    "    }),\n",
    "    (\"RandomForest_Classifier\", RandomForestClassifier, {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"criterion\" : [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"max_depth\": [None, 5, 10],\n",
    "        \"min_samples_split\": [2, 5, 7],\n",
    "        \"min_samples_leaf\": [1, 2, 5]\n",
    "    }),\n",
    "    (\"GradientBoosting_Classifier\", GradientBoostingClassifier, {\n",
    "        'criterion': ['friedman_mse', 'squared_error'],\n",
    "        \"n_estimators\": [50,100, 150],\n",
    "        \"max_depth\": [3, 5, 10],\n",
    "        \"learning_rate\": [0.01, 0.1,1,10]\n",
    "    })\n",
    "    ]\n",
    "\n",
    "    for model_name,model,hyperparameters in models_to_evaluate:\n",
    "        print(f\"Evaluating{model_name}.....\")\n",
    "        best_model,best_hyperparameters,performance_metrics = tune_classification_model_hyperparameters(model,X_train,y_train,\n",
    "            X_val,y_val,hyperparameters)\n",
    "\n",
    "        #saving the model\n",
    "        model_folder = f\"models/classification/{model_name.lower()}\"\n",
    "        save_model(best_model,best_hyperparameters,performance_metrics,model_folder)\n",
    "        print(f\"Model Saved for {model_name}\\n\")\n",
    "\n",
    "\n",
    "def find_best_models():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    best_model = None\n",
    "    best_hyperparameters = {}\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    models_to_evaluate = [\"Logistic_Regression\",\"DecisionTree_Classifier\",\"RandomForest_Classifier\",\"GradientBoosting_Classifier\"]\n",
    "\n",
    "    for model in models_to_evaluate:\n",
    "        model_folder = f\"models/classification/{model.lower()}\"\n",
    "        performance_metrics_path = os.path.join(model_folder,'performance_metrics.json')\n",
    "\n",
    "        with open (performance_metrics_path,'r') as f:\n",
    "            performance_metrics = json.load(f) \n",
    "\n",
    "    model_accuracy = performance_metrics.get(\"validation_accuracy\",float)\n",
    "\n",
    "    if model_accuracy > best_accuracy:\n",
    "        best_model = joblib.load(os.path.join(model_folder,'model.joblib'))\n",
    "        hyperparameters_path = os.path.join(model_folder,'hyperparameters.json')\n",
    "        with open (hyperparameters_path,\"r\") as f:\n",
    "            best_hyperparameters= json.load(f)\n",
    "        best_accuracy = model_accuracy\n",
    "        model_name_string = f\"{model}\"\n",
    "\n",
    "    print(f\"\\n{model_name_string} is the best model.\")\n",
    "    return best_model,best_hyperparameters,performance_metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('airbnb-property-listing/tabular_data/clean_listing.csv')\n",
    "    X,y = load_airbnb(df,label='Category')\n",
    "    X_train,y_train,X_test,y_test,X_val,y_val = split_X_y(X,y)\n",
    "    performance_metrics = train_logistic_model(X_train,y_train,X_test,y_test)\n",
    "    print(f\"\\n Initial Logistic Regression Performance Metrics:\\n\\t Test Accuracy: {performance_metrics['test_accuracy']}\\n\\t Test Precision: {performance_metrics['test_precision']}\\n\\t Test Recall: {performance_metrics['test_recall']}\\n\\t Test f1: {performance_metrics['test_f1']}\")\n",
    "    evaluate_all_models(X_train,y_train,X_val,y_val)\n",
    "    best_model,best_hyperparameters,performance_metrics= find_best_models()\n",
    "    print(f\"The best Hyperparameters are:\\n\\t {best_hyperparameters}\")\n",
    "    print(f\"The best Performance Metrics are:\\n\\t {performance_metrics}\")\n",
    "\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_accuracy_score = accuracy_score(y_test,y_test_pred)\n",
    "    test_precision_score = precision_score(y_test,y_test_pred,average ='micro')\n",
    "    test_recall_score = recall_score(y_test,y_test_pred,average ='micro')\n",
    "    test_f1_score = f1_score(y_test,y_test_pred,average ='micro')\n",
    "    print(f'Test Accuracy:\\t {test_accuracy_score}')\n",
    "    print(f'Test Precision:\\t {test_precision_score}')\n",
    "    print(f'Test Recall:\\t {test_recall_score}')    \n",
    "    print(f'Test F1:\\t {test_f1_score}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Treehouses\n",
      "1      Treehouses\n",
      "2      Treehouses\n",
      "3      Treehouses\n",
      "4      Treehouses\n",
      "          ...    \n",
      "885    Beachfront\n",
      "886    Beachfront\n",
      "887    Beachfront\n",
      "888    Beachfront\n",
      "889    Beachfront\n",
      "Name: Category, Length: 890, dtype: object\n",
      "0      Treehouses\n",
      "1      Treehouses\n",
      "2      Treehouses\n",
      "3      Treehouses\n",
      "4      Treehouses\n",
      "          ...    \n",
      "885    Beachfront\n",
      "886    Beachfront\n",
      "887    Beachfront\n",
      "888    Beachfront\n",
      "889    Beachfront\n",
      "Name: Category, Length: 890, dtype: object\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "print(y)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(y)\n",
    "\n",
    "print(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(LogisticRegression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to evaluate\n",
    "models_to_evaluate = [\n",
    "(\"LinearRegression\", SGDRegressor, {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "    \"max_iter\": [500, 1000, 2000, 3000, 4000],\n",
    "    \"tol\": [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "}),\n",
    "(\"DecisionTree\", DecisionTreeRegressor, {\n",
    "    \"max_depth\": [None, 5, 3, 1],\n",
    "    \"min_samples_split\": [2, 5, 7, 1],\n",
    "    \"min_samples_leaf\": [10, 8, 6]\n",
    "}),\n",
    "(\"RandomForest\", RandomForestRegressor, {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [None, 5, 10, 15],\n",
    "    \"min_samples_split\": [2, 5, 7, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 6]\n",
    "}),\n",
    "(\"GradientBoosting\", GradientBoostingRegressor, {\n",
    "    \"n_estimators\": [100, 150, 200],\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"learning_rate\": [0.01, 0.1]\n",
    "})\n",
    "]\n",
    "\n",
    "#just kept the earlier version\n",
    "models_to_evaluate = [\n",
    "(\"LinearRegression\", SGDRegressor, {\n",
    "    \"alpha\": [0.001, 0.0001, 0.00001],  # Adding more alpha values for regularization\n",
    "    \"max_iter\": [500, 1000, 2000],      # Trying higher values for max_iter\n",
    "    \"tol\": [1e-2, 1e-3, 1e-4]            # No change, already a reasonable range\n",
    "}),\n",
    "(\"DecisionTree\", DecisionTreeRegressor, {\n",
    "    \"max_depth\": [None, 5, 10, 15],      # Trying a higher value for max_depth\n",
    "    \"min_samples_split\": [2, 5, 10],      # Increasing min_samples_split for more regularization\n",
    "    \"min_samples_leaf\": [10, 8, 6]        # No change, already a reasonable range\n",
    "}),\n",
    "(\"RandomForest\", RandomForestRegressor, {\n",
    "    \"n_estimators\": [300, 400, 500],      # Trying higher values for n_estimators\n",
    "    \"max_depth\": [None, 10, 20],           # Limiting max_depth to control tree complexity\n",
    "    \"min_samples_split\": [2, 5, 10],       # No change, already a reasonable range\n",
    "    \"min_samples_leaf\": [6, 8, 10]         # No change, already a reasonable range\n",
    "}),\n",
    "(\"GradientBoosting\", GradientBoostingRegressor, {\n",
    "    \"n_estimators\": [200, 300, 400],       # Trying higher values for n_estimators\n",
    "    \"max_depth\": [3, 5, 7],                 # No change, already a reasonable range\n",
    "    \"learning_rate\": [0.01, 0.005, 0.001]   # Trying lower values for learning_rate\n",
    "})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely, I apologize for the confusion. Here's the updated `README.md` content written in Markdown syntax that you can directly copy:\n",
    "\n",
    "```markdown\n",
    "# Airbnb Model Evaluation Framework\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Description](#description)\n",
    "- [Installation](#installation)\n",
    "- [Usage](#usage)\n",
    "- [File Structure](#file-structure)\n",
    "- [License](#license)\n",
    "\n",
    "## Description\n",
    "\n",
    "The **Airbnb Model Evaluation Framework** is a versatile tool designed to systematically train, tune, and evaluate machine learning models for various tasks. Inspired by the challenges faced by the Airbnb team, this framework aims to streamline the process of building effective models for different data types, such as tabular, image, or text data.\n",
    "\n",
    "By combining the power of libraries like Pandas, Scikit-Learn, and NumPy, along with tools for hyperparameter tuning and data preprocessing, this framework provides a flexible solution for creating accurate and robust machine learning models.\n",
    "\n",
    "### Aim\n",
    "\n",
    "The main objective of this project is to create a reusable framework that simplifies the process of developing machine learning models for different datasets and tasks. By offering a structured approach to data preprocessing, model training, hyperparameter tuning, and evaluation, this framework aims to save time and effort while ensuring the creation of effective models.\n",
    "\n",
    "### What I Learned\n",
    "\n",
    "Through the development of this project, I gained valuable insights into various aspects of machine learning, including data preprocessing, model evaluation, and hyperparameter tuning. I learned how to leverage libraries like Scikit-Learn to create comprehensive workflows for building and evaluating machine learning models. Additionally, I discovered the importance of effective documentation to ensure that users can understand and utilize the framework.\n",
    "\n",
    "## Installation\n",
    "\n",
    "To get started with the Airbnb Model Evaluation Framework, follow these steps:\n",
    "\n",
    "1. Clone the repository:\n",
    "   \n",
    "   ```\n",
    "   git clone https://github.com/anany14/Modelling_Airbnbs_property_listing_dataset.git\n",
    "   cd Modelling_Airbnbs_property_listing_dataset\n",
    "   ```\n",
    "\n",
    "2. Install the required packages:\n",
    "\n",
    "   ```\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "## Usage\n",
    "\n",
    "The framework can be utilized for various tasks related to machine learning model development and evaluation. The provided Python scripts (`tabular_data.py`, `modelling.py`, and `classification.py`) demonstrate how to use the framework for data preprocessing, building regression and classification models, and evaluating their performance.\n",
    "\n",
    "1. Run `tabular_data.py` to clean and preprocess the dataset.\n",
    "2. Run `modelling.py` to build and evaluate regression models for predicting the price per night.\n",
    "3. Run `classification.py` to build and evaluate classification models for categorizing property listings.\n",
    "\n",
    "## File Structure\n",
    "\n",
    "The project directory is organized as follows:\n",
    "\n",
    "```\n",
    "Airbnb_Model_Evaluation_Framework/\n",
    "‚îú‚îÄ‚îÄ tabular_data.py\n",
    "‚îú‚îÄ‚îÄ modelling.py\n",
    "‚îú‚îÄ‚îÄ classification.py\n",
    "‚îú‚îÄ‚îÄ README.md\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ airbnb-property-listing/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ tabular_data/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clean_listing.csv\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "The main components of the project include the Python scripts, the `airbnb-property-listing` directory containing the dataset, and the `README.md` file for project documentation.\n",
    "\n",
    "## License\n",
    "\n",
    "Distributed under the MIT License. See [LICENSE.txt](LICENSE.txt) for more information.\n",
    "\n",
    "For any questions or inquiries, please feel free to reach out:\n",
    "\n",
    "Anany Tripathi - ananytripathi10@gmail.com\n",
    "\n",
    "Project Link: [https://github.com/anany14/Modelling_Airbnbs_property_listing_dataset](https://github.com/anany14/Modelling_Airbnbs_property_listing_dataset)\n",
    "```\n",
    "\n",
    "Now you can copy and paste this Markdown content into your `README.md` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# markdown 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course! Here's your original `README.md` content with the new sections and instructions added in Markdown format:\n",
    "\n",
    "```markdown\n",
    "# Modelling_Airbnbs_property_listing_dataset\n",
    "\n",
    "Welcome to the **Airbnb Model Evaluation Framework**! This project aims to create a versatile framework for systematically training, tuning, and evaluating machine learning models for a wide range of tasks, inspired by the challenges faced by the Airbnb team. Whether you're working with tabular, image, or text data, this framework will help you build effective models and streamline the evaluation process.\n",
    "\n",
    "## Built With\n",
    "\n",
    "This project leverages several essential frameworks and tools to achieve its goals:\n",
    "\n",
    "- [Pandas](https://pandas.pydata.org/) - Data manipulation and analysis in Python.\n",
    "- [Scikit-Learn](https://scikit-learn.org/) - Machine learning library in Python.\n",
    "- [NumPy](https://numpy.org/) - Fundamental package for scientific computing with Python.\n",
    "- [Joblib](https://joblib.readthedocs.io/) - Efficiently save and load Python objects.\n",
    "- [JSON](https://www.json.org/) - For storing hyperparameters and performance metrics.\n",
    "- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) - Grid search for hyperparameter tuning.\n",
    "- [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) - Feature scaling.\n",
    "\n",
    "## Description\n",
    "\n",
    "The **Airbnb Model Evaluation Framework** is designed to streamline the process of training, tuning, and evaluating machine learning models for a variety of tasks. Whether you're predicting prices, categorizing listings, or tackling other challenges, this framework provides a structured approach to achieve accurate results.\n",
    "\n",
    "### Aim\n",
    "\n",
    "The main objective of this project is to create a reusable framework that simplifies the process of developing machine learning models for different datasets and tasks. By offering a structured approach to data preprocessing, model training, hyperparameter tuning, and evaluation, this framework aims to save time and effort while ensuring the creation of effective models.\n",
    "\n",
    "### What I Learned\n",
    "\n",
    "Through the development of this project, I gained valuable insights into various aspects of machine learning, including data preprocessing, model evaluation, and hyperparameter tuning. I learned how to leverage libraries like Scikit-Learn to create comprehensive workflows for building and evaluating machine learning models. Additionally, I discovered the importance of effective documentation to ensure that users can understand and utilize the framework.\n",
    "\n",
    "## Installation\n",
    "\n",
    "To get started with the Airbnb Model Evaluation Framework, follow these steps:\n",
    "\n",
    "1. Clone the repository:\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/anany14/Modelling_Airbnbs_property_listing_dataset.git\n",
    "   cd Modelling_Airbnbs_property_listing_dataset\n",
    "   ```\n",
    "\n",
    "2. Install the required packages:\n",
    "\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "## Usage\n",
    "\n",
    "Use the Airbnb Model Evaluation Framework to build and evaluate machine learning models. The provided python scripts demonstrate how to use the framework for regression and classification tasks. Feel free to modify the scripts or create your own to adapt the framework to your specific dataset and task.\n",
    "\n",
    "1. Run the `tabular_data.py` script to download the dataset as a [PANDAS] DataFrame and then clean the data, remove unnecessary data, convert datatypes, and get all the columns in the right format with the correct values.\n",
    "\n",
    "2. Run the `modelling.py` script to build and evaluate regression models to predict the price per night of the Airbnb property listings using various models such as [SGDRegressor],[DecisionTreeRegressor],[RandomForestRegressor], and [GradientBoostingRegressor].\n",
    "\n",
    "3. Run the `classification.py` script to build and evaluate classification models for categorizing property listings.\n",
    "\n",
    "## File Structure\n",
    "\n",
    "The project directory is organized as follows:\n",
    "\n",
    "```\n",
    "Modelling_Airbnbs_property_listing_dataset/\n",
    "‚îú‚îÄ‚îÄ tabular_data.py\n",
    "‚îú‚îÄ‚îÄ modelling.py\n",
    "‚îú‚îÄ‚îÄ classification.py\n",
    "‚îú‚îÄ‚îÄ README.md\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ airbnb-property-listing/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ tabular_data/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clean_listing.csv\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "The main components of the project include the Python scripts, the `airbnb-property-listing` directory containing the dataset, and the `README.md` file for project documentation.\n",
    "\n",
    "## License\n",
    "\n",
    "Distributed under the MIT License. See [LICENSE.txt](LICENSE.txt) for more information.\n",
    "\n",
    "## Contact\n",
    "\n",
    "For any questions or inquiries, please feel free to reach out:\n",
    "\n",
    "Anany Tripathi - ananytripathi10@gmail.com\n",
    "\n",
    "Project Link: [https://github.com/anany14/Modelling_Airbnbs_property_listing_dataset](https://github.com/anany14/Modelling_Airbnbs_property_listing_dataset)\n",
    "```\n",
    "\n",
    "You can now copy and paste this Markdown content into your `README.md` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_all_models(models: list, X_val: pd.DataFrame, y_val: pd.Series,file_path: str = None):\n",
    "\n",
    "    \"\"\"\n",
    "    # Get the indices of the top 3 maximum values\n",
    "    top_indicesY = y_val.nlargest(3).index\n",
    "    top_indicesX = X_val.nlargest(3).index\n",
    "    # Drop the top 3 maximum values by index\n",
    "    y_val = y_val.drop(top_indicesY)\n",
    "    X_val = X_val.drop(top_indicesX)\n",
    "    \"\"\"\n",
    "\n",
    "    Charcoal = \"#36454F\"\n",
    "    Teal = \"#008080\"\n",
    "    Coral = \"#FF6B6B\"\n",
    "    Peach = \"#FFDAB9\"\n",
    "    Olive_Green = \"#808000\"\n",
    "\n",
    "    if len(models) == 4:\n",
    "        plt.figure(figsize=(12, 8))  # Adjust the figure size\n",
    "\n",
    "        for k, model_name in enumerate(models, start=1):\n",
    "\n",
    "            if model_name == 'LinearRegression':\n",
    "                scaler = MinMaxScaler()\n",
    "                X_val_scaled = scaler.fit_transform(X_val)\n",
    "            else:\n",
    "                X_val_scaled = X_val\n",
    "            \n",
    "            model_folder = f\"models/regression/{model_name.lower()}\"\n",
    "            model = joblib.load(os.path.join(model_folder, 'model.joblib'))\n",
    "            y_pred_val = model.predict(X_val_scaled)\n",
    "\n",
    "            plt.subplot(2, 2, k)  # Define the subplot layout\n",
    "            plt.scatter(X_val.iloc[:, 0], y_pred_val, color=Teal, label='Predicted Values', marker='x')\n",
    "            plt.scatter(X_val.iloc[:, 0], y_val, color= Coral, label='Actual Values', marker='o')\n",
    "            # Fit a linear regression line to the scatter plot\n",
    "            plt.title(model_name)\n",
    "            plt.xlabel(\"Actual Values\")\n",
    "            plt.ylabel(\"Predicted Values\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()  # Adjust subplot layout\n",
    "\n",
    "        plt.show()\n",
    "        if file_path:\n",
    "            plt.savefig(file_path,format='png')\n",
    "            print(f\"SubPlot saved as {file_path}\")\n",
    "\n",
    "    if len(models) == 1:\n",
    "\n",
    "        model = models[0]\n",
    "\n",
    "        if isinstance(model,SGDRegressor):\n",
    "            scaler = MinMaxScaler()\n",
    "            X_val_scaled = scaler.fit_transform(X_val)\n",
    "        else:\n",
    "            X_val_scaled = X_val\n",
    "\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        plt.figure(figsize=(8,6))\n",
    "\n",
    "        plt.subplot(2,2,1)\n",
    "        plt.scatter(X_val.iloc[:, 0], y_val, color = Teal, label=\"Actual Values\", marker='o')\n",
    "        plt.scatter(X_val.iloc[:, 0], y_pred_val, color=Coral, label=\"Predicted Values\",marker='x')\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.title(\"Linear Regression\")\n",
    "        plt.legend()\n",
    "\n",
    "        residuals = y_val - y_pred_val\n",
    "        plt.subplot(2,2,2)\n",
    "        plt.scatter(X_val.iloc[:, 0], residuals,color = Olive_Green)\n",
    "        plt.axhline(y=0, color=Charcoal, linestyle='--')\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.title(\"Residual Plot\")\n",
    "\n",
    "        plt.subplot(2,2,3)\n",
    "        plt.hist(residuals, bins=20, edgecolor='k',color=Peach)\n",
    "        plt.xlabel(\"Residuals\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Distribution of Residuals\")\n",
    "\n",
    "        import statsmodels.api as sm\n",
    "        plt.subplot(2,2,4)\n",
    "        ax = plt.gca()\n",
    "        sm.qqplot(residuals, line='r',ax=ax)\n",
    "        plt.title(\"Q-Q Plot\")\n",
    "\n",
    "        if isinstance(model,SGDRegressor):\n",
    "            plt.suptitle(\"SGDRegressor without Hyperparameters\")\n",
    "        else:\n",
    "            plt.suptitle(f\"Best Model {model}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Modelling_Airbnbs_property_listing_dataset\n",
    "\n",
    "Welcome to the **Airbnb Model Evaluation Framework**! This project aims to create a versatile framework for systematically training, tuning, and evaluating machine learning models for a wide range of tasks, inspired by the challenges faced by the Airbnb team. Whether you're working with tabular, image, or text data, this framework will help you build effective models and streamline the evaluation process.\n",
    "\n",
    "## Built With\n",
    "\n",
    "This project leverages several essential frameworks and tools to achieve its goals:\n",
    "\n",
    "- [GitHub](https://github.com/) - Hosting and version control for collaborative development.\n",
    "- [Jupyter Notebook](https://jupyterlab.readthedocs.io/en) - An open source web application that allows you to create and share.\n",
    "- [Python 3.7](https://www.python.org/downloads) - The programming language used in this project.\n",
    "- [Pandas](https://pandas.pydata.org/) - Data manipulation and analysis in Python.\n",
    "- [Scikit-Learn](https://scikit-learn.org/) - Machine learning library in Python.\n",
    "- [NumPy](https://numpy.org/) - Fundamental package for scientific computing with Python.\n",
    "- [Joblib](https://joblib.readthedocs.io/) - Efficiently save and load Python objects.\n",
    "- [JSON](https://www.json.org/) - For storing hyperparameters and performance metrics.\n",
    "- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) - Grid search for hyperparameter tuning.\n",
    "- [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) - Feature scaling.\n",
    "- [Matplotlib](https://matplotlib.org/stable/index.html) - Plotting graphs, charts etc., in Python.\n",
    "\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To get started with the Airbnb Model Evaluation Framework, follow these steps:\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before you begin, ensure you have the following installed:\n",
    "\n",
    "- Python (>=3.6)\n",
    "- Jupyter Notebook (optional)\n",
    "\n",
    "### Installation\n",
    "\n",
    "1. Clone the repository:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/anany14/Modelling_Airbnbs_property_listing_dataset.git\n",
    "cd Modelling_Airbnbs_property_listing_dataset\n",
    "```\n",
    "\n",
    "2. Install the required packages:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Usage\n",
    "\n",
    "Use the Airbnb Model Evaluation Framework to build and evaluate machine learning models. The provided python scripts demonstrate how to use the framework for regression and classification tasks. Feel free to modify the scripts or create your own to adapt the framework to your specific dataset and task.\n",
    "\n",
    "1. Run the `tabular_data.py` script to download the dataset as a [PANDAS] DataFrame and then clean the data, remove unnecessary data, convert datatypes and get all the columns in the right format with the correct values.\n",
    "\n",
    "2. Run the `modelling.py` script to perform regression modeling, hyperparameter tuning, and evaluation of the models to predict the price per night of the Airbnb property listings using various models such as [SGDRegressor],[DecisionTreeRegressor],[RandomForestRegressor] and [GradientBoostingRegressor].\n",
    "\n",
    "3. Run the `classification.py` script to train, tune, and evaluate classification models on the Airbnb dataset for predicting property categories using models like [LogisticRegression], [DecisionTreeClassifier], [RandomForestClassifier], and [GradientBoostingClassifier].\n",
    "\n",
    "4. Both the scripts contain functions to plot the performance of different models which are in the folders `plots/regression` and `plots/classification` \n",
    "\n",
    "5. \n",
    "\n",
    "6. \n",
    "\n",
    "\n",
    "\n",
    "<!---\n",
    "\n",
    "## Roadmap\n",
    "We have some exciting plans for the future of this framework:\n",
    "\n",
    "- Add support for additional data types and tasks.\n",
    "- Enhance the visualization of results.\n",
    "- Improve documentation with more usage examples.\n",
    "- Introduce multi-language support.\n",
    "- Include more configurable neural network architectures.\n",
    "--->\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project aims to build a systematic framework for training, tuning, and evaluating machine learning models on various tasks, inspired by the challenges tackled by the Airbnb team. The framework will be designed to handle different types of data, including tabular, image, and text data. The goal is to create a flexible and reusable system that can be applied to any dataset.\n",
    "\n",
    "### Milestones\n",
    "1. Set up the environment: Prepare the development environment to start building the framework.\n",
    "\n",
    "2. Data preparation: Understand the structure of the Airbnb dataset and perform data cleaning and preprocessing.\n",
    "\n",
    "3. Create a regression model: Build machine learning models that predict the price of the Airbnb listing per night and evaluate their performance.\n",
    "\n",
    "4. Create a classification model: Develop classification models for specific tasks and assess their effectiveness.\n",
    "\n",
    "5. Create a configurable neural network: Utilize a neural network to predict the nightly listing price using the numerical data from the tabular dataset.\n",
    "\n",
    "6. Reuse the framework for another use-case: Test the flexibility of the framework by applying it to a different dataset, ensuring that it can handle various data types.\n",
    "\n",
    "## Data Cleaning (tabular_data.py)\n",
    "\n",
    "The `tabular_data.py` module contains functions for cleaning and preprocessing the Airbnb property listing dataset. The key steps include:\n",
    "\n",
    "1. **Removing Rows with Missing Ratings:** The `remove_rows_with_missing_columns` function removes rows with missing values in specific rating columns (e.g., cleanliness, accuracy, communication, location, check-in, and value ratings).\n",
    "\n",
    "2. **Fixing Problematic Rows:** The `fix_problematic_rows` function manually fixes specific rows with shifting issues in column values, ensuring proper alignment.\n",
    "\n",
    "3. **Combining Description and Amenities:** The `combine_description_settings` function processes the description and amenities columns by cleaning and combining list items into a single string.\n",
    "\n",
    "4. **Setting Default Feature Values:** The `set_default_feature_values` function fills empty entries in the guests, beds, bathrooms, and bedrooms columns with default values.\n",
    "\n",
    "5. **Converting Data Types:** The `convert_dtypes_and_optimise_df` function converts specific columns to appropriate data types, optimizes the DataFrame, and drops unnecessary columns.\n",
    "\n",
    "6. **Cleaning Tabular Data:** The `clean_tabular_data` function applies a series of data cleaning steps, combining the above functions to obtain a cleaned DataFrame.\n",
    "\n",
    "7. **load_airbnb:** The `load_airbnb` function Extract features and labels from the DataFrame. \n",
    "\n",
    "## Regression Modeling (modelling.py)\n",
    "\n",
    "The `modelling.py` module focuses on building and evaluating regression models to predict the price per night of the Airbnb property listings. The key steps include:\n",
    "\n",
    "1. **Data Splitting:** The `split_X_y` function splits the dataset into training, testing, and validation sets, ensuring that the data is ready for model training and evaluation.\n",
    "\n",
    "2. **Initial Model Training:** The `train_regression_model` function trains a regression model (default is Stochastic Gradient Descent - SGDRegressor) and prints its initial performance on the training and test sets.\n",
    "\n",
    "3. **Custom Hyperparameter Tuning:** The `custom_tune_regression_model_hyperparameters` function performs a grid search over a range of hyperparameter values for a given regression model. It returns the best model, best hyperparameters, and performance metrics (validation RMSE, test RMSE).\n",
    "\n",
    "4. **Hyperparameter Tuning with GridSearchCV:** The `tune_regression_model_hyperparameters` function uses GridSearchCV for hyperparameter tuning, allowing us to explore a wider range of hyperparameter values for different regression models.\n",
    "\n",
    "5. **Model Evaluation and Selection:** The `evaluate_all_models` function evaluates multiple regression models (Linear Regression, Decision Tree, Random Forest, Gradient Boosting) by tuning their hyperparameters. The best model is selected based on validation RMSE, and the trained models are saved along with their performance metrics.\n",
    "\n",
    "6. **Finding the Best Model:** The `find_best_model` function identifies the best-performing model based on the saved validation RMSE values from the earlier model evaluations.\n",
    "\n",
    "7. **Plotting Data:** The `plot_all_models` function plots either a comparison of different models using scatterplots or the total performance if a single model is provided.\n",
    "\n",
    "#### Model Selection and Metrics\n",
    "\n",
    "In the file `modelling.py`, we evaluated several regression models:\n",
    "\n",
    "1. **Linear Regression (SGDRegressor)**: We used Stochastic Gradient Descent as a baseline regression model.\n",
    "\n",
    "2. **Decision Tree Regressor**: A decision tree-based regression model that can capture non-linear relationships.\n",
    "\n",
    "3. **Random Forest Regressor**: An ensemble model combining multiple decision trees for improved predictive performance.\n",
    "\n",
    "4. **Gradient Boosting Regressor**: A boosting algorithm that combines weak learners into a strong predictive model.\n",
    "\n",
    "### Model Performance\n",
    "\n",
    "The best model based on the validation RMSE is the chosen model for making predictions on new data. The key metrics used for evaluation are:\n",
    "\n",
    "1. **Root Mean Squared Error (RMSE)**: A measure of the average deviation between the predicted and actual values. Lower RMSE indicates better model performance.\n",
    "\n",
    "2. **R-squared (R2) Score**: A measure of how well the model explains the variance in the target variable. Higher R2 score indicates a better fit to the data.\n",
    "\n",
    "![This is the Model performance of the LinearRegression class without tuning the hyperparameters](plots/regression/sgdregression_without_hyperparameters.png)\n",
    "\n",
    "### Model Performance Metrics \n",
    "\n",
    "- **Linear Regression (SGDRegressor)**:\n",
    "    - best hyperparameters: {'alpha': 1e-05, 'max_iter': 1000, 'tol': 0.001}\n",
    "    - gridsearch_rmse: 97.7362631514105\n",
    "    - validation_rmse: 127.4768534817222\n",
    "    - validation_r2: -0.23837397130168658\n",
    "\n",
    "- **Decision Tree Regressor**:\n",
    "    - best hyperparameters: {'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
    "    - gridsearch_rmse: 99.83932831904127\n",
    "    - validation_rmse: 104.07966226324588\n",
    "    - validation_r2: 0.174492747038261\n",
    "\n",
    "- **Random Forest Regressor**:\n",
    "    - best hyperparameters: {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 300}\n",
    "    - gridsearch_rmse: 93.53765214999888\n",
    "    - validation_rmse: 101.63984683370312\n",
    "    - validation_r2: 0.212741880233853\n",
    "\n",
    "- **Gradient Boosting Regressor**:\n",
    "    - best hyperparameters: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 300}\n",
    "    - gridsearch_rmse: 102.7416140657607\n",
    "    - validation_rmse: 107.80684569675182\n",
    "    - validation_r2: 0.11430983256579175\n",
    "\n",
    "\n",
    "![Comparison Plot](plots/regression/model_comparison.png)\n",
    "\n",
    "\n",
    "### Best Model \n",
    "\n",
    "- The best model and it's best hyperparameters to fit the data is `GradientBoostingRegressor(learning_rate=0.01, n_estimators=300, random_state=42)`\n",
    "- The RMSE on the testing dataset is `131.073797`\n",
    "\n",
    "![Best Model Performance](plots/regression/best_model.png)\n",
    "\n",
    "### Further Experiments\n",
    "\n",
    "While we have explored a variety of regression models and performed hyperparameter tuning, there are several additional experiments we could consider:\n",
    "\n",
    "1. **Feature Engineering**: We can experiment with creating new features based on domain knowledge or feature interactions to potentially improve model performance.\n",
    "\n",
    "2. **Advanced Ensemble Models**: We can explore more advanced ensemble methods such as XGBoost and LightGBM to see if they provide further improvements.\n",
    "\n",
    "3. **Cross-Validation Strategies**: We used a simple train-validation-test split, but we can experiment with more advanced cross-validation strategies to robustly evaluate model performance.\n",
    "\n",
    "4. **Fine-Tuning Hyperparameters**: We can perform more exhaustive grid searches or use Bayesian optimization to fine-tune hyperparameters and achieve even better model performance.\n",
    "\n",
    "5. **Handling Outliers**: Exploring techniques to handle outliers in the data may improve the robustness of our models.\n",
    "\n",
    "## \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Contributing\n",
    "\n",
    "Contributions to this project are highly appreciated. If you have suggestions or improvements, please follow these steps:\n",
    "\n",
    "1. Fork the project.\n",
    "2. Create a new branch: `git checkout -b feature/awesome-feature`.\n",
    "3. Commit your changes: `git commit -m 'Add some awesome feature'`.\n",
    "4. Push the branch: `git push origin feature/awesome-feature`.\n",
    "5. Open a pull request with the tag \"enhancement.\"\n",
    "\n",
    "Your contributions will help make this framework even more useful to the community.\n",
    "\n",
    "## License\n",
    "\n",
    "Distributed under the MIT License. See [LICENSE.txt](LICENSE.txt) for more information.\n",
    "\n",
    "## Contact\n",
    "\n",
    "For any questions or inquiries, please feel free to reach out:\n",
    "\n",
    "Anany Tripathi - ananytripathi10@gmail.com\n",
    "\n",
    "Project Link: [https://github.com/anany14/Modelling_Airbnbs_property_listing_dataset]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"plots/regression/sgdregression_without_hyperparameters.png\" alt=\"This is the Model performance of the LinearRegression class without tuning the hyperparameters\">\n",
    "  <figcaption>Figure 1: This is the Model performance of the LinearRegression class without tuning the hyperparameters</figcaption>\n",
    "</figure>\n",
    "\n",
    "![This is the Model performance of the LinearRegression class without tuning the hyperparameters](plots/regression/sgdregression_without_hyperparameters.png)\n",
    "\n",
    "### Model Performance Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Treehouses', 'Chalets', 'Amazing pools', 'Offbeat', 'Beachfront'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('airbnb-property-listing/tabular_data/clean_listing.csv')\n",
    "df['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treehouses       207\n",
      "Chalets          194\n",
      "Offbeat          182\n",
      "Amazing pools    175\n",
      "Beachfront       132\n",
      "Name: Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "category_counts = df['Category'].value_counts()\n",
    "print(category_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scikit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anany\\OneDrive\\Desktop\\Github\\AIcore\\Modelling_Airbnbs_property_listing_dataset\\test_airbnb.ipynb Cell 59\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Anany/OneDrive/Desktop/Github/AIcore/Modelling_Airbnbs_property_listing_dataset/test_airbnb.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscikit\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_confusion_matrix\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scikit'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PyTorch Dataset called AirbnbNightlyPriceRegressionDataset that returns a tuple of (features, label) when indexed. The features should be a tensor of the numerical tabular features of the house. The second element is a scalar of the price per night.\n",
    "\n",
    "Create a dataloader for the train set and test set that shuffles the data. Further, split the train set into train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tabular_data import load_airbnb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbnbNightlyPriceRegressionDataset(Dataset):\n",
    "    def __init__(self,df = pd.read_csv('airbnb-property-listing/tabular_data/clean_listing.csv')) -> None:\n",
    "        super().__init__()\n",
    "        self.X,self.Y = load_airbnb(df,label=\"Price_Night\")\n",
    "        print(self.X.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.X.iloc[index]), torch.tensor(self.Y.iloc[index])\n",
    "    \n",
    "\n",
    "def split_data(dataset):\n",
    "    train_dataset, test_dataset = random_split(dataset, [int(len(dataset)* 0.875), len(dataset)-int(len(dataset)*0.875)])\n",
    "\n",
    "    train_dataset,validation_dataset = random_split(train_dataset, [int(len(train_dataset)*0.85), len(train_dataset)-int(len(train_dataset)*0.85)])\n",
    "\n",
    "    print(f\"\\tTraining: {len(train_dataset)}\")\n",
    "    print(f\"\\tValidation: {len(validation_dataset)}\")\n",
    "    print(f\"\\tTesting: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(890, 12)\n",
      "\tTraining: 661\n",
      "\tValidation: 117\n",
      "\tTesting: 112\n"
     ]
    }
   ],
   "source": [
    "dataset = AirbnbNightlyPriceRegressionDataset()\n",
    "batch_size = 16\n",
    "train_dataset, validation_dataset, test_dataset = split_data(dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size = batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a PyTorch model class containing the architecture for a fully connected neural network.\n",
    "\n",
    "To start with, it should only ingest the numerical tabular data. We will process the text and image features later.\n",
    "\n",
    "Don't train it yet. Instead, just ensure that it can perform a forward pass on a batch of data and produce an output of the correct shape.\n",
    "\n",
    "To to this, define the start of a function called train which takes in the model, the data loader, and the number of epochs. For now, just get the first batch of data from the dataloader and pass it through the model, then break out of the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anany\\AppData\\Local\\Temp\\ipykernel_16464\\946948608.py:27: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(predictions,labels)\n",
      "C:\\Users\\Anany\\AppData\\Local\\Temp\\ipykernel_16464\\946948608.py:27: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(predictions,labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79855.0703125\n",
      "71938.4375\n",
      "13047.1767578125\n",
      "17818.318359375\n",
      "45645.953125\n",
      "75197.9140625\n",
      "8587.2685546875\n",
      "16103.2646484375\n",
      "16404.14453125\n",
      "2822.174072265625\n",
      "9982.234375\n",
      "9645.6181640625\n",
      "13652.3447265625\n",
      "36795.54296875\n",
      "23548.26171875\n",
      "32572.87890625\n",
      "17190.751953125\n",
      "9810.9892578125\n",
      "29355.8203125\n",
      "6083.658203125\n",
      "23606.62109375\n",
      "41538.296875\n",
      "20868.458984375\n",
      "24582.0\n",
      "19318.44140625\n",
      "42825.48046875\n",
      "48336.5234375\n",
      "15080.6884765625\n",
      "32393.794921875\n",
      "37036.71484375\n",
      "17175.1328125\n",
      "9758.0419921875\n",
      "39359.26953125\n",
      "19415.33203125\n",
      "46992.98828125\n",
      "21149.3671875\n",
      "2783.848388671875\n",
      "27011.87890625\n",
      "38924.28515625\n",
      "18142.3203125\n",
      "77948.71875\n",
      "2195.530517578125\n",
      "12619.439453125\n",
      "17754.7734375\n",
      "39960.16796875\n",
      "20372.53515625\n",
      "35663.67578125\n",
      "40543.37109375\n",
      "16649.234375\n",
      "22692.232421875\n",
      "65094.20703125\n",
      "24533.19921875\n",
      "8587.3203125\n",
      "21363.67578125\n",
      "14504.0703125\n",
      "40078.6640625\n",
      "15071.3408203125\n",
      "21425.58203125\n",
      "5623.02734375\n",
      "14502.5537109375\n",
      "10355.77734375\n",
      "17878.21484375\n",
      "14572.5458984375\n",
      "32429.98046875\n",
      "43561.96875\n",
      "4833.30224609375\n",
      "8747.2421875\n",
      "43704.20703125\n",
      "24974.703125\n",
      "14908.6884765625\n",
      "10664.23046875\n",
      "16871.787109375\n",
      "15584.943359375\n",
      "54685.578125\n",
      "77490.0234375\n",
      "120761.8671875\n",
      "29701.6875\n",
      "22770.41015625\n",
      "40312.796875\n",
      "46020.69921875\n",
      "33529.30078125\n",
      "20193.568359375\n",
      "12798.6494140625\n",
      "26040.125\n",
      "30207.1171875\n",
      "17519.716796875\n",
      "49486.3203125\n",
      "15309.87890625\n",
      "14182.2421875\n",
      "86471.7578125\n",
      "8743.853515625\n",
      "12516.869140625\n",
      "25231.064453125\n",
      "67520.3046875\n",
      "69173.359375\n",
      "18200.802734375\n",
      "52565.921875\n",
      "75034.4140625\n",
      "7163.69677734375\n",
      "26907.953125\n",
      "8837.470703125\n",
      "6990.8076171875\n",
      "29720.525390625\n",
      "36813.765625\n",
      "9830.8125\n",
      "22450.99609375\n",
      "11130.578125\n",
      "15587.537109375\n",
      "5623.3701171875\n",
      "45820.765625\n",
      "13810.7197265625\n",
      "5095.302734375\n",
      "66159.890625\n",
      "41337.48046875\n",
      "17204.375\n",
      "22145.26953125\n",
      "11595.4814453125\n",
      "55206.1796875\n",
      "23047.59375\n",
      "14331.9111328125\n",
      "48561.94921875\n",
      "9964.2880859375\n",
      "21947.763671875\n",
      "7705.880859375\n",
      "8962.8486328125\n",
      "89452.578125\n",
      "6257.55615234375\n",
      "9996.275390625\n",
      "19771.59375\n",
      "12104.693359375\n",
      "9458.0791015625\n",
      "24108.853515625\n",
      "7057.90625\n",
      "11130.7646484375\n",
      "11957.201171875\n",
      "63024.0234375\n",
      "44466.3203125\n",
      "8332.591796875\n",
      "33147.74609375\n",
      "10710.3125\n",
      "18686.923828125\n",
      "35552.140625\n",
      "10790.517578125\n",
      "19314.482421875\n",
      "10259.1787109375\n",
      "26008.6171875\n",
      "17400.9296875\n",
      "46927.90625\n",
      "43187.37890625\n",
      "30378.263671875\n",
      "83324.671875\n",
      "10177.4296875\n",
      "16209.345703125\n",
      "77463.03125\n",
      "6582.07275390625\n",
      "17501.58203125\n",
      "28338.05078125\n",
      "10302.748046875\n",
      "14411.876953125\n",
      "27832.001953125\n",
      "24845.54296875\n",
      "25064.482421875\n",
      "125956.359375\n",
      "66178.265625\n",
      "40602.71484375\n",
      "16334.908203125\n",
      "37428.265625\n",
      "20722.966796875\n",
      "12093.0439453125\n",
      "46760.6171875\n",
      "18454.26953125\n",
      "12994.150390625\n",
      "87165.0625\n",
      "36877.71484375\n",
      "39119.3671875\n",
      "9401.5380859375\n",
      "9404.6484375\n",
      "23152.583984375\n",
      "30514.85546875\n",
      "80729.578125\n",
      "16608.6640625\n",
      "9837.5458984375\n",
      "7438.4658203125\n",
      "11589.4794921875\n",
      "12341.26171875\n",
      "11877.673828125\n",
      "6191.56494140625\n",
      "12781.708984375\n",
      "22841.478515625\n",
      "90240.4765625\n",
      "33862.44140625\n",
      "32301.4375\n",
      "18590.388671875\n",
      "37909.7734375\n",
      "17745.74609375\n",
      "51130.77734375\n",
      "15289.3984375\n",
      "49348.69921875\n",
      "21899.29296875\n",
      "8037.4140625\n",
      "50028.0625\n",
      "16190.79296875\n",
      "11283.421875\n",
      "22615.7578125\n",
      "44482.375\n",
      "69834.6015625\n",
      "21252.943359375\n",
      "12021.3759765625\n",
      "13816.26953125\n",
      "11671.7978515625\n",
      "91562.25\n",
      "32994.73046875\n",
      "14444.6171875\n",
      "69040.9453125\n",
      "45831.59765625\n",
      "14770.125\n",
      "14184.1455078125\n",
      "2709.78271484375\n",
      "9459.83203125\n",
      "61985.78515625\n",
      "22572.37109375\n",
      "21567.72265625\n",
      "44583.19921875\n",
      "11754.400390625\n",
      "17825.822265625\n",
      "65883.359375\n",
      "8021.73828125\n",
      "10525.6904296875\n",
      "15616.939453125\n",
      "16317.703125\n",
      "15976.6875\n",
      "8092.30419921875\n",
      "11771.236328125\n",
      "18501.24609375\n",
      "24179.044921875\n",
      "10845.5556640625\n",
      "6072.6884765625\n",
      "43538.20703125\n",
      "15341.7197265625\n",
      "12239.623046875\n",
      "16434.908203125\n",
      "51124.34375\n",
      "14100.8896484375\n",
      "73637.1640625\n",
      "4780.2001953125\n",
      "6656.4990234375\n",
      "41567.9921875\n",
      "79905.2578125\n",
      "38088.3359375\n",
      "51410.7890625\n",
      "25093.705078125\n",
      "3627.57666015625\n",
      "11874.498046875\n",
      "16784.48046875\n",
      "15321.7451171875\n",
      "39742.05859375\n",
      "50429.86328125\n",
      "12708.6630859375\n",
      "20458.96875\n",
      "25877.916015625\n",
      "10097.423828125\n",
      "35174.78515625\n",
      "19566.234375\n",
      "21818.712890625\n",
      "18858.982421875\n",
      "14192.1357421875\n",
      "97409.7734375\n",
      "12340.701171875\n",
      "9553.2958984375\n",
      "78215.7890625\n",
      "30552.7734375\n",
      "5145.59716796875\n",
      "36967.890625\n",
      "13981.466796875\n",
      "18917.947265625\n",
      "5792.9287109375\n",
      "54282.8046875\n",
      "23806.97265625\n",
      "12534.9072265625\n",
      "29681.412109375\n",
      "9554.908203125\n",
      "41208.140625\n",
      "21812.30859375\n",
      "9052.564453125\n",
      "25582.27734375\n",
      "26882.814453125\n",
      "38611.796875\n",
      "13165.54296875\n",
      "82585.078125\n",
      "13405.576171875\n",
      "76794.1171875\n",
      "51383.015625\n",
      "8084.16796875\n",
      "6050.51416015625\n",
      "68188.921875\n",
      "24934.720703125\n",
      "16605.525390625\n",
      "16396.06640625\n",
      "14042.640625\n",
      "46551.3359375\n",
      "53929.2734375\n",
      "5422.56640625\n",
      "34373.3828125\n",
      "22824.171875\n",
      "10824.2470703125\n",
      "10618.515625\n",
      "52095.58203125\n",
      "5066.93212890625\n",
      "158526.9375\n",
      "25979.015625\n",
      "52319.10546875\n",
      "3351.36572265625\n",
      "14195.8173828125\n",
      "16074.3173828125\n",
      "7201.240234375\n",
      "9001.66796875\n",
      "15320.892578125\n",
      "8344.8662109375\n",
      "14133.9306640625\n",
      "23963.17578125\n",
      "13527.25\n",
      "31706.34375\n",
      "21550.3203125\n",
      "7191.056640625\n",
      "27695.36328125\n",
      "31241.892578125\n",
      "34524.34375\n",
      "50957.390625\n",
      "60188.2265625\n",
      "17726.181640625\n",
      "7222.0498046875\n",
      "15815.5791015625\n",
      "84805.1328125\n",
      "4885.26611328125\n",
      "17525.765625\n",
      "7428.9638671875\n",
      "20614.404296875\n",
      "42753.171875\n",
      "11622.02734375\n",
      "39940.875\n",
      "13379.6103515625\n",
      "40920.3828125\n",
      "17079.83203125\n",
      "16462.734375\n",
      "10593.52734375\n",
      "8090.01220703125\n",
      "6388.29296875\n",
      "46975.171875\n",
      "66255.9375\n",
      "10126.6630859375\n",
      "8694.005859375\n",
      "19843.4140625\n",
      "20453.845703125\n",
      "80019.7734375\n",
      "13708.8564453125\n",
      "16501.388671875\n",
      "10105.314453125\n",
      "31957.978515625\n",
      "16661.525390625\n",
      "5863.8388671875\n",
      "57665.2890625\n",
      "82614.65625\n",
      "17585.583984375\n",
      "52321.59375\n",
      "43063.42578125\n",
      "21617.943359375\n",
      "24589.455078125\n",
      "72969.6796875\n",
      "43573.76171875\n",
      "9143.8125\n",
      "21425.1015625\n",
      "31470.884765625\n",
      "11133.287109375\n",
      "33938.2578125\n",
      "7014.5224609375\n",
      "15296.146484375\n",
      "25106.642578125\n",
      "35206.40625\n",
      "117960.71875\n",
      "37596.8359375\n",
      "13013.7939453125\n",
      "46421.3203125\n",
      "37357.375\n",
      "11332.498046875\n",
      "12115.83203125\n",
      "14884.9609375\n",
      "8901.71484375\n",
      "8032.94482421875\n",
      "40515.41015625\n",
      "31706.505859375\n",
      "100544.34375\n",
      "17103.677734375\n",
      "7466.9912109375\n",
      "8212.203125\n",
      "39635.16796875\n",
      "43881.953125\n",
      "17438.669921875\n",
      "8282.953125\n",
      "22701.138671875\n",
      "30770.76171875\n",
      "11116.18359375\n",
      "20352.732421875\n",
      "9820.5966796875\n",
      "8649.984375\n",
      "11925.8046875\n",
      "61753.6875\n",
      "19384.6015625\n",
      "22493.048828125\n",
      "16188.0986328125\n",
      "7216.19775390625\n",
      "25350.41796875\n",
      "33449.203125\n",
      "10979.783203125\n",
      "78044.609375\n",
      "15884.158203125\n",
      "6522.99462890625\n",
      "36068.5625\n",
      "33058.4765625\n",
      "47272.4296875\n",
      "24735.392578125\n"
     ]
    }
   ],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define layers and architecture here\n",
    "        self.layers = torch.nn.Sequential(\n",
    "        nn.Linear(12,16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "    \n",
    "\n",
    "model = NN()\n",
    "\n",
    "\n",
    "def train(model,train_loader,epochs=10):\n",
    "\n",
    "\n",
    "    for epoch in range (epochs):\n",
    "        for batch in train_loader:\n",
    "            features, labels = batch\n",
    "            features = features.float()\n",
    "            labels = labels.float()\n",
    "            predictions = model(features)\n",
    "            loss = F.mse_loss(predictions,labels)\n",
    "            loss.backward()\n",
    "            print(loss.item())\n",
    "\n",
    "\n",
    "train(model,train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anany\\AppData\\Local\\Temp\\ipykernel_16464\\72914350.py:14: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(predictions,labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74346.53125\n",
      "6347444.5\n",
      "25392204.0\n",
      "55882952.0\n",
      "95484840.0\n",
      "142093568.0\n",
      "189801120.0\n",
      "236522208.0\n",
      "277600448.0\n",
      "307712608.0\n",
      "332396352.0\n",
      "338345920.0\n",
      "336197216.0\n",
      "318091840.0\n",
      "289744320.0\n",
      "249819152.0\n",
      "201686592.0\n",
      "157252208.0\n",
      "110003760.0\n",
      "67922192.0\n",
      "33608492.0\n",
      "10734712.0\n",
      "451891.40625\n",
      "3943898.0\n",
      "20050970.0\n",
      "48571672.0\n",
      "88170432.0\n",
      "131289760.0\n",
      "179694048.0\n",
      "226740016.0\n",
      "269129280.0\n",
      "304220416.0\n",
      "328662400.0\n",
      "342756192.0\n",
      "338799840.0\n",
      "323523680.0\n",
      "294718976.0\n",
      "259180128.0\n",
      "214224256.0\n",
      "167253024.0\n",
      "119256672.0\n",
      "75372520.0\n",
      "39464848.0\n",
      "14391321.0\n",
      "1283866.25\n",
      "2158710.0\n",
      "16574468.0\n",
      "42324444.0\n",
      "79928728.0\n",
      "122724480.0\n",
      "170792736.0\n",
      "218566528.0\n",
      "264507408.0\n",
      "299497408.0\n",
      "322431744.0\n",
      "341520000.0\n",
      "336976128.0\n",
      "325748160.0\n",
      "301876032.0\n",
      "264868224.0\n",
      "224581280.0\n",
      "176014416.0\n",
      "127638752.0\n",
      "83741960.0\n",
      "45978968.0\n",
      "18164674.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anany\\AppData\\Local\\Temp\\ipykernel_16464\\72914350.py:14: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(predictions,labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2988421.0\n",
      "1072037.375\n",
      "12213029.0\n",
      "36026328.0\n",
      "70972216.0\n",
      "115143944.0\n",
      "161408176.0\n",
      "211357568.0\n",
      "252835424.0\n",
      "291749504.0\n",
      "320773760.0\n",
      "337211200.0\n",
      "340022976.0\n",
      "329555424.0\n",
      "306597312.0\n",
      "274293056.0\n",
      "232021696.0\n",
      "186829328.0\n",
      "136655008.0\n",
      "91502416.0\n",
      "52609520.0\n",
      "22586790.0\n",
      "4808687.0\n",
      "146195.28125\n",
      "8654319.0\n",
      "30763380.0\n",
      "64057524.0\n",
      "105537088.0\n",
      "151499680.0\n",
      "198533088.0\n",
      "246121920.0\n",
      "285201088.0\n",
      "316235616.0\n",
      "336656448.0\n",
      "342080992.0\n",
      "334189504.0\n",
      "315396480.0\n",
      "281279168.0\n",
      "241443136.0\n",
      "195366896.0\n",
      "147885920.0\n",
      "100348608.0\n",
      "58773580.0\n",
      "27326844.0\n",
      "7551305.5\n",
      "26021.8046875\n",
      "6247070.5\n",
      "25132104.0\n",
      "56106824.0\n",
      "96222616.0\n",
      "143035408.0\n",
      "190299360.0\n",
      "237538608.0\n",
      "278620160.0\n",
      "310870816.0\n",
      "332553344.0\n",
      "339616480.0\n",
      "337177440.0\n",
      "319824800.0\n",
      "289272352.0\n",
      "249061504.0\n",
      "203632864.0\n",
      "157600304.0\n",
      "109842656.0\n",
      "67694264.0\n",
      "33824200.0\n",
      "10707257.0\n",
      "400693.6875\n",
      "4095321.75\n",
      "20607724.0\n",
      "50142368.0\n",
      "88246008.0\n",
      "134002976.0\n",
      "181423904.0\n",
      "229364896.0\n",
      "272235872.0\n",
      "306395680.0\n",
      "329805344.0\n",
      "340323392.0\n",
      "340183808.0\n",
      "325260000.0\n",
      "295132096.0\n",
      "258364672.0\n",
      "213475680.0\n",
      "165581744.0\n",
      "118379584.0\n",
      "74662064.0\n",
      "38938952.0\n",
      "13851422.0\n",
      "1307123.625\n",
      "2247003.5\n",
      "16986630.0\n",
      "43284580.0\n",
      "79932528.0\n",
      "124522424.0\n",
      "173042192.0\n",
      "219104992.0\n",
      "261914112.0\n",
      "302496064.0\n",
      "326485408.0\n",
      "340113088.0\n",
      "338220864.0\n",
      "326423904.0\n",
      "303767904.0\n",
      "267784576.0\n",
      "222782432.0\n",
      "173721440.0\n",
      "127355728.0\n",
      "83456080.0\n",
      "45222936.0\n",
      "17186570.0\n",
      "2826557.5\n",
      "892496.6875\n",
      "12974436.0\n",
      "37293156.0\n",
      "72919040.0\n",
      "115623024.0\n",
      "162851632.0\n",
      "209839408.0\n",
      "256903376.0\n",
      "294365632.0\n",
      "322517280.0\n",
      "337320480.0\n",
      "342316544.0\n",
      "331002016.0\n",
      "308025216.0\n",
      "272032352.0\n",
      "232155104.0\n",
      "184202656.0\n",
      "136367456.0\n",
      "91516456.0\n",
      "52128608.0\n",
      "22264686.0\n",
      "4278833.5\n",
      "222763.890625\n",
      "9619668.0\n",
      "32395352.0\n",
      "65139260.0\n",
      "106712728.0\n",
      "153096800.0\n",
      "200546144.0\n",
      "248190720.0\n",
      "285973568.0\n",
      "317038592.0\n",
      "336224448.0\n",
      "339745184.0\n",
      "337653248.0\n",
      "312095008.0\n",
      "281813920.0\n",
      "239715920.0\n",
      "194183552.0\n",
      "145335568.0\n",
      "98043728.0\n",
      "57915480.0\n",
      "27417328.0\n",
      "6991332.0\n",
      "22697.7265625\n",
      "6797369.0\n",
      "26113610.0\n",
      "57956044.0\n",
      "98115488.0\n",
      "145287808.0\n",
      "190965568.0\n",
      "239473520.0\n",
      "279033952.0\n",
      "313108544.0\n",
      "332594304.0\n",
      "341777952.0\n",
      "336752640.0\n",
      "318555008.0\n",
      "285148160.0\n",
      "247679248.0\n",
      "204194784.0\n",
      "153510832.0\n",
      "108202384.0\n",
      "64880560.0\n",
      "32498112.0\n",
      "10025736.0\n",
      "292547.625\n",
      "4366240.0\n",
      "21461136.0\n",
      "50028128.0\n",
      "88909192.0\n",
      "134220480.0\n",
      "181403712.0\n",
      "229374784.0\n",
      "273686720.0\n",
      "306195264.0\n",
      "329076288.0\n",
      "340176832.0\n",
      "338327296.0\n",
      "321305408.0\n",
      "293726304.0\n",
      "258231648.0\n",
      "212420128.0\n",
      "164734496.0\n",
      "117699072.0\n",
      "73578784.0\n",
      "38282776.0\n",
      "13329284.0\n",
      "1311027.875\n",
      "2348081.75\n",
      "17188108.0\n",
      "44044416.0\n",
      "80794416.0\n",
      "125103240.0\n",
      "173785904.0\n",
      "219771072.0\n",
      "264589888.0\n",
      "301157312.0\n",
      "325964480.0\n",
      "339813312.0\n",
      "340348608.0\n",
      "326933952.0\n",
      "300694912.0\n",
      "264810400.0\n",
      "220669600.0\n",
      "172177696.0\n",
      "127625632.0\n",
      "82401320.0\n",
      "45175728.0\n",
      "17509444.0\n",
      "2406193.75\n",
      "1071809.875\n",
      "12891958.0\n",
      "37701364.0\n",
      "73362448.0\n",
      "116436296.0\n",
      "163760784.0\n",
      "211600912.0\n",
      "254428512.0\n",
      "292325856.0\n",
      "321182400.0\n",
      "336047040.0\n",
      "341690304.0\n",
      "332275040.0\n",
      "307596800.0\n",
      "271800608.0\n",
      "230883104.0\n",
      "183856224.0\n",
      "135849664.0\n",
      "90109760.0\n",
      "50912976.0\n",
      "22304418.0\n",
      "4703351.0\n",
      "253752.984375\n",
      "9609294.0\n",
      "31550356.0\n",
      "65768848.0\n",
      "107249368.0\n",
      "154153136.0\n",
      "201760848.0\n",
      "248034928.0\n",
      "286107296.0\n",
      "319267008.0\n",
      "334147456.0\n",
      "338561600.0\n",
      "334552256.0\n",
      "312804736.0\n",
      "280135712.0\n",
      "240255440.0\n",
      "193893024.0\n",
      "144787008.0\n",
      "98964240.0\n",
      "57594844.0\n",
      "27136062.0\n",
      "6880061.0\n",
      "75010.1796875\n",
      "6550985.5\n",
      "26541642.0\n",
      "57951664.0\n",
      "97834800.0\n",
      "143368816.0\n",
      "192140432.0\n",
      "240046928.0\n",
      "277654848.0\n",
      "313654816.0\n",
      "331796096.0\n",
      "340474944.0\n",
      "335111936.0\n",
      "315240480.0\n",
      "287798272.0\n",
      "246720432.0\n",
      "202158240.0\n",
      "153471376.0\n",
      "107138504.0\n",
      "65336056.0\n",
      "32321060.0\n",
      "9784287.0\n",
      "366529.6875\n",
      "4164339.75\n",
      "21531150.0\n",
      "51048648.0\n",
      "88009680.0\n",
      "134699616.0\n",
      "181671776.0\n",
      "229886080.0\n",
      "271809056.0\n",
      "307288576.0\n",
      "328992864.0\n",
      "341777344.0\n",
      "337694592.0\n",
      "322193216.0\n",
      "292269248.0\n",
      "256929600.0\n",
      "212580880.0\n",
      "162269760.0\n",
      "115564192.0\n",
      "73024000.0\n",
      "37733436.0\n",
      "13316920.0\n",
      "1265447.875\n",
      "2545901.0\n",
      "17092008.0\n",
      "44171856.0\n",
      "80405608.0\n",
      "124254968.0\n",
      "172685888.0\n",
      "219163152.0\n",
      "264727056.0\n",
      "301409408.0\n",
      "327073056.0\n",
      "338960544.0\n",
      "337453664.0\n",
      "326092544.0\n",
      "299768800.0\n",
      "266551648.0\n",
      "223985504.0\n",
      "171607472.0\n",
      "126028720.0\n",
      "82285136.0\n",
      "43625768.0\n",
      "17172598.0\n",
      "2488626.25\n",
      "1120030.25\n",
      "13208254.0\n",
      "37959508.0\n",
      "73195904.0\n",
      "116424752.0\n",
      "163700944.0\n",
      "209181008.0\n",
      "256830720.0\n",
      "293258560.0\n",
      "321355744.0\n",
      "338483872.0\n",
      "342178592.0\n",
      "331003456.0\n",
      "307780832.0\n",
      "273356096.0\n",
      "231074464.0\n",
      "183932480.0\n",
      "134666816.0\n",
      "90253920.0\n",
      "50210720.0\n"
     ]
    }
   ],
   "source": [
    "def train(model,train_loader,epochs=10):\n",
    "\n",
    "    optimiser = torch.optim.SGD(model.parameters(),lr = 0.01)\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    batch_idx = 0\n",
    "\n",
    "    for epoch in range (epochs):\n",
    "        for batch in train_loader:\n",
    "            features, labels = batch\n",
    "            features = features.float()\n",
    "            labels = labels.float()\n",
    "            predictions = model(features)\n",
    "            loss = F.mse_loss(predictions,labels)\n",
    "            loss.backward()\n",
    "            print(loss.item())\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad\n",
    "            writer.add_scalar('loss',loss.item(),batch_idx)\n",
    "            batch_idx += 1\n",
    "\n",
    "\n",
    "train(model,train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the training loop so that it iterates through every batch in the dataset for the specified number of epochs, and optimises the model parameters. You should add a step to evaluate the model performance on the validation dataset after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(890, 12)\n",
      "\tTraining: 661\n",
      "\tValidation: 117\n",
      "\tTesting: 112\n",
      "50360.91015625\n",
      "7317992375320576.0\n",
      "2302495686656.0\n",
      "2211399860224.0\n",
      "2123915067392.0\n",
      "2039607066624.0\n",
      "1959010762752.0\n",
      "1881251512320.0\n",
      "1806868938752.0\n",
      "1735296155648.0\n",
      "1666569207808.0\n",
      "1600602112000.0\n",
      "1537212940288.0\n",
      "1476406149120.0\n",
      "1417811066880.0\n",
      "1361638850560.0\n",
      "1307825668096.0\n",
      "1256049868800.0\n",
      "1206231236608.0\n",
      "1158485639168.0\n",
      "1112670208000.0\n",
      "1068486295552.0\n",
      "1026221342720.0\n",
      "985565822976.0\n",
      "946646810624.0\n",
      "909036486656.0\n",
      "873208217600.0\n",
      "838577422336.0\n",
      "805332123648.0\n",
      "773343936512.0\n",
      "742779912192.0\n",
      "713301557248.0\n",
      "685065306112.0\n",
      "657891852288.0\n",
      "631879696384.0\n",
      "606925750272.0\n",
      "582903922688.0\n",
      "559861727232.0\n",
      "537592496128.0\n",
      "516311416832.0\n",
      "495912910848.0\n",
      "476160196608.0\n",
      "457382821888.0\n",
      "439274209280.0\n",
      "421946589184.0\n",
      "405171109888.0\n",
      "389107482624.0\n",
      "373823143936.0\n",
      "358939557888.0\n",
      "344661426176.0\n",
      "331029872640.0\n",
      "317899702272.0\n",
      "305324490752.0\n",
      "293271830528.0\n",
      "281638043648.0\n",
      "270465957888.0\n",
      "259781476352.0\n",
      "249641189376.0\n",
      "239614164992.0\n",
      "230140805120.0\n",
      "221015523328.0\n",
      "212228734976.0\n",
      "203892359168.0\n",
      "195774791680.0\n",
      "188018425856.0\n",
      "180661583872.0\n",
      "173467402240.0\n",
      "166549454848.0\n",
      "159990218752.0\n",
      "153669271552.0\n",
      "147564986368.0\n",
      "141697155072.0\n",
      "136108523520.0\n",
      "130719809536.0\n",
      "125533650944.0\n",
      "120544206848.0\n",
      "115782926336.0\n",
      "111186886656.0\n",
      "106771300352.0\n",
      "102555648000.0\n",
      "98521432064.0\n",
      "94596505600.0\n",
      "90842210304.0\n",
      "87304847360.0\n",
      "83790471168.0\n",
      "80507568128.0\n",
      "77292781568.0\n",
      "74246979584.0\n",
      "71324172288.0\n",
      "68512768000.0\n",
      "65755303936.0\n",
      "63172837376.0\n",
      "60674981888.0\n",
      "58238017536.0\n",
      "55961149440.0\n",
      "53726920704.0\n",
      "51583836160.0\n",
      "49565437952.0\n",
      "47601709056.0\n",
      "45718724608.0\n",
      "43891306496.0\n",
      "42162970624.0\n",
      "40490196992.0\n",
      "38900002816.0\n",
      "37342937088.0\n",
      "35866480640.0\n",
      "34456248320.0\n",
      "33086656512.0\n",
      "31774461952.0\n",
      "30532583424.0\n",
      "29312772096.0\n",
      "28136939520.0\n",
      "27039090688.0\n",
      "25957267456.0\n",
      "24935286784.0\n",
      "23937087488.0\n",
      "23001194496.0\n",
      "22088079360.0\n",
      "21238530048.0\n",
      "20380440576.0\n",
      "19565680640.0\n",
      "18811367424.0\n",
      "18036533248.0\n",
      "17338558464.0\n",
      "16640462848.0\n",
      "15971305472.0\n",
      "15350733824.0\n",
      "14742691840.0\n",
      "14164240384.0\n",
      "13615738880.0\n",
      "13073071104.0\n",
      "12538146816.0\n",
      "12058816512.0\n",
      "11585423360.0\n",
      "11108327424.0\n",
      "10675201024.0\n",
      "10246944768.0\n",
      "9839894528.0\n",
      "9457354752.0\n",
      "9082658816.0\n",
      "8713572352.0\n",
      "8383917056.0\n",
      "8035351552.0\n",
      "7725806592.0\n",
      "7416912384.0\n",
      "7125605376.0\n",
      "6840300032.0\n",
      "6578288640.0\n",
      "6325090816.0\n",
      "6058994688.0\n",
      "5819445760.0\n",
      "5594305024.0\n",
      "5365242880.0\n",
      "5162751488.0\n",
      "4957059584.0\n",
      "4759452672.0\n",
      "4563215360.0\n",
      "4387058176.0\n",
      "4217084416.0\n",
      "4040468480.0\n",
      "3888656640.0\n",
      "3735577600.0\n",
      "3591849984.0\n",
      "3442225152.0\n",
      "3310163712.0\n",
      "3173847808.0\n",
      "3049591552.0\n",
      "2928487936.0\n",
      "2813517056.0\n",
      "2699141376.0\n",
      "2590170112.0\n",
      "2489132032.0\n",
      "2396285440.0\n",
      "2304993280.0\n",
      "2213106176.0\n",
      "2124222464.0\n",
      "2033873280.0\n",
      "1955399168.0\n",
      "1878614144.0\n",
      "1803266688.0\n",
      "1734860672.0\n",
      "1661549312.0\n",
      "1601714176.0\n",
      "1533040128.0\n",
      "1474124416.0\n",
      "1415477376.0\n",
      "1355239168.0\n",
      "1306551680.0\n",
      "1252632704.0\n",
      "1205894016.0\n",
      "1159644672.0\n",
      "1111269760.0\n",
      "1065542784.0\n",
      "1026630784.0\n",
      "985441280.0\n",
      "946829312.0\n",
      "906161536.0\n",
      "871292480.0\n",
      "839079488.0\n",
      "800701376.0\n",
      "769322368.0\n",
      "738954048.0\n",
      "713635648.0\n",
      "688574336.0\n",
      "654658688.0\n",
      "628866560.0\n",
      "606476544.0\n",
      "581698048.0\n",
      "561025472.0\n",
      "534584576.0\n",
      "515822464.0\n",
      "496166400.0\n",
      "473741472.0\n",
      "456355328.0\n",
      "436978624.0\n",
      "423734528.0\n",
      "404068928.0\n",
      "388481984.0\n",
      "373524544.0\n",
      "357169792.0\n",
      "343887456.0\n",
      "331592672.0\n",
      "316807616.0\n",
      "305629920.0\n",
      "291962464.0\n",
      "283505312.0\n",
      "269881952.0\n",
      "260823312.0\n",
      "248486000.0\n",
      "239057328.0\n",
      "228697424.0\n",
      "220794240.0\n",
      "211725136.0\n",
      "204452544.0\n",
      "195622064.0\n",
      "188025072.0\n",
      "182111088.0\n",
      "172406464.0\n",
      "167775040.0\n",
      "159360928.0\n",
      "152868000.0\n",
      "146268688.0\n",
      "140104768.0\n",
      "136218336.0\n",
      "130092760.0\n",
      "124823760.0\n",
      "120053608.0\n",
      "115375384.0\n",
      "113069664.0\n",
      "106712056.0\n",
      "102305544.0\n",
      "98194544.0\n",
      "93717992.0\n",
      "91869968.0\n",
      "88191312.0\n",
      "83558400.0\n",
      "80687216.0\n",
      "77833296.0\n",
      "75092656.0\n",
      "71807120.0\n",
      "68179272.0\n",
      "65950948.0\n",
      "63456864.0\n",
      "60519888.0\n",
      "58119380.0\n",
      "56090312.0\n",
      "53182384.0\n",
      "51535384.0\n",
      "49949120.0\n",
      "46842292.0\n",
      "44849880.0\n",
      "44333984.0\n",
      "41506644.0\n",
      "40300424.0\n",
      "38569000.0\n",
      "37238244.0\n",
      "35891060.0\n",
      "35048176.0\n",
      "32807860.0\n",
      "32946054.0\n",
      "30346516.0\n",
      "28861174.0\n",
      "27808060.0\n",
      "27295770.0\n",
      "25603682.0\n",
      "24640624.0\n",
      "23791810.0\n",
      "22818856.0\n",
      "21826212.0\n",
      "20552246.0\n",
      "20460232.0\n",
      "19430242.0\n",
      "18318208.0\n",
      "17703810.0\n",
      "16906786.0\n",
      "16876140.0\n",
      "16093098.0\n",
      "15678792.0\n",
      "14693586.0\n",
      "14030316.0\n",
      "13606873.0\n",
      "12878345.0\n",
      "12118688.0\n",
      "12144450.0\n",
      "11554441.0\n",
      "10926448.0\n",
      "10649458.0\n",
      "10397183.0\n",
      "10065566.0\n",
      "9168550.0\n",
      "9365478.0\n",
      "9088282.0\n",
      "8118816.5\n",
      "8480951.0\n",
      "7577433.0\n",
      "7550420.0\n",
      "7286236.0\n",
      "6995057.0\n",
      "6512607.0\n",
      "6358633.0\n",
      "6188965.0\n",
      "5933590.5\n",
      "5497659.5\n",
      "5283469.0\n",
      "5042693.5\n",
      "4881549.0\n",
      "4759525.5\n",
      "4406356.0\n",
      "4424343.0\n",
      "4153022.0\n",
      "4081079.0\n",
      "3852278.0\n",
      "3706658.25\n",
      "3647328.0\n",
      "3829083.5\n",
      "3269068.5\n",
      "3075410.75\n",
      "3180720.25\n",
      "3063725.0\n",
      "2813020.25\n",
      "2601540.5\n",
      "2592127.5\n",
      "2545943.0\n",
      "2261166.75\n",
      "2203918.5\n",
      "2250071.0\n",
      "2164916.25\n",
      "2179115.25\n",
      "1960469.875\n",
      "1785690.5\n",
      "1769499.625\n",
      "1704279.875\n",
      "1681712.125\n",
      "1536484.125\n",
      "1589389.5\n",
      "1622547.0\n",
      "1348215.25\n",
      "1398312.5\n",
      "1410730.375\n",
      "1276715.0\n",
      "1171357.75\n",
      "1063972.875\n",
      "1061256.375\n",
      "1036939.9375\n",
      "1087041.75\n",
      "956212.25\n",
      "925850.5625\n",
      "984175.1875\n",
      "1254242.875\n",
      "783917.875\n",
      "857801.625\n",
      "749660.25\n",
      "754508.25\n",
      "878909.0625\n",
      "682484.75\n",
      "639741.125\n",
      "687281.6875\n",
      "500534.1875\n",
      "545198.4375\n",
      "592066.0\n",
      "663130.9375\n",
      "526765.125\n",
      "456926.625\n",
      "488025.3125\n",
      "450326.09375\n",
      "433802.3125\n",
      "543540.75\n",
      "543637.125\n",
      "458086.125\n",
      "333728.375\n",
      "324818.34375\n",
      "448396.03125\n",
      "380795.09375\n",
      "363767.125\n",
      "237931.015625\n",
      "316141.0625\n",
      "265112.6875\n",
      "276178.84375\n",
      "198275.328125\n",
      "293490.0\n",
      "228644.84375\n",
      "216712.15625\n",
      "300884.875\n",
      "278661.78125\n",
      "202107.53125\n",
      "188532.359375\n",
      "200715.28125\n",
      "177200.03125\n",
      "177263.71875\n",
      "160878.296875\n",
      "231674.03125\n",
      "239726.46875\n",
      "182111.703125\n",
      "114309.5390625\n",
      "105484.203125\n",
      "162360.59375\n",
      "104869.46875\n",
      "93600.6953125\n",
      "104911.1484375\n",
      "161098.46875\n",
      "384287.51439604984 \t 118219.82010688781 \n",
      " -4569790937.00087 \t -9018667.25294384\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tabular_data import load_airbnb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AirbnbNightlyPriceRegressionDataset(Dataset):\n",
    "    def __init__(self,df = pd.read_csv('airbnb-property-listing/tabular_data/clean_listing.csv')) -> None:\n",
    "        super().__init__()\n",
    "        self.X,self.y = load_airbnb(df,label=\"Price_Night\")\n",
    "        print(self.X.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.X.iloc[index]), torch.tensor(self.y.iloc[index])\n",
    "    \n",
    "\n",
    "def split_data(dataset):\n",
    "    train_dataset, test_dataset = random_split(dataset, [int(len(dataset)* 0.875), len(dataset)-int(len(dataset)*0.875)])\n",
    "\n",
    "    train_dataset,validation_dataset = random_split(train_dataset, [int(len(train_dataset)*0.85), len(train_dataset)-int(len(train_dataset)*0.85)])\n",
    "\n",
    "\n",
    "    print(f\"\\tTraining: {len(train_dataset)}\")\n",
    "    print(f\"\\tValidation: {len(validation_dataset)}\")\n",
    "    print(f\"\\tTesting: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n",
    "\n",
    "dataset = AirbnbNightlyPriceRegressionDataset()\n",
    "batch_size = 16\n",
    "train_dataset, validation_dataset, test_dataset = split_data(dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size = batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define layers and architecture here\n",
    "        self.layers = torch.nn.Sequential(\n",
    "        nn.Linear(12,16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "    \n",
    "\n",
    "model = NN()\n",
    "\n",
    "\n",
    "def train(model,train_loader,validation_loader,epochs=10):\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    optimiser = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "    writer = SummaryWriter()\n",
    "    batch_idx = 0\n",
    "    train_rmse_loss = 0\n",
    "    val_rmse_loss = 0\n",
    "    training_r2 = 0\n",
    "    validation_r2 = 0\n",
    "\n",
    "    for epoch in range (epochs):\n",
    "        for batch in train_loader:\n",
    "            X_train,y_train = batch\n",
    "            X_train = X_train.type(torch.float32)\n",
    "            #X_train = torch.tensor(scaler.fit_transform(X_train))\n",
    "            y_train = y_train.type(torch.float32)\n",
    "            y_train = y_train.view(-1,1)\n",
    "            train_prediction = model(X_train)\n",
    "            train_loss = F.mse_loss(train_prediction,y_train)\n",
    "            train_loss = train_loss.type(torch.float32)\n",
    "            train_loss.backward()\n",
    "            print(train_loss.item())\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "            writer.add_scalar('training_loss',train_loss.item(),batch_idx)\n",
    "            batch_idx += 1\n",
    "            rmse_loss = torch.sqrt(train_loss)\n",
    "            train_rmse_loss += rmse_loss.item()\n",
    "            train_r2 = r2_score(y_train.detach().numpy(), train_prediction.detach().numpy())\n",
    "            writer.add_scalar('training_r2', train_r2, batch_idx)\n",
    "            training_r2 += train_r2\n",
    "  \n",
    "\n",
    "        for batch in validation_loader:\n",
    "            X_val,y_val = batch\n",
    "            X_val = X_val.type(torch.float32)\n",
    "            #X_val = torch.tensor(scaler.fit_transform(X_val))\n",
    "            y_val = y_val.type(torch.float32)\n",
    "            y_val = y_val.view(-1, 1)\n",
    "            val_prediction = model(X_val)\n",
    "            val_loss = F.mse_loss(val_prediction,y_val)\n",
    "            val_loss = val_loss.type(torch.float32)\n",
    "            writer.add_scalar('validation_loss',val_loss.item(),batch_idx)\n",
    "            rmse_loss = torch.sqrt(val_loss)\n",
    "            val_rmse_loss += rmse_loss.item()\n",
    "\n",
    "            val_r2 = r2_score(y_val.detach().numpy(), val_prediction.detach().numpy())\n",
    "            writer.add_scalar('validation_r2', val_r2, batch_idx)\n",
    "            validation_r2 += val_r2\n",
    "\n",
    "    train_rmse_loss = train_rmse_loss/(epochs*len(train_loader))\n",
    "    val_rmse_loss = val_rmse_loss/(epochs*len(validation_loader))\n",
    "    training_r2 = training_r2/(epochs*len(train_loader))\n",
    "    validation_r2 = validation_r2/(epochs*len(validation_loader))\n",
    "\n",
    "    print(train_rmse_loss,'\\t',val_rmse_loss,'\\n',training_r2,'\\t',validation_r2)\n",
    "\n",
    "\n",
    "train(model,train_loader,validation_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a YAML file called nn_config.yaml, next to your modelling.py file, that defines the architecture of the neural network.\n",
    "\n",
    "Specify:\n",
    "\n",
    "The name of the optimiser used under a key called optimiser\n",
    "The learning rate\n",
    "The width of each hidden layer under a key called hidden_layer_width (For simplicity, make all of the hidden layers the same width)\n",
    "The depth of the model\n",
    "Then, define a function called get_nn_config which reads in this file and returns it as a dictionary.\n",
    "\n",
    "Pass the config into your train function as the hyperparameter dictionary which you define earlier.\n",
    "\n",
    "Specify a keyword argument called \"config\" which must be passed to your model class upon initialisation.\n",
    "\n",
    "Your network should then use that config to set the corresponding hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(890, 11)\n",
      "\tTraining: 661\n",
      "\tValidation: 117\n",
      "\tTesting: 112\n",
      "24299.98046875\n",
      "2125616.5\n",
      "29200.6640625\n",
      "63550.37109375\n",
      "48436.71484375\n",
      "27551.830078125\n",
      "23151.6015625\n",
      "137900.8125\n",
      "50107.7734375\n",
      "32045.06640625\n",
      "15574.578125\n",
      "45444.9765625\n",
      "61764.31640625\n",
      "32600.271484375\n",
      "42111.66015625\n",
      "24801.2890625\n",
      "48804.30859375\n",
      "29310.712890625\n",
      "17769.91015625\n",
      "13185214464.0\n",
      "5274263.0\n",
      "5254326.5\n",
      "5074803.0\n",
      "4575399.0\n",
      "4507879.5\n",
      "4334583.0\n",
      "4248119.5\n",
      "3949467.5\n",
      "3819284.75\n",
      "3865109.75\n",
      "3627288.5\n",
      "3455473.5\n",
      "3283108.5\n",
      "3085965.0\n",
      "3015773.0\n",
      "3115557.0\n",
      "3017009.5\n",
      "2718989.25\n",
      "2667573.0\n",
      "2417226.25\n",
      "2501353.0\n",
      "2364543.25\n",
      "2131716.5\n",
      "2282125.75\n",
      "2015729.0\n",
      "1945326.0\n",
      "1836175.0\n",
      "1882777.75\n",
      "1748271.125\n",
      "1559702.75\n",
      "1491552.5\n",
      "1612133.25\n",
      "1619310.5\n",
      "1343701.0\n",
      "1357825.5\n",
      "1389750.25\n",
      "1294485.125\n",
      "1181809.25\n",
      "1362904.25\n",
      "1000977.625\n",
      "1054761.875\n",
      "993071.8125\n",
      "1007311.875\n",
      "1016005.375\n",
      "843238.1875\n",
      "781434.0625\n",
      "877666.3125\n",
      "776722.0\n",
      "760743.75\n",
      "798708.625\n",
      "825662.25\n",
      "716272.375\n",
      "747417.0625\n",
      "644895.375\n",
      "624002.5\n",
      "566984.4375\n",
      "498284.71875\n",
      "665648.625\n",
      "523266.9375\n",
      "591460.625\n",
      "560600.0625\n",
      "427436.25\n",
      "427206.375\n",
      "686487.9375\n",
      "412497.125\n",
      "333202.3125\n",
      "332449.15625\n",
      "329624.09375\n",
      "338029.84375\n",
      "300067.8125\n",
      "310633.75\n",
      "341281.96875\n",
      "420637.875\n",
      "279934.0625\n",
      "281255.71875\n",
      "377367.8125\n",
      "221560.59375\n",
      "252137.28125\n",
      "217417.828125\n",
      "322177.3125\n",
      "158519.0\n",
      "187455.953125\n",
      "217393.109375\n",
      "256518.75\n",
      "271483.75\n",
      "184747.625\n",
      "201487.578125\n",
      "151578.5\n",
      "159207.375\n",
      "218841.765625\n",
      "138284.171875\n",
      "128065.0625\n",
      "173409.09375\n",
      "113418.046875\n",
      "181727.390625\n",
      "113347.4375\n",
      "157129.875\n",
      "130874.46875\n",
      "94700.6953125\n",
      "131095.125\n",
      "108958.5078125\n",
      "103965.640625\n",
      "78914.421875\n",
      "105247.0625\n",
      "63414.5078125\n",
      "62401.88671875\n",
      "63510.390625\n",
      "167227.90625\n",
      "80908.875\n",
      "83166.296875\n",
      "97765.953125\n",
      "49722.65625\n",
      "56623.875\n",
      "105442.875\n",
      "39610.43359375\n",
      "56358.54296875\n",
      "59169.90625\n",
      "63614.0546875\n",
      "57988.21484375\n",
      "37593.31640625\n",
      "95448.2578125\n",
      "76439.390625\n",
      "40995.6328125\n",
      "27123.150390625\n",
      "65701.6875\n",
      "38717.52734375\n",
      "38078.6328125\n",
      "80798.6875\n",
      "32542.810546875\n",
      "31957.91015625\n",
      "64391.36328125\n",
      "14708.951171875\n",
      "41817.921875\n",
      "34056.12109375\n",
      "19192.0390625\n",
      "11845.5791015625\n",
      "39767.96875\n",
      "37995.78125\n",
      "14315.162109375\n",
      "19023.533203125\n",
      "112639.203125\n",
      "49559.4296875\n",
      "14149.126953125\n",
      "19179.060546875\n",
      "37035.6953125\n",
      "24177.087890625\n",
      "93335.53125\n",
      "13493.662109375\n",
      "40122.1484375\n",
      "11730.1953125\n",
      "24882.640625\n",
      "27356.732421875\n",
      "18111.333984375\n",
      "52581.4140625\n",
      "53752.93359375\n",
      "10995.2041015625\n",
      "5702.35888671875\n",
      "23513.513671875\n",
      "48749.703125\n",
      "8364.58984375\n",
      "60633.94140625\n",
      "10052.05078125\n",
      "4968.0146484375\n",
      "36225.5390625\n",
      "13641.646484375\n",
      "22499.03515625\n",
      "10568.423828125\n",
      "62267.16015625\n",
      "16436.29296875\n",
      "7620.75048828125\n",
      "2585.901123046875\n",
      "11186.392578125\n",
      "23022.056640625\n",
      "75293.671875\n",
      "5473.998046875\n",
      "2138.16015625\n",
      "1465.7723388671875\n",
      "72345.5625\n",
      "29168.30859375\n",
      "8506.7353515625\n",
      "13338.408203125\n",
      "27829.9609375\n",
      "5980.57373046875\n",
      "4071.53564453125\n",
      "16213.6337890625\n",
      "14840.388671875\n",
      "29321.142578125\n",
      "26702.3359375\n",
      "17203.068359375\n",
      "7618.1884765625\n",
      "70391.28125\n",
      "14878.4697265625\n",
      "8327.67578125\n",
      "23339.21875\n",
      "16939.17578125\n",
      "41851.109375\n",
      "21145.408203125\n",
      "5212.72021484375\n",
      "13159.7177734375\n",
      "5403.57080078125\n",
      "31909.552734375\n",
      "4212.13623046875\n",
      "47511.8671875\n",
      "21842.251953125\n",
      "5149.1806640625\n",
      "33204.23046875\n",
      "16162.380859375\n",
      "10606.8974609375\n",
      "11042.5361328125\n",
      "9807.4306640625\n",
      "7288.591796875\n",
      "15483.357421875\n",
      "6109.92626953125\n",
      "4372.640625\n",
      "65259.76171875\n",
      "12468.375\n",
      "15265.2587890625\n",
      "1793.189453125\n",
      "39006.66796875\n",
      "25252.5078125\n",
      "11490.783203125\n",
      "32305.603515625\n",
      "4602.21630859375\n",
      "6103.86474609375\n",
      "13893.9375\n",
      "10216.1572265625\n",
      "17450.224609375\n",
      "4091.58642578125\n",
      "3747.30712890625\n",
      "14839.818359375\n",
      "9276.76953125\n",
      "1801.927001953125\n",
      "4234.86376953125\n",
      "11496.3662109375\n",
      "48277.60546875\n",
      "81937.859375\n",
      "4923.6640625\n",
      "16594.23828125\n",
      "5043.41943359375\n",
      "4135.93896484375\n",
      "35906.203125\n",
      "44066.9765625\n",
      "9088.4453125\n",
      "18983.544921875\n",
      "10991.2197265625\n",
      "7805.41845703125\n",
      "10231.75\n",
      "3215.89208984375\n",
      "24368.29296875\n",
      "19685.54296875\n",
      "4117.60693359375\n",
      "25323.626953125\n",
      "30365.314453125\n",
      "26789.978515625\n",
      "33164.0078125\n",
      "9016.255859375\n",
      "22689.1953125\n",
      "41890.22265625\n",
      "12688.521484375\n",
      "4057.2578125\n",
      "3319.364501953125\n",
      "7940.7880859375\n",
      "12027.4228515625\n",
      "3894.850830078125\n",
      "6614.05419921875\n",
      "5650.728515625\n",
      "22340.498046875\n",
      "19411.453125\n",
      "7215.11083984375\n",
      "14933.240234375\n",
      "4628.015625\n",
      "5954.3837890625\n",
      "4527.5126953125\n",
      "3116.6826171875\n",
      "2422.657958984375\n",
      "4638.7119140625\n",
      "3842.72509765625\n",
      "25063.13671875\n",
      "17688.794921875\n",
      "56064.95703125\n",
      "14548.90234375\n",
      "5288.67578125\n",
      "9580.236328125\n",
      "9614.453125\n",
      "5415.15283203125\n",
      "4291.734375\n",
      "27874.453125\n",
      "10205.796875\n",
      "11390.0966796875\n",
      "18517.14453125\n",
      "44777.6796875\n",
      "9288.9853515625\n",
      "98898.59375\n",
      "26107.96484375\n",
      "12765.912109375\n",
      "8453.2763671875\n",
      "16098.4462890625\n",
      "3103.0\n",
      "3626.904541015625\n",
      "6132.2041015625\n",
      "13878.77734375\n",
      "54486.0234375\n",
      "18942.17578125\n",
      "3614.18359375\n",
      "11816.6005859375\n",
      "5900.49462890625\n",
      "3595.31689453125\n",
      "36367.5703125\n",
      "4360.7255859375\n",
      "8266.9404296875\n",
      "5584.09814453125\n",
      "23070.080078125\n",
      "4328.0078125\n",
      "12296.8349609375\n",
      "18542.103515625\n",
      "5065.63916015625\n",
      "16582.26953125\n",
      "6546.1005859375\n",
      "18434.3828125\n",
      "4492.36279296875\n",
      "21501.654296875\n",
      "3366.08984375\n",
      "8416.751953125\n",
      "3319.2138671875\n",
      "5052.1669921875\n",
      "2128.263916015625\n",
      "45959.2109375\n",
      "7539.3974609375\n",
      "13385.201171875\n",
      "63914.375\n",
      "5375.8134765625\n",
      "6008.67138671875\n",
      "6724.0390625\n",
      "3589.19091796875\n",
      "29363.689453125\n",
      "11977.0576171875\n",
      "12154.0966796875\n",
      "4990.9921875\n",
      "6636.392578125\n",
      "10112.244140625\n",
      "4227.34814453125\n",
      "73805.296875\n",
      "24795.87890625\n",
      "4739.8876953125\n",
      "30156.802734375\n",
      "11020.833984375\n",
      "6493.83349609375\n",
      "22465.8125\n",
      "8920.326171875\n",
      "41372.41015625\n",
      "38318.28125\n",
      "5364.6201171875\n",
      "20493.9375\n",
      "14342.0361328125\n",
      "8377.861328125\n",
      "24138.2578125\n",
      "12578.6279296875\n",
      "36795.1953125\n",
      "31847.388671875\n",
      "5632.96875\n",
      "9559.671875\n",
      "6990.58447265625\n",
      "3710.54248046875\n",
      "12138.8671875\n",
      "11656.55078125\n",
      "55564.37109375\n",
      "9859.357421875\n",
      "46504.8671875\n",
      "42367.19140625\n",
      "4063.018798828125\n",
      "6966.669921875\n",
      "6242.97021484375\n",
      "25330.916015625\n",
      "8534.697265625\n",
      "9184.541015625\n",
      "22076.494140625\n",
      "10960.0927734375\n",
      "63151.17578125\n",
      "5845.8095703125\n",
      "6510.12646484375\n",
      "11221.240234375\n",
      "4547.248046875\n",
      "3746.197509765625\n",
      "12133.525390625\n",
      "6256.73193359375\n",
      "19970.912109375\n",
      "11157.00390625\n",
      "28628.7890625\n",
      "4985.01708984375\n",
      "5734.505859375\n",
      "17119.478515625\n",
      "69701.828125\n",
      "15037.990234375\n",
      "11608.228515625\n",
      "14522.6748046875\n",
      "5570.53662109375\n",
      "4273.46923828125\n",
      "10213.8203125\n",
      "17233.076171875\n",
      "5840.63916015625\n",
      "626.27115973518 \t 324.3483664512634 \n",
      " -4628.17733161017 \t -45.51073493418418\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tabular_data import load_airbnb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import yaml\n",
    "\n",
    "\n",
    "class AirbnbNightlyPriceRegressionDataset(Dataset):\n",
    "    def __init__(self,df = pd.read_csv('airbnb-property-listing/tabular_data/clean_listing.csv',index_col='Unnamed: 0')) -> None:\n",
    "        super().__init__()\n",
    "        self.X,self.y = load_airbnb(df,label=\"Price_Night\")\n",
    "        print(self.X.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.X.iloc[index]), torch.tensor(self.y.iloc[index])\n",
    "    \n",
    "\n",
    "def split_data(dataset):\n",
    "    train_dataset, test_dataset = random_split(dataset, [int(len(dataset)* 0.875), len(dataset)-int(len(dataset)*0.875)])\n",
    "\n",
    "    train_dataset,validation_dataset = random_split(train_dataset, [int(len(train_dataset)*0.85), len(train_dataset)-int(len(train_dataset)*0.85)])\n",
    "\n",
    "    print(f\"\\tTraining: {len(train_dataset)}\")\n",
    "    print(f\"\\tValidation: {len(validation_dataset)}\")\n",
    "    print(f\"\\tTesting: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n",
    "\n",
    "dataset = AirbnbNightlyPriceRegressionDataset()\n",
    "batch_size = 16\n",
    "train_dataset, validation_dataset, test_dataset = split_data(dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size = batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        # Define layers and architecture here\n",
    "        self.hidden_layer_width = config['hidden_layer_width']\n",
    "        self.layers = torch.nn.Sequential(\n",
    "        nn.Linear(11,self.hidden_layer_width),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.hidden_layer_width,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "    \n",
    "\n",
    "def get_nn_config(config_file='nn_config.yaml'):\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "def train(train_loader,validation_loader,config,epochs=10):\n",
    "\n",
    "    #scaler = MinMaxScaler()\n",
    "    model = NN(config)\n",
    "    optimiser_name = config['optimiser']\n",
    "    optimiser_class = getattr(torch.optim, optimiser_name)\n",
    "    optimiser = optimiser_class(model.parameters(), lr=config['learning_rate'])\n",
    "    writer = SummaryWriter()\n",
    "    batch_idx = 0\n",
    "    train_rmse_loss = 0\n",
    "    val_rmse_loss = 0\n",
    "    training_r2 = 0\n",
    "    validation_r2 = 0\n",
    "\n",
    "    for epoch in range (epochs):\n",
    "        for batch in train_loader:\n",
    "            X_train,y_train = batch\n",
    "            X_train = X_train.type(torch.float32)\n",
    "            #X_train = torch.tensor(scaler.fit_transform(X_train))\n",
    "            y_train = y_train.type(torch.float32)\n",
    "            y_train = y_train.view(-1,1)\n",
    "            train_prediction = model(X_train)\n",
    "            train_loss = F.mse_loss(train_prediction,y_train)\n",
    "            train_loss = train_loss.type(torch.float32)\n",
    "            train_loss.backward()\n",
    "            print(train_loss.item())\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "            writer.add_scalar('training_loss',train_loss.item(),batch_idx)\n",
    "            batch_idx += 1\n",
    "            rmse_loss = torch.sqrt(train_loss)\n",
    "            train_rmse_loss += rmse_loss.item()\n",
    "            train_r2 = r2_score(y_train.detach().numpy(), train_prediction.detach().numpy())\n",
    "            writer.add_scalar('training_r2', train_r2, batch_idx)\n",
    "            training_r2 += train_r2\n",
    "  \n",
    "\n",
    "        for batch in validation_loader:\n",
    "            X_val,y_val = batch\n",
    "            X_val = X_val.type(torch.float32)\n",
    "            #X_val = torch.tensor(scaler.fit_transform(X_val))\n",
    "            y_val = y_val.type(torch.float32)\n",
    "            y_val = y_val.view(-1, 1)\n",
    "            val_prediction = model(X_val)\n",
    "            val_loss = F.mse_loss(val_prediction,y_val)\n",
    "            val_loss = val_loss.type(torch.float32)\n",
    "            writer.add_scalar('validation_loss',val_loss.item(),batch_idx)\n",
    "            rmse_loss = torch.sqrt(val_loss)\n",
    "            val_rmse_loss += rmse_loss.item()\n",
    "\n",
    "            val_r2 = r2_score(y_val.detach().numpy(), val_prediction.detach().numpy())\n",
    "            writer.add_scalar('validation_r2', val_r2, batch_idx)\n",
    "            validation_r2 += val_r2\n",
    "\n",
    "    train_rmse_loss = train_rmse_loss/(epochs*len(train_loader))\n",
    "    val_rmse_loss = val_rmse_loss/(epochs*len(validation_loader))\n",
    "    training_r2 = training_r2/(epochs*len(train_loader))\n",
    "    validation_r2 = validation_r2/(epochs*len(validation_loader))\n",
    "\n",
    "    print(train_rmse_loss,'\\t',val_rmse_loss,'\\n',training_r2,'\\t',validation_r2)\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == 'main':\n",
    "\"\"\"\n",
    "def main():\n",
    "    config = get_nn_config()\n",
    "    train(train_loader,validation_loader,config)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new folder named neural_networks. Inside it, create a folder called regression.\n",
    "\n",
    "Adapt your function called save_model so that it detects whether the model is a PyTorch module, and if so, saves the torch model in a file called model.pt, its hyperparameters in a file called hyperparameters.json, and its performance metrics in a file called metrics.json.\n",
    "\n",
    "Your metrics should include:\n",
    "\n",
    "The RMSE loss of your model under a key called RMSE_loss for training, validation, and test sets\n",
    "The R^2 score of your model under a key called R_squared for training, validation, and test sets\n",
    "The time taken to train the model under a key called training_duration\n",
    "The average time taken to make a prediction under a key called inference_latency\n",
    "Every time you train a model, create a new folder whose name is the current date and time.\n",
    "\n",
    "So, for example, a model trained on the 1st of January at 08:00:00 would be saved in a folder called models/neural_networks/regression/2018-01-01_08:00:00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tabular_data import load_airbnb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import yaml\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Create the folders as per your instructions\n",
    "save_path = os.path.join('models', 'neural_networks', 'regression', datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Define a function to save the model, hyperparameters, and metrics\n",
    "def save_model(model, hyperparameters, metrics):\n",
    "    # Save the PyTorch model\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, 'model.pt'))\n",
    "    \n",
    "    # Save the hyperparameters as JSON\n",
    "    with open(os.path.join(save_path, 'hyperparameters.json'), 'w') as json_file:\n",
    "        json.dump(hyperparameters, json_file, indent=4)\n",
    "    \n",
    "    # Save the metrics as JSON\n",
    "    with open(os.path.join(save_path, 'metrics.json'), 'w') as json_file:\n",
    "        json.dump(metrics, json_file, indent=4)\n",
    "\n",
    "class AirbnbNightlyPriceRegressionDataset(Dataset):\n",
    "    def __init__(self, df=pd.read_csv('airbnb-property-listing/tabular_data/clean_listing.csv', index_col='Unnamed: 0')):\n",
    "        super().__init__()\n",
    "        self.X, self.y = load_airbnb(df, label=\"Price_Night\")\n",
    "        print(self.X.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.X.iloc[index]), torch.tensor(self.y.iloc[index])\n",
    "\n",
    "def split_data(dataset):\n",
    "    train_dataset, test_dataset = random_split(dataset, [int(len(dataset) * 0.875), len(dataset) - int(len(dataset) * 0.875)])\n",
    "    train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * 0.85), len(train_dataset) - int(len(train_dataset) * 0.85)])\n",
    "\n",
    "    print(f\"\\tTraining: {len(train_dataset)}\")\n",
    "    print(f\"\\tValidation: {len(validation_dataset)}\")\n",
    "    print(f\"\\tTesting: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n",
    "\n",
    "dataset = AirbnbNightlyPriceRegressionDataset()\n",
    "batch_size = 16\n",
    "train_dataset, validation_dataset, test_dataset = split_data(dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_width = config['hidden_layer_width']\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            nn.Linear(11, self.hidden_layer_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_layer_width, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "def get_nn_config(config_file='nn_config.yaml'):\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "def train(train_loader, validation_loader, config, epochs=10):\n",
    "    model = NN(config)\n",
    "    optimiser_name = config['optimiser']\n",
    "    optimiser_class = getattr(optim, optimiser_name)\n",
    "    optimiser = optimiser_class(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Initialize metrics dictionaries\n",
    "    training_metrics = {\n",
    "        'RMSE_loss': [],\n",
    "        'R_squared': []\n",
    "    }\n",
    "    validation_metrics = {\n",
    "        'RMSE_loss': [],\n",
    "        'R_squared': []\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            X_train, y_train = batch\n",
    "            X_train = X_train.type(torch.float32)\n",
    "            y_train = y_train.type(torch.float32)\n",
    "            y_train = y_train.view(-1, 1)\n",
    "            \n",
    "            train_prediction = model(X_train)\n",
    "            train_loss = F.mse_loss(train_prediction, y_train)\n",
    "            train_loss = train_loss.type(torch.float32)\n",
    "            train_loss.backward()\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "        train_rmse_loss = torch.sqrt(train_loss)\n",
    "        training_r2 = r2_score(y_train.detach().numpy(), train_prediction.detach().numpy())\n",
    "\n",
    "        # Store training metrics\n",
    "        training_metrics['RMSE_loss'].append(train_rmse_loss.item())\n",
    "        training_metrics['R_squared'].append(training_r2)\n",
    "  \n",
    "        for batch in validation_loader:\n",
    "            X_val, y_val = batch\n",
    "            X_val = X_val.type(torch.float32)\n",
    "            y_val = y_val.type(torch.float32)\n",
    "            y_val = y_val.view(-1, 1)\n",
    "            \n",
    "            val_prediction = model(X_val)\n",
    "            val_loss = F.mse_loss(val_prediction, y_val)\n",
    "            val_loss = val_loss.type(torch.float32)\n",
    "\n",
    "        val_rmse_loss = torch.sqrt(val_loss)\n",
    "        validation_r2 = r2_score(y_val.detach().numpy(), val_prediction.detach().numpy())\n",
    "\n",
    "        # Store validation metrics\n",
    "        validation_metrics['RMSE_loss'].append(val_rmse_loss.item())\n",
    "        validation_metrics['R_squared'].append(validation_r2)\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_training_rmse = sum(training_metrics['RMSE_loss']) / len(training_metrics['RMSE_loss'])\n",
    "    avg_training_r2 = sum(training_metrics['R_squared']) / len(training_metrics['R_squared'])\n",
    "    avg_validation_rmse = sum(validation_metrics['RMSE_loss']) / len(validation_metrics['RMSE_loss'])\n",
    "    avg_validation_r2 = sum(validation_metrics['R_squared']) / len(validation_metrics['R_squared'])\n",
    "\n",
    "    # Create a metrics dictionary\n",
    "    metrics = {\n",
    "        'RMSE_loss': {\n",
    "            'training': avg_training_rmse,\n",
    "            'validation': avg_validation_rmse,\n",
    "            'test': None\n",
    "        },\n",
    "        'R_squared': {\n",
    "            'training': avg_training_r2,\n",
    "            'validation': avg_validation_r2,\n",
    "            'test': None\n",
    "        },\n",
    "        'training_duration': training_duration,\n",
    "        'inference_latency': None\n",
    "    }\n",
    "\n",
    "    # Save the model, hyperparameters, and metrics\n",
    "    save_model(model, config, metrics)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config = get_nn_config()\n",
    "    train(train_loader, validation_loader, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tabular_data import load_airbnb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import yaml\n",
    "\n",
    "\n",
    "class AirbnbNightlyPriceRegressionDataset(Dataset):\n",
    "    def __init__(self,df = pd.read_csv('airbnb-property-listing/tabular_data/clean_listing.csv',index_col='Unnamed: 0')) -> None:\n",
    "        super().__init__()\n",
    "        self.X,self.y = load_airbnb(df,label=\"Price_Night\")\n",
    "        print(self.X.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.X.iloc[index]), torch.tensor(self.y.iloc[index])\n",
    "    \n",
    "\n",
    "def split_data(dataset):\n",
    "    train_dataset, test_dataset = random_split(dataset, [int(len(dataset)* 0.875), len(dataset)-int(len(dataset)*0.875)])\n",
    "\n",
    "    train_dataset,validation_dataset = random_split(train_dataset, [int(len(train_dataset)*0.85), len(train_dataset)-int(len(train_dataset)*0.85)])\n",
    "\n",
    "    print(f\"\\tTraining: {len(train_dataset)}\")\n",
    "    print(f\"\\tValidation: {len(validation_dataset)}\")\n",
    "    print(f\"\\tTesting: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n",
    "\n",
    "dataset = AirbnbNightlyPriceRegressionDataset()\n",
    "batch_size = 16\n",
    "train_dataset, validation_dataset, test_dataset = split_data(dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size = batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        # Define layers and architecture here\n",
    "        self.hidden_layer_width = config['hidden_layer_width']\n",
    "        self.layers = torch.nn.Sequential(\n",
    "        nn.Linear(11,self.hidden_layer_width),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.hidden_layer_width,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "    \n",
    "\n",
    "def get_nn_config(config_file='nn_config.yaml'):\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "def train(train_loader,validation_loader,config,epochs=10):\n",
    "\n",
    "    #scaler = MinMaxScaler()\n",
    "    model = NN(config)\n",
    "    optimiser_name = config['optimiser']\n",
    "    optimiser_class = getattr(torch.optim, optimiser_name)\n",
    "    optimiser = optimiser_class(model.parameters(), lr=config['learning_rate'])\n",
    "    writer = SummaryWriter()\n",
    "    batch_idx = 0\n",
    "    train_rmse_loss = 0\n",
    "    val_rmse_loss = 0\n",
    "    training_r2 = 0\n",
    "    validation_r2 = 0\n",
    "\n",
    "    for epoch in range (epochs):\n",
    "        for batch in train_loader:\n",
    "            X_train,y_train = batch\n",
    "            X_train = X_train.type(torch.float32)\n",
    "            #X_train = torch.tensor(scaler.fit_transform(X_train))\n",
    "            y_train = y_train.type(torch.float32)\n",
    "            y_train = y_train.view(-1,1)\n",
    "            train_prediction = model(X_train)\n",
    "            train_loss = F.mse_loss(train_prediction,y_train)\n",
    "            train_loss = train_loss.type(torch.float32)\n",
    "            train_loss.backward()\n",
    "            print(train_loss.item())\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "            writer.add_scalar('training_loss',train_loss.item(),batch_idx)\n",
    "            batch_idx += 1\n",
    "            rmse_loss = torch.sqrt(train_loss)\n",
    "            train_rmse_loss += rmse_loss.item()\n",
    "            train_r2 = r2_score(y_train.detach().numpy(), train_prediction.detach().numpy())\n",
    "            writer.add_scalar('training_r2', train_r2, batch_idx)\n",
    "            training_r2 += train_r2\n",
    "  \n",
    "\n",
    "        for batch in validation_loader:\n",
    "            X_val,y_val = batch\n",
    "            X_val = X_val.type(torch.float32)\n",
    "            #X_val = torch.tensor(scaler.fit_transform(X_val))\n",
    "            y_val = y_val.type(torch.float32)\n",
    "            y_val = y_val.view(-1, 1)\n",
    "            val_prediction = model(X_val)\n",
    "            val_loss = F.mse_loss(val_prediction,y_val)\n",
    "            val_loss = val_loss.type(torch.float32)\n",
    "            writer.add_scalar('validation_loss',val_loss.item(),batch_idx)\n",
    "            rmse_loss = torch.sqrt(val_loss)\n",
    "            val_rmse_loss += rmse_loss.item()\n",
    "\n",
    "            val_r2 = r2_score(y_val.detach().numpy(), val_prediction.detach().numpy())\n",
    "            writer.add_scalar('validation_r2', val_r2, batch_idx)\n",
    "            validation_r2 += val_r2\n",
    "\n",
    "    train_rmse_loss = train_rmse_loss/(epochs*len(train_loader))\n",
    "    val_rmse_loss = val_rmse_loss/(epochs*len(validation_loader))\n",
    "    training_r2 = training_r2/(epochs*len(train_loader))\n",
    "    validation_r2 = validation_r2/(epochs*len(validation_loader))\n",
    "\n",
    "    print(train_rmse_loss,'\\t',val_rmse_loss,'\\n',training_r2,'\\t',validation_r2)\n",
    "\n",
    "\n",
    "def test_model(test_loader,config):\n",
    "    \n",
    "    model=NN(config)\n",
    "    writer = SummaryWriter()\n",
    "    batch_idx = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        X_test,y_test = batch\n",
    "        X_test = X_test.type(torch.float32)\n",
    "        y_test = y_test.type(torch.float32)\n",
    "        y_test = y_test.view(-1,1)\n",
    "\n",
    "        test_prediction = model(X_test)\n",
    "\n",
    "        test_loss = F.mse_loss(test_prediction,y_test)\n",
    "        test_loss = test_loss.type(torch.float32)\n",
    "        writer.add_scalar(\"test_loss\",test_loss,batch_idx)\n",
    "        batch_idx += 1 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_model():\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == 'main':\n",
    "\"\"\"\n",
    "def main():\n",
    "    config = get_nn_config()\n",
    "    train(train_loader,validation_loader,config)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "config = {\n",
    "    'optimiser': ['SGD','Adam','ADAGRAD'],\n",
    "    'learning_rate': [0.01,0.02],\n",
    "    'hidden_layer_width': [12,14,16]}\n",
    "configs=[]\n",
    "\n",
    "for hyperparam_values in itertools.product(*config.values()):\n",
    "    hyperparam_dict = dict(zip(config.keys(), hyperparam_values))\n",
    "    configs.append(hyperparam_dict)\n",
    "print(len(configs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tabular_data import load_airbnb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import yaml\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "class AirbnbNightlyPriceRegressionDataset(Dataset):\n",
    "    def __init__(self,df = pd.read_csv('airbnb-property-listing/tabular_data/clean_listing.csv',index_col='Unnamed: 0')) -> None:\n",
    "        super().__init__()\n",
    "        self.X,self.y = load_airbnb(df,label=\"Price_Night\")\n",
    "        print(self.X.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.X.iloc[index]), torch.tensor(self.y.iloc[index])\n",
    "    \n",
    "\n",
    "def split_data(dataset):\n",
    "    train_dataset, test_dataset = random_split(dataset, [int(len(dataset)* 0.875), len(dataset)-int(len(dataset)*0.875)])\n",
    "\n",
    "    train_dataset,validation_dataset = random_split(train_dataset, [int(len(train_dataset)*0.85), len(train_dataset)-int(len(train_dataset)*0.85)])\n",
    "\n",
    "    print(f\"\\tTraining: {len(train_dataset)}\")\n",
    "    print(f\"\\tValidation: {len(validation_dataset)}\")\n",
    "    print(f\"\\tTesting: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n",
    "\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        # Define layers and architecture here\n",
    "        self.hidden_layer_width = config['hidden_layer_width']\n",
    "        self.layers = torch.nn.Sequential(\n",
    "        nn.Linear(11,self.hidden_layer_width),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.hidden_layer_width,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "    \n",
    "\n",
    "def get_nn_config(config_file='nn_config.yaml'):\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "def generate_nn_config():\n",
    "\n",
    "    configs = []\n",
    "    config = {\n",
    "        'optimiser': ['SGD','Adam','Adagrad'],\n",
    "        'learning_rate': [0.01,0.02],\n",
    "        'hidden_layer_width': [12,14,16]}\n",
    "    \n",
    "    for hyperparam_values in itertools.product(*config.values()):\n",
    "        hyperparam_dict = dict(zip(config.keys(), hyperparam_values))\n",
    "        configs.append(hyperparam_dict)\n",
    "\n",
    "    return configs    \n",
    "\n",
    "\n",
    "def train(train_loader,validation_loader,config,epochs=10):\n",
    "\n",
    "    #scaler = MinMaxScaler()\n",
    "    model = NN(config)\n",
    "    optimiser_name = config['optimiser']\n",
    "    optimiser_class = getattr(torch.optim, optimiser_name)\n",
    "    optimiser = optimiser_class(model.parameters(), lr=config['learning_rate'])\n",
    "    writer = SummaryWriter()\n",
    "    batch_idx = 0\n",
    "    train_rmse_loss = 0\n",
    "    val_rmse_loss = 0\n",
    "    training_r2 = 0\n",
    "    validation_r2 = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range (epochs):\n",
    "        for batch in train_loader:\n",
    "            X_train,y_train = batch\n",
    "            X_train = X_train.type(torch.float32)\n",
    "            #X_train = torch.tensor(scaler.fit_transform(X_train))\n",
    "            y_train = y_train.type(torch.float32)\n",
    "            y_train = y_train.view(-1,1)\n",
    "            train_prediction = model(X_train)\n",
    "            train_loss = F.mse_loss(train_prediction,y_train)\n",
    "            train_loss = train_loss.type(torch.float32)\n",
    "            train_loss.backward()\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "            writer.add_scalar('training_loss',train_loss.item(),batch_idx)\n",
    "            batch_idx += 1\n",
    "            rmse_loss = torch.sqrt(train_loss)\n",
    "            train_rmse_loss += rmse_loss.item()\n",
    "            train_r2 = r2_score(y_train.detach().numpy(), train_prediction.detach().numpy())\n",
    "            writer.add_scalar('training_r2', train_r2, batch_idx)\n",
    "            training_r2 += train_r2\n",
    "  \n",
    "\n",
    "        for batch in validation_loader:\n",
    "            X_val,y_val = batch\n",
    "            X_val = X_val.type(torch.float32)\n",
    "            #X_val = torch.tensor(scaler.fit_transform(X_val))\n",
    "            y_val = y_val.type(torch.float32)\n",
    "            y_val = y_val.view(-1, 1)\n",
    "            val_prediction = model(X_val)\n",
    "            val_loss = F.mse_loss(val_prediction,y_val)\n",
    "            val_loss = val_loss.type(torch.float32)\n",
    "            writer.add_scalar('validation_loss',val_loss.item(),batch_idx)\n",
    "            rmse_loss = torch.sqrt(val_loss)\n",
    "            val_rmse_loss += rmse_loss.item()\n",
    "\n",
    "            val_r2 = r2_score(y_val.detach().numpy(), val_prediction.detach().numpy())\n",
    "            writer.add_scalar('validation_r2', val_r2, batch_idx)\n",
    "            validation_r2 += val_r2\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time\n",
    "\n",
    "    train_rmse_loss = train_rmse_loss/(epochs*len(train_loader))\n",
    "    val_rmse_loss = val_rmse_loss/(epochs*len(validation_loader))\n",
    "    training_r2 = training_r2/(epochs*len(train_loader))\n",
    "    validation_r2 = validation_r2/(epochs*len(validation_loader))\n",
    "\n",
    "\n",
    "    return model,train_rmse_loss,val_rmse_loss,training_r2,validation_r2,training_duration\n",
    "\n",
    "\n",
    "def test_model(model,test_loader):\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    batch_idx = 0\n",
    "    test_rmse_loss = 0\n",
    "    testing_r2 = 0\n",
    "    total_pred_time = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        X_test,y_test = batch\n",
    "        X_test = X_test.type(torch.float32)\n",
    "        y_test = y_test.type(torch.float32)\n",
    "        y_test = y_test.view(-1,1)\n",
    "        pred_start_time = time.time()\n",
    "        test_prediction = model(X_test)\n",
    "        pred_end_time = time.time()\n",
    "        test_loss = F.mse_loss(test_prediction,y_test)\n",
    "        test_loss = test_loss.type(torch.float32)\n",
    "        writer.add_scalar(\"test_loss\",test_loss,batch_idx)\n",
    "        rmse_loss = torch.sqrt(test_loss)\n",
    "        test_rmse_loss += rmse_loss.item()\n",
    "\n",
    "        test_r2 = r2_score(y_test.detach().numpy(),test_prediction.detach().numpy())\n",
    "        writer.add_scalar(\"test_r2\",test_r2,batch_idx)\n",
    "        testing_r2 += test_r2\n",
    "        \n",
    "        total_pred_time += (pred_end_time - pred_start_time)\n",
    "        batch_idx += 1 \n",
    "\n",
    "    test_rmse_loss = test_rmse_loss/len(test_loader)\n",
    "    testing_r2 = testing_r2/len(test_loader)\n",
    "    inference_latency = total_pred_time/len(test_loader)\n",
    "\n",
    "    return test_rmse_loss,testing_r2,inference_latency\n",
    "\n",
    "\n",
    "def save_model(model,hyperparameters,performance_metrics,file_path=\"models\\\\neural_networks\\\\regression\"):\n",
    "    \"\"\"Saves the trained neural network to a file path with name as hyperparameter values and performance metrics.\"\"\"\n",
    "\n",
    "    save_path = os.path.join(file_path,datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\"))\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_path,'model.pt'))\n",
    "\n",
    "    with open(os.path.join(save_path,'hyperparameters.json'),'w') as f:\n",
    "        json.dump(hyperparameters,f,indent=4)\n",
    "\n",
    "    with open(os.path.join(save_path,'performance_metrics.json'),'w') as f:\n",
    "        json.dump(performance_metrics,f,indent=4)\n",
    "\n",
    "\n",
    "def find_best_nn():\n",
    "\n",
    "    dataset = AirbnbNightlyPriceRegressionDataset()\n",
    "    batch_size = 16\n",
    "    train_dataset, validation_dataset, test_dataset = split_data(dataset)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size = batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True)\n",
    "    \n",
    "    best_model = None\n",
    "    best_hyperparameters = None\n",
    "    best_rmse = float('inf')\n",
    "\n",
    "    config = generate_nn_config()\n",
    "\n",
    "    for index,element in enumerate(config):\n",
    "        print(f\"Training model {index + 1}/{len(config)} with config: {element}\")\n",
    "        model,train_rmse_loss,val_rmse_loss,training_r2,validation_r2,training_duration = train(train_loader,validation_loader,element)\n",
    "\n",
    "        if val_rmse_loss < best_rmse:\n",
    "            best_rmse = val_rmse_loss\n",
    "            best_model = model\n",
    "            best_hyperparameters = element\n",
    "            test_rmse_loss,testing_r2,inference_latency = test_model(model,test_loader)\n",
    "            performance_metrics = {\n",
    "            \"RMSE_loss\": {\n",
    "            \"training\": train_rmse_loss,\n",
    "            \"validation\": val_rmse_loss,\n",
    "            \"test\": test_rmse_loss\n",
    "            },\n",
    "            \"R_squared\": {\n",
    "            \"training\": training_r2,\n",
    "            \"validation\": validation_r2,\n",
    "            \"test\": testing_r2\n",
    "            },\n",
    "            \"training_duration\": training_duration,\n",
    "            \"inference_latency\": inference_latency\n",
    "            }\n",
    "\n",
    "    save_model(best_model,best_hyperparameters,performance_metrics)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_best_nn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling_Airbnbs_property_listing_dataset\n",
    "\n",
    "Welcome to the **Airbnb Model Evaluation Framework**! This project aims to create a versatile framework for systematically training, tuning, and evaluating machine learning models for a wide range of tasks, inspired by the challenges faced by the Airbnb team. Whether you're working with tabular, image, or text data, this framework will help you build effective models and streamline the evaluation process.\n",
    "\n",
    "## Built With\n",
    "\n",
    "This project leverages several essential frameworks and tools to achieve its goals:\n",
    "\n",
    "- [GitHub](https://github.com/) - Hosting and version control for collaborative development.\n",
    "- [Jupyter Notebook](https://jupyterlab.readthedocs.io/en) - An open source web application that allows you to create and share.\n",
    "- [Python 3.7](https://www.python.org/downloads) - The programming language used in this project.\n",
    "- [Pandas](https://pandas.pydata.org/) - Data manipulation and analysis in Python.\n",
    "- [Scikit-Learn](https://scikit-learn.org/) - Machine learning library in Python.\n",
    "- [NumPy](https://numpy.org/) - Fundamental package for scientific computing with Python.\n",
    "- [Joblib](https://joblib.readthedocs.io/) - Efficiently save and load Python objects.\n",
    "- [JSON](https://www.json.org/) - For storing hyperparameters and performance metrics.\n",
    "- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) - Grid search for hyperparameter tuning.\n",
    "- [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) - Feature scaling.\n",
    "- [Matplotlib](https://matplotlib.org/stable/index.html) - Plotting graphs, charts etc., in Python.\n",
    "- [TensorBoard](https://www.tensorflow.org/tensorboard) - Tool for visualizing training and performance metrics of neural networks.\n",
    "\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To get started with the Airbnb Model Evaluation Framework, follow these steps:\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before you begin, ensure you have the following installed:\n",
    "\n",
    "- Python (>=3.6)\n",
    "- Jupyter Notebook (optional)\n",
    "\n",
    "### Installation\n",
    "\n",
    "1. Clone the repository:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/anany14/Modelling_Airbnbs_property_listing_dataset.git\n",
    "cd Modelling_Airbnbs_property_listing_dataset\n",
    "```\n",
    "\n",
    "2. Install the required packages:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Usage\n",
    "\n",
    "Use the Airbnb Model Evaluation Framework to build and evaluate machine learning models. The provided python scripts demonstrate how to use the framework for regression and classification tasks. Feel free to modify the scripts or create your own to adapt the framework to your specific dataset and task.\n",
    "\n",
    "1. Run the `tabular_data.py` script to download the dataset as a [PANDAS] DataFrame and then clean the data, remove unnecessary data, convert datatypes and get all the columns in the right format with the correct values.\n",
    "\n",
    "2. Run the `modelling.py` script to perform regression modeling, hyperparameter tuning, and evaluation of the models to predict the price per night of the Airbnb property listings using various models such as [SGDRegressor],[DecisionTreeRegressor],[RandomForestRegressor] and [GradientBoostingRegressor].\n",
    "\n",
    "3. Run the `classification.py` script to train, tune, and evaluate classification models on the Airbnb dataset for predicting property categories using models like [LogisticRegression], [DecisionTreeClassifier], [RandomForestClassifier], and [GradientBoostingClassifier].\n",
    "\n",
    "4. Run the `neural_network.py` script to build and train a neural network model for property price prediction. Details of the architecture and training process are provided below in the `neural_network` section\n",
    "\n",
    "\n",
    "5. \n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project aims to build a systematic framework for training, tuning, and evaluating machine learning models on various tasks, inspired by the challenges tackled by the Airbnb team. The framework will be designed to handle different types of data, including tabular, image, and text data. The goal is to create a flexible and reusable system that can be applied to any dataset.\n",
    "\n",
    "### Milestones\n",
    "1. Set up the environment: Prepare the development environment to start building the framework.\n",
    "\n",
    "2. Data preparation: Understand the structure of the Airbnb dataset and perform data cleaning and preprocessing.\n",
    "\n",
    "3. Create a regression model: Build machine learning models that predict the price of the Airbnb listing per night and evaluate their performance.\n",
    "\n",
    "4. Create a classification model: Develop classification models for specific tasks and assess their effectiveness.\n",
    "\n",
    "5. Create a configurable neural network: Utilize a neural network to predict the nightly listing price using the numerical data from the tabular dataset.\n",
    "\n",
    "6. Reuse the framework for another use-case: Test the flexibility of the framework by applying it to a different dataset, ensuring that it can handle various data types.\n",
    "\n",
    "## Data Cleaning (tabular_data.py)\n",
    "\n",
    "The `tabular_data.py` module contains functions for cleaning and preprocessing the Airbnb property listing dataset. The key steps include:\n",
    "\n",
    "1. **Removing Rows with Missing Ratings:** The `remove_rows_with_missing_columns` function removes rows with missing values in specific rating columns (e.g., cleanliness, accuracy, communication, location, check-in, and value ratings).\n",
    "\n",
    "2. **Fixing Problematic Rows:** The `fix_problematic_rows` function manually fixes specific rows with shifting issues in column values, ensuring proper alignment.\n",
    "\n",
    "3. **Combining Description and Amenities:** The `combine_description_settings` function processes the description and amenities columns by cleaning and combining list items into a single string.\n",
    "\n",
    "4. **Setting Default Feature Values:** The `set_default_feature_values` function fills empty entries in the guests, beds, bathrooms, and bedrooms columns with default values.\n",
    "\n",
    "5. **Converting Data Types:** The `convert_dtypes_and_optimise_df` function converts specific columns to appropriate data types, optimizes the DataFrame, and drops unnecessary columns.\n",
    "\n",
    "6. **Cleaning Tabular Data:** The `clean_tabular_data` function applies a series of data cleaning steps, combining the above functions to obtain a cleaned DataFrame.\n",
    "\n",
    "7. **load_airbnb:** The `load_airbnb` function Extract features and labels from the DataFrame. \n",
    "\n",
    "## Regression Modeling (modelling.py)\n",
    "\n",
    "The `modelling.py` module focuses on building and evaluating regression models to predict the price per night of the Airbnb property listings. The key steps include:\n",
    "\n",
    "1. **Data Splitting:** The `split_X_y` function splits the dataset into training, testing, and validation sets, ensuring that the data is ready for model training and evaluation.\n",
    "\n",
    "2. **Initial Model Training:** The `train_regression_model` function trains a regression model (default is Stochastic Gradient Descent - SGDRegressor) and prints its initial performance on the training and test sets.\n",
    "\n",
    "3. **Custom Hyperparameter Tuning:** The `custom_tune_regression_model_hyperparameters` function performs a grid search over a range of hyperparameter values for a given regression model. It returns the best model, best hyperparameters, and performance metrics (validation RMSE, test RMSE).\n",
    "\n",
    "4. **Hyperparameter Tuning with GridSearchCV:** The `tune_regression_model_hyperparameters` function uses GridSearchCV for hyperparameter tuning, allowing us to explore a wider range of hyperparameter values for different regression models.\n",
    "\n",
    "5. **Save Model:** The `save_model` function saves a regression model, its hyperparameters and performance metrics in the folder of choice.\n",
    "\n",
    "6. **Model Evaluation and Selection:** The `evaluate_all_models` function evaluates multiple regression models (Linear Regression, Decision Tree, Random Forest, Gradient Boosting) by tuning their hyperparameters. The best model is selected based on validation RMSE, and the trained models are saved along with their performance metrics.\n",
    "\n",
    "7. **Finding the Best Model:** The `find_best_model` function identifies the best-performing model based on the saved validation RMSE values from the earlier model evaluations.\n",
    "\n",
    "8. **Plotting Data:** The `plot_all_models` function plots either a comparison of different models using scatterplots or the total performance if a single model is provided.\n",
    "\n",
    "9. **Running all the functions:** : the `main` function runs the functions in the correct order.\n",
    "\n",
    "\n",
    "#### Model Selection and Metrics\n",
    "\n",
    "In the file `modelling.py`, we evaluated several regression models:\n",
    "\n",
    "1. **Linear Regression (SGDRegressor)**: We used Stochastic Gradient Descent as a baseline regression model.\n",
    "\n",
    "2. **Decision Tree Regressor**: A decision tree-based regression model that can capture non-linear relationships.\n",
    "\n",
    "3. **Random Forest Regressor**: An ensemble model combining multiple decision trees for improved predictive performance.\n",
    "\n",
    "4. **Gradient Boosting Regressor**: A boosting algorithm that combines weak learners into a strong predictive model.\n",
    "\n",
    "### Model Performance\n",
    "\n",
    "The best model based on the validation RMSE is the chosen model for making predictions on new data. The key metrics used for evaluation are:\n",
    "\n",
    "1. **Root Mean Squared Error (RMSE)**: A measure of the average deviation between the predicted and actual values. Lower RMSE indicates better model performance.\n",
    "\n",
    "2. **R-squared (R2) Score**: A measure of how well the model explains the variance in the target variable. Higher R2 score indicates a better fit to the data.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"plots/regression/sgdregression_without_hyperparameters.png\" alt=\"This is the Model performance of the LinearRegression class without tuning the hyperparameters\">\n",
    "  <figcaption>Figure 1: This is the Model performance of the LinearRegression class without tuning the hyperparameters</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Model Performance Metrics \n",
    "\n",
    "- **Linear Regression (SGDRegressor)**:\n",
    "    - best hyperparameters: {\"alpha\": 0.1,\"early_stopping\": false,\"loss\": \"squared_epsilon_insensitive\",\"max_iter\": 1000,\"penalty\": \"l1\",\"tol\": 0.01}\n",
    "    - \"gridsearch_rmse\": 97.5211322126212,\n",
    "    - \"validation_rmse\": 133.73609364043634,\n",
    "    - \"validation_r2\": -0.36297036119731807\n",
    "\n",
    "- **Decision Tree Regressor**:\n",
    "    - best hyperparameters: {\"criterion\": \"poisson\", \"max_depth\": 2, \"min_samples_leaf\": 5, \"min_samples_split\": 2, \"splitter\": \"best\"}\n",
    "    - \"gridsearch_rmse\": 97.96613641100369,\n",
    "    - \"validation_rmse\": 109.43713370660083,\n",
    "    - \"validation_r2\": 0.0873199354688664\n",
    "\n",
    "- **Random Forest Regressor**:\n",
    "    - best hyperparameters: {\"criterion\": \"absolute_error\",\"max_depth\": null,\"min_samples_leaf\": 5,\"min_samples_split\": 2,\"n_estimators\": 200}\n",
    "    - \"gridsearch_rmse\": 92.62686487608235,\n",
    "    - \"validation_rmse\": 101.86512427627144,\n",
    "    - \"validation_r2\": 0.2092482102837555\n",
    "\n",
    "- **Gradient Boosting Regressor**:\n",
    "    - best hyperparameters: {\"alpha\": 0.9,\"learning_rate\": 0.01,\"loss\": \"huber\",\"max_depth\": 3,\"n_estimators\": 300}\n",
    "    - \"gridsearch_rmse\": 95.05038890665303,\n",
    "    - \"validation_rmse\": 102.33429938368708,\n",
    "    - \"validation_r2\": 0.20194727298172055\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"plots/regression/model_comparison.png\" alt=\"This comparison Plot is between 4 regression models and is made to choose the best model\">\n",
    "  <figcaption>Figure 2: This comparison Plot is between 4 regression models and is made to choose the best model</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Best Model \n",
    "\n",
    "- The best model and it's best hyperparameters to fit the data is `RandomForestRegressor(criterion='absolute_error', min_samples_leaf=5,n_estimators=200, random_state=42)` \n",
    "- The RMSE on the testing dataset is `131.073797`\n",
    "\n",
    "<figure>\n",
    "    <img src = \"plots/regression/best_model.png\" alt=\" Best Model Performance\">\n",
    "    <figcaption>Figure 3: Best Model Performance</figcaption>\n",
    "<figure>\n",
    "\n",
    "\n",
    "### Further Experiments\n",
    "\n",
    "While we have explored a variety of regression models and performed hyperparameter tuning, there are several additional experiments we could consider:\n",
    "\n",
    "1. **Feature Engineering**: We can experiment with creating new features based on domain knowledge or feature interactions to potentially improve model performance.\n",
    "\n",
    "2. **Advanced Ensemble Models**: We can explore more advanced ensemble methods such as XGBoost and LightGBM to see if they provide further improvements.\n",
    "\n",
    "3. **Cross-Validation Strategies**: We used a simple train-validation-test split, but we can experiment with more advanced cross-validation strategies to robustly evaluate model performance.\n",
    "\n",
    "4. **Fine-Tuning Hyperparameters**: We can perform more exhaustive grid searches or use Bayesian optimization to fine-tune hyperparameters and achieve even better model performance.\n",
    "\n",
    "5. **Handling Outliers**: Exploring techniques to handle outliers in the data may improve the robustness of our models.\n",
    "\n",
    "## Classification Modelling \n",
    "\n",
    "The file `classification.py` is dedicated to constructing and assessing classification models for predicting specific outcomes in the Airbnb property listings dataset.\n",
    "\n",
    "1. **Data Preparation and Splitting:** The `_split_X_y` function readies the dataset for model training, validation, and testing by approprimately splitting it.\n",
    "\n",
    "2. **Initial Model Training:** The `train_logistic_model` function trains a baseline classification model (default is Logistic Regression) and provides an overview of its performance on the training and test sets.\n",
    "\n",
    "3. **Hyperparameter Tuning with GridSearchCV:** The `tune_classification_model_hyperparameters` uses GridSearchCV to fine-tune hyperparameters, allowing for an exhaustive exploration of parameter space for various classification models.\n",
    "\n",
    "4. **Save Model:** The `save_model` function saves a regression model, its hyperparameters and performance metrics in the folder of choice.\n",
    "\n",
    "5. **Model Evaluation and Selection**: The `evaluate_all_models` function assesses multiple classification models (Logistic Regression, Decision Tree, Random Forest, Gradient Boosting) by optimizing their hyperparameters. The selection of the best model is guided by validation accuracy, and the trained models, along with their performance metrics, are saved.\n",
    "\n",
    "6. **Identifying the Optimal Model:** The `find_best_model` function determines the highest-performing model based on validation accuracy scores from prior evaluations.\n",
    "\n",
    "7. **Visualizing Results:** The `plot_models` function generates visualization plots, depicting either a comparative analysis of different models  or the comprehensive performance of a single model.\n",
    "\n",
    "8. **Running all the functions:** : the `main` function runs the functions in the correct order.\n",
    "\n",
    "#### Model Selection and Metrics\n",
    "\n",
    "Within the `classification.py` file, a range of classification models were evaluated:\n",
    "\n",
    "**Logistic Regression:** A fundamental classification algorithm suitable for binary classification tasks.\n",
    "\n",
    "**Decision Tree Classifier:** A model based on decision tree principles that can capture complex decision boundaries.\n",
    "\n",
    "**Random Forest Classifier:** An ensemble model combining multiple decision trees for enhanced predictive accuracy.\n",
    "\n",
    "**Gradient Boosting Classifier:** A boosting algorithm that amalgamates weak learners into a robust predictive model.\n",
    "\n",
    "### Model Performance Assessment\n",
    "\n",
    "The optimal model, chosen based on validation accuracy, serves as the primary predictor for new data. The primary evaluation metrics encompass:\n",
    "\n",
    "**Validation Accuracy:** Measures the ratio of correctly predicted instances in the validation set. Higher accuracy indicates better model performance.\n",
    "\n",
    "**Precision, Recall, F1-Score:** These metrics gauge the classifier's performance concerning true positive rate, positive predictive value, and balance between precision and recall.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"plots\\classification\\logisticclassification_confusion_wo_hyperparameters.png\" alt=\"This is the Confusion Matrix for the LogisticRegression class without tuning the hyperparameters\">\n",
    "  <figcaption>Figure 4: This is the Confusion Matrix for the LogisticRegression class without tuning the hyperparameters</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Model Performance Metrics\n",
    "\n",
    "- **Logistic Regression:**\n",
    "    - Best Hyperparameters: {\"max_iter\": 10000,\"penalty\": null,\"solver\": \"saga\"}\n",
    "    - \"training_accuracy\": 0.962050625430907,\n",
    "    - \"validation_accuracy\": 0.9662921348314607,\n",
    "    - \"validation_precision\": 0.9662921348314607,\n",
    "    - \"validaiton_recall\": 0.9662921348314607,\n",
    "    - \"validation_f1\": 0.9662921348314607\n",
    "\n",
    "- **GradientBoosting Classifier:**\n",
    "    - {\"criterion\": \"friedman_mse\",\"learning_rate\": 0.01,\"max_depth\": 3,\"n_estimators\": 50}\n",
    "    - \"training_accuracy\": 0.9915886929971437,\n",
    "    - \"validation_accuracy\": 0.9887640449438202,\n",
    "    - \"validation_precision\": 0.9887640449438202,\n",
    "    - \"validaiton_recall\": 0.9887640449438202,\n",
    "    - \"validation_f1\": 0.9887640449438202\n",
    "\n",
    "- **RandomForset Classifier:**\n",
    "    - {\"criterion\": \"entropy\",\"max_depth\": null,\"min_samples_leaf\": 1,\"min_samples_split\": 2,\"n_estimators\": 300}\n",
    "    - \"training_accuracy\": 0.9831773859942874,\n",
    "    - \"validation_accuracy\": 0.9887640449438202,\n",
    "    - \"validation_precision\": 0.9887640449438202,\n",
    "    - \"validaiton_recall\": 0.9887640449438202,\n",
    "    - \"validation_f1\": 0.9887640449438202\n",
    "\n",
    "- **DecisionTree Classifiers:**\n",
    "    - {\"criterion\": \"gini\",\"max_depth\": null,\"min_samples_leaf\": 1,\"min_samples_split\": 2,\"splitter\": \"best\"}\n",
    "    - \"training_accuracy\": 0.9915886929971437,\n",
    "    - \"validation_accuracy\": 0.9887640449438202,\n",
    "    - \"validation_precision\": 0.9887640449438202,\n",
    "    - \"validaiton_recall\": 0.9887640449438202,\n",
    "    - \"validation_f1\": 0.9887640449438202\n",
    "\n",
    "<figure>\n",
    "  <img src=\"plots\\classification\\classification_models_comparison.png\" alt=\"This comparison Plot is between 4 classification models and is made to choose the best model\">\n",
    "  <figcaption>Figure 5: This comparison Plot is between 4 classification models and is made to choose the best model</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Best Model \n",
    "\n",
    "- The best model and it's best hyperparameters to fit the data is `DecisionTree Classifier('criterion'= 'gini', 'max_depth'= None, 'min_samples_leaf'= 1, 'min_samples_split'=2, 'splitter'='best')`\n",
    "\n",
    "- The Accuracy on the testing dataset is `0.9935897435897436`\n",
    "\n",
    "<figure>\n",
    "    <img src = \"plots\\classification\\best_model_confusion_matrix.png\" alt=\" Best Model Confusion Matrix\">\n",
    "    <figcaption>Figure 6: Best Model Confusion Matrix</figcaption>\n",
    "<figure>\n",
    "\n",
    "\n",
    "### Further Experiments\n",
    "\n",
    "While the exploration of various classification models and hyperparameter tuning was conducted, further avenues for experimentation include:\n",
    "\n",
    "**Feature Engineering:** Experimentation with creating novel features can be pursued to potentially enhance model performance.\n",
    "\n",
    "**Advanced Ensemble Techniques:** Exploration of advanced ensemble models like XGBoost and LightGBM could lead to superior results.\n",
    "\n",
    "**Cross-Validation Strategies:** Adoption of sophisticated cross-validation methodologies may enhance the robustness of model evaluation.\n",
    "\n",
    "**Fine-Tuning Hyperparameters:** More exhaustive hyperparameter tuning via Bayesian optimization could yield optimal results.\n",
    "\n",
    "**Handling Imbalanced Data:** Techniques for addressing imbalanced classes can contribute to improved model performance.\n",
    "\n",
    "\n",
    "\n",
    "## Neural Network Modeling (neural_network.py)\n",
    "\n",
    "The `neural_network.py` module focuses on building and assessing a neural network-based model for predicting the price per night of Airbnb property listings. This section provides an overview of the neural network architecture, training process, and model performance evaluation.\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "The neural network architecture employed for this task is a feedforward neural network with the following layers:\n",
    "\n",
    "1. **Input Layer**: The input layer accepts features extracted from the dataset. The number of input neurons matches the dimensionality of the feature vector.\n",
    "\n",
    "2. **Hidden Layers**: This neural network contains two hidden layers. The number of neurons in these layers is adjustable, offering flexibility in model complexity.\n",
    "\n",
    "3. **Output Layer**: The output layer consists of a single neuron, which predicts the price per night.\n",
    "\n",
    "The activation functions used in the hidden layers are ReLU (Rectified Linear Unit), and a linear activation is used in the output layer, as this is a regression task.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "The neural network is trained using the Mean Squared Error (MSE) loss function and the Adam optimizer. The training dataset is split into training and validation sets to monitor training progress and prevent overfitting. The training process encompasses:\n",
    "\n",
    "1. **Forward Pass**: Input data is propagated forward through the network, resulting in predictions.\n",
    "\n",
    "2. **Loss Computation**: The MSE loss between the predicted prices and actual prices is calculated.\n",
    "\n",
    "3. **Backpropagation**: Gradients of the loss with respect to the network's weights and biases are computed.\n",
    "\n",
    "4. **Weight Updates**: The Adam optimizer is employed to update the model's weights and biases, minimizing the loss.\n",
    "\n",
    "5. **Training Metrics**: Training and validation metrics, such as Root Mean Squared Error (RMSE) and R-squared, are logged using TensorBoard for analysis and visualization.\n",
    "\n",
    "### TensorBoard Visualization\n",
    "\n",
    "Below, you'll find screenshots of training and performance metrics obtained using TensorBoard. These images present all the neural network models trained, facilitating comparison.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"path/to/your/image.png\" alt=\"Description of the image\">\n",
    "  <figcaption>Figure: Description of the image</figcaption>\n",
    "</figure>\n",
    "\n",
    "#### Training and Performance of the Best-Parameterized Neural Network\n",
    "\n",
    "In this section, we highlight the training and performance characteristics of the best-parameterized neural network.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"path/to/your/image.png\" alt=\"Description of the image\">\n",
    "  <figcaption>Figure: Description of the image</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Best Model Selection\n",
    "\n",
    "The best-performing neural network model is chosen based on evaluation metrics such as RMSE and R-squared. This model becomes the primary predictor for new data.\n",
    "\n",
    "### Further Experimentation\n",
    "\n",
    "While this neural network-based approach demonstrates its effectiveness, avenues for further experimentation include:\n",
    "\n",
    "- **Architecture Exploration**: Experimenting with different neural network architectures (e.g., more hidden layers, different activation functions) could potentially yield improved performance.\n",
    "\n",
    "- **Hyperparameter Tuning**: Fine-tuning hyperparameters, such as learning rate and the number of neurons in hidden layers, can enhance model generalization.\n",
    "\n",
    "- **Feature Engineering**: Exploring additional features or feature transformations may lead to better predictive power.\n",
    "\n",
    "- **Regularization Techniques**: Implementing dropout layers or L1/L2 regularization can help prevent overfitting.\n",
    "\n",
    "- **Data Augmentation**: For image or text data, data augmentation techniques can be applied to increase the size and diversity of the training dataset.\n",
    "\n",
    "- **Transfer Learning**: Leveraging pre-trained neural networks (e.g., using transfer learning with models like BERT or ResNet) may be beneficial for specific data types.\n",
    "\n",
    "Your experimentation can be documented, and results can be visualized using TensorBoard or other relevant tools.\n",
    "\n",
    "<!-- Include your TensorBoard screenshots and model performance details specific to your neural network models as needed. -->\n",
    "\n",
    "<!-- Additional sections of your README remain unchanged -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Contributing\n",
    "\n",
    "Contributions to this project are highly appreciated. If you have suggestions or improvements, please follow these steps:\n",
    "\n",
    "1. Fork the project.\n",
    "2. Create a new branch: `git checkout -b feature/awesome-feature`.\n",
    "3. Commit your changes: `git commit -m 'Add some awesome feature'`.\n",
    "4. Push the branch: `git push origin feature/awesome-feature`.\n",
    "5. Open a pull request with the tag \"enhancement.\"\n",
    "\n",
    "Your contributions will help make this framework even more useful to the community.\n",
    "\n",
    "## License\n",
    "\n",
    "Distributed under the MIT License. See [LICENSE.txt](LICENSE.txt) for more information.\n",
    "\n",
    "## Contact\n",
    "\n",
    "For any questions or inquiries, please feel free to reach out:\n",
    "\n",
    "Anany Tripathi - ananytripathi10@gmail.com\n",
    "\n",
    "Project Link: [https://github.com/anany14/Modelling_Airbnbs_property_listing_dataset]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID  Category  Title  Description  Amenities  Location  guests   beds  \\\n",
      "0    False     False  False        False      False     False   False  False   \n",
      "1    False     False  False        False      False     False   False  False   \n",
      "2    False     False  False        False      False     False   False  False   \n",
      "3    False     False  False        False      False     False   False  False   \n",
      "4    False     False  False        False      False     False   False  False   \n",
      "..     ...       ...    ...          ...        ...       ...     ...    ...   \n",
      "885  False     False  False        False      False     False   False  False   \n",
      "886  False     False  False        False      False     False   False  False   \n",
      "887  False     False  False        False      False     False   False  False   \n",
      "888  False     False  False        False      False     False   False  False   \n",
      "889  False     False  False        False      False     False   False  False   \n",
      "\n",
      "     bathrooms  Price_Night  Cleanliness_rating  Accuracy_rating  \\\n",
      "0        False        False               False            False   \n",
      "1        False        False               False            False   \n",
      "2        False        False               False            False   \n",
      "3        False        False               False            False   \n",
      "4        False        False               False            False   \n",
      "..         ...          ...                 ...              ...   \n",
      "885      False        False               False            False   \n",
      "886      False        False               False            False   \n",
      "887      False        False               False            False   \n",
      "888      False        False               False            False   \n",
      "889      False        False               False            False   \n",
      "\n",
      "     Communication_rating  Location_rating  Check-in_rating  Value_rating  \\\n",
      "0                   False            False            False         False   \n",
      "1                   False            False            False         False   \n",
      "2                   False            False            False         False   \n",
      "3                   False            False            False         False   \n",
      "4                   False            False            False         False   \n",
      "..                    ...              ...              ...           ...   \n",
      "885                 False            False            False         False   \n",
      "886                 False            False            False         False   \n",
      "887                 False            False            False         False   \n",
      "888                 False            False            False         False   \n",
      "889                 False            False            False         False   \n",
      "\n",
      "     amenities_count    url  bedrooms  \n",
      "0              False  False     False  \n",
      "1              False  False     False  \n",
      "2              False  False     False  \n",
      "3              False  False     False  \n",
      "4              False  False     False  \n",
      "..               ...    ...       ...  \n",
      "885            False  False     False  \n",
      "886            False  False     False  \n",
      "887            False  False     False  \n",
      "888            False  False     False  \n",
      "889            False  False     False  \n",
      "\n",
      "[890 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "from tabular_data import load_airbnb  \n",
    "df = pd.read_csv('airbnb-property-listing/tabular_data/clean_listing.csv', index_col='Unnamed: 0')\n",
    "X,y = load_airbnb(df,label=\"Price_Night\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Category', 'Title', 'Description', 'Amenities', 'Location',\n",
       "       'guests', 'beds', 'bathrooms', 'Price_Night', 'Cleanliness_rating',\n",
       "       'Accuracy_rating', 'Communication_rating', 'Location_rating',\n",
       "       'Check-in_rating', 'Value_rating', 'amenities_count', 'url',\n",
       "       'bedrooms'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(df.isnull())\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def preprocess_data(self, df:pd.DataFrame,label:str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \n",
    "        y = df[label]\n",
    "        X = df.drop(columns=[label ,'ID', 'Title', 'Description', 'Amenities', 'Location', 'url'])\n",
    "        encoder = LabelEncoder()\n",
    "        X['Category'] = encoder.fit_transform(X['Category'])\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        X_scaled = pd.DataFrame(X_scaled,columns=X.columns)\n",
    "\n",
    "        return X_scaled, y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
